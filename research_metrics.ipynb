{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricizing LLMaAiTB-E\n",
    "\n",
    "- Our Focus: Generation Quality\n",
    "- Measurement Techniques: \n",
    "    - Vector Comparison\n",
    "    - Human Preference Sample (A/B/C Testing)\n",
    "    - ???\n",
    "- Iterative documents to measure:\n",
    "    - Concepts (Generation phase)\n",
    "    - Slides (Teaching phase)\n",
    "    - Questions (Testing phase)\n",
    "- Resources for testing:\n",
    "    - Expert (From classes)\n",
    "    - GPT4 (Generation)\n",
    "    - LLMaAiTB-E (Teachabull)\n",
    "- Main concepts to cover:\n",
    "    - Object Oriented Programming\n",
    "    - Programming Language Semantics\n",
    "    - Math\n",
    "    - History\n",
    "    - \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Helper Functions\n",
    "We will demonstrate our metrics using OpenAI's Vector Embeddings on our generated documents. We decided to use OpenAI's embeddings due to their large document size capacity. We agreed that this method would prove to be the best while comparing large documents.\n",
    "\n",
    "## LLM Prompt/Text Completion\n",
    "\n",
    "\n",
    "## Vector Comparison\n",
    "Embeddings: OpenAIâ€™s text embeddings measure the relatedness of text strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pickle as pkl\n",
    "from AITutor_Backend.src.TutorUtils.concepts import *\n",
    "from AITutor_Backend.src.TutorUtils.notebank import NoteBank\n",
    "from AITutor_Backend.src.TutorUtils.slides import SlidePlan, Slide, SlidePlanner, Purpose\n",
    "from AITutor_Backend.src.TutorUtils.questions import Question, QuestionSuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPENAI HELPER FUNCTIONS \n",
    "def request_output_from_llm(prompt, model: str):\n",
    "    \"\"\"Requests the Concept information from an LLM.\n",
    "\n",
    "    Args:\n",
    "        prompt: (str) - string to get passed to the model\n",
    "        model: (str) - \n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI() \n",
    "    \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "    {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": prompt,\n",
    "    },\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=8000,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Embeddings (1, 1536): [[-0.00125975  0.00951579  0.0048352  ... -0.01831474 -0.00588236\n",
      "  -0.04087433]]\n",
      "Cosine Similarity: 1.0\n"
     ]
    }
   ],
   "source": [
    "### Vector Functions\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "def tokenizer(text):\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    return tokens\n",
    "\n",
    "def process_in_batches(tokens, batch_size=8000):\n",
    "    for i in range(0, len(tokens), batch_size):\n",
    "        yield tokens[i:i + batch_size]\n",
    "\n",
    "def create_embeddings(text):\n",
    "    tokens = tokenizer(text)\n",
    "    embeddings = []\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")  # Reuse the encoding for decoding\n",
    "\n",
    "    for token_batch in process_in_batches(tokens):\n",
    "        # Convert token batch back to string\n",
    "        batch_text = encoding.decode(token_batch)\n",
    "        batch_embedding = get_embedding(batch_text)\n",
    "        embeddings.append(batch_embedding)\n",
    "\n",
    "    return np.mean([embeddings], axis=1)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n",
    "# Test works\n",
    "embeddings = create_embeddings(\"Research/generation_data/slides/Expert/codingSlides_expert.json\")\n",
    "print(f\"Vector Embeddings {embeddings.shape}:\", embeddings, )\n",
    "\n",
    "# Test = 1\n",
    "vec1 = np.array([1, 2, 3])\n",
    "vec2 = np.array([2, 4, 6])\n",
    "similarity = cosine_similarity(vec1, vec2)\n",
    "print(f\"Cosine Similarity: {similarity}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts\n",
    "- Preprocessing\n",
    "- Generation of Graph for (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notebanks from AI Tutor\n",
    "current_topic = \"NLP\"\n",
    "main_concept = \"Natural Language Processing: Regular Expressions, Text Normalization, Edit Distance\"\n",
    "tutor_plan_nlp = '''Main Concept: Regular Expressions, Text Normalization, Edit Distance in Natural Language Processing (NLP)\n",
    "Student is a computer science student with no prior knowledge of the topic, requiring an introductory lesson.\n",
    "Student is taking an NLP class, suggesting the lessons are for academic purposes and should cover necessary conceptual detail.\n",
    "Student provided a chapter summary that includes key subtopics; this will be a guide in structuring the lesson plan.\n",
    "Tutor shall educate on the following concepts:\n",
    "Subconcept: Introduction to Regular Expressions\n",
    "Subconcept: Uses of Regular Expressions in NLP\n",
    "Subconcept: Basic Syntax and Operators of Regular Expressions\n",
    "Subconcept: Practical Examples and Exercises Using Regular Expressions\n",
    "Subconcept: Introduction to Text Normalization\n",
    "Subconcept: Tokenization of Text\n",
    "Subconcept: Lemmatization and its Importance\n",
    "Subconcept: Sentence Segmentation Techniques\n",
    "Subconcept: Introduction to Edit Distance\n",
    "Subconcept: Applications of Edit Distance Algorithm in NLP\n",
    "Subconcept: Calculation of Edit Distance and String Alignment\n",
    "Tutor will apply practical examples relevant to modern NLP applications, such as chatbots, using the chapter summary as a conversational context.\n",
    "Tutor will provide hands-on practice problems and ensure the student understands the implementation of the concepts.\n",
    "Student's objective: To gain a foundational understanding of the chapter's main points, to apply this understanding in an academic setting, and to perform well in the NLP class.\n",
    "Since the student might need to have a deep understanding of the class material, the lesson should provide a solid theoretical basis, followed by practical application.\n",
    "Concept: Regular Expressions, Text Normalization, Edit Distance in Natural Language Processing (NLP)\n",
    "Concept: Introduction to Regular Expressions\n",
    "Concept: Uses of Regular Expressions in NLP\n",
    "Concept: Basic Syntax and Operators of Regular Expressions\n",
    "Concept: Practical Examples and Exercises Using Regular Expressions\n",
    "Concept: Introduction to Text Normalization\n",
    "Concept: Tokenization of Text\n",
    "Concept: Lemmatization and its Importance\n",
    "Concept: Sentence Segmentation Techniques\n",
    "Concept: Introduction to Edit Distance\n",
    "Concept: Applications of Edit Distance Algorithm in NLP\n",
    "Concept: Calculation of Edit Distance and String Alignment\n",
    "Concept: Practical Examples and Exercises in Modern NLP Applications (e.g., Chatbots)\n",
    "Concept: Hands-on Practice Problems\n",
    "Concept: Foundational Understanding of Main Points\n",
    "Concept: Academic Application of Concepts\n",
    "Concept: Theoretical Basis Followed by Practical Application\n",
    "Student's Interest Statement: I find natural language processing interesting and important since I am taking it as a course in college where I will be tested\n",
    "Student's Slides Preference Statement: I want to be taught by information and examples\n",
    "Student's Questions Preference Statement: 2 of multiple choice, 2 of free response and 2 coding questions'''\n",
    "\n",
    "tutor_plan_economics = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "tutor_plan_calc = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "tutor_plan_art = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "tutor_plan_history = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "current_plan = {'NLP': tutor_plan_nlp, \"history\": tutor_plan_history, \"art\":tutor_plan_art, \"econ\": tutor_plan_economics, \"calc\": tutor_plan_calc}[current_topic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"Natural Language Processing: Regular Expressions, Text Normalization, Edit Distance\",\n",
      "\"definition\": \"Natural Language Processing (NLP) is a field of computer science and linguistics concerned with the interactions between computers and human (natural) languages. Within NLP, Regular Expressions are used for pattern matching and text processing tasks. Text Normalization is the process of transforming text into a canonical (standard) form, which includes Tokenization of Text , Lemmatization , and Sentence Segmentation Techniques . Edit Distance is a way of quantifying how dissimilar two strings (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other. These concepts are foundational in NLP for various applications, such as information extraction, text mining, and conversational agents like chatbots. They facilitate the processing and understanding of large volumes of text data, enabling the development of intelligent systems that can perform tasks involving natural language understanding and generation.\",\n",
      "\"latex\": \"Natural Language Processing: Regular Expressions, Text Normalization, Edit Distance\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Regular Expressions\",\n",
      "\"definition\": \"Regular expressions, commonly known as regex, are sequences of characters that define a search pattern, primarily used in string processing like pattern matching, text normalization, and text parsing. These expressions are constructed using a specific syntax that includes a variety of characters and operators such as wildcards, character classes, quantifiers, and anchors. Understanding Regular Expressions is essential in fields such as Natural Language Processing (NLP) , as they allow for efficient data manipulation and pattern recognition within large bodies of text. They serve as a foundational tool for tasks such as tokenization , Lemmatization , and developing algorithms related to Edit Distance , which are crucial for text analysis and preparing data for further NLP tasks.\",\n",
      "\"latex\": \"Regular Expressions\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Natural Language Processing (NLP)\",\n",
      "\"definition\": \"Natural Language Processing (NLP) is a field of artificial intelligence that enables computers to understand, interpret, and generate human language. It encompasses a range of computational techniques and algorithms, such as Regular Expressions , Text Normalization , and Edit Distance , to facilitate the processing of large volumes of natural language data. NLP is essential in various applications including machine translation, sentiment analysis, information extraction, and conversational agents. By employing methods like Tokenization of Text , Lemmatization , and Sentence Segmentation Techniques , NLP systems can analyze text and speech to perform tasks that typically require human understanding, thus bridging the gap between human communication and machine interpretation.\",\n",
      "\"latex\": \"Natural Language Processing (NLP)\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Text Normalization\",\n",
      "\"definition\": \"Text Normalization in the context of Natural Language Processing (NLP) involves the process of transforming text into a more consistent and standardized form. It is essential for preprocessing data in NLP tasks, making it easier for algorithms to understand and process language data. This process includes tokenization of text , which divides text into smaller units such as words or phrases, and lemmatization , which reduces words to their base or dictionary form. Text Normalization also encompasses sentence segmentation techniques , which involve dividing a text into its constituent sentences, providing a structured approach to analyzing and understanding text. These normalization techniques are crucial for a wide array of NLP applications, from text-to-speech systems to sentiment analysis.\",\n",
      "\"latex\": \"Text Normalization\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Edit Distance\",\n",
      "\"definition\": \"In the context of Natural Language Processing (NLP) , Edit Distance is a way of quantifying how dissimilar two strings (e.g., words) are by counting the minimum number of operations required to transform one string into the other. The operations are typically insertions, deletions, or substitutions of a single character. Edit Distance has important applications in NLP, such as spell checking, plagiarism detection, and DNA sequencing. It is also fundamental in algorithms that perform Text Normalization , including tasks like tokenization of text , lemmatization , and sentence segmentation . Understanding Edit Distance helps in developing algorithms for text similarity and is essential for many modern NLP applications, including language translation systems and chatbots.\",\n",
      "\"latex\": \"Edit Distance\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Tokenization of Text\",\n",
      "\"definition\": \"In the field of Natural Language Processing (NLP) , Tokenization of Text is the process of converting a sequence of characters into a sequence of tokens, which are the building blocks of text. Tokens are typically words, phrases, or other meaningful elements that can be used for further processing, such as Text Normalization , parsing, and understanding. Tokenization is a fundamental step in NLP as it directly affects the quality of the input for subsequent tasks like sentiment analysis , entity recognition , and syntactic analysis . The tokenization process involves various challenges, including handling different scripts, distinguishing between punctuation and text, and dealing with contractions and special characters.\",\n",
      "\"latex\": \"Tokenization of Text\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Lemmatization\",\n",
      "\"definition\": \"In the context of Natural Language Processing (NLP) , Lemmatization refers to the algorithmic process of determining the lemma for a given word. A lemma is the canonical form, dictionary form, or citation form of a set of words. For instance, 'run', 'runs', 'ran', and 'running' all reduce to the lemma 'run'. The goal of lemmatization is to reduce inflectional forms of a word into a single base or root form that has a dictionary meaning. Unlike stemming, which simply removes or truncates suffixes from a word, lemmatization involves a more sophisticated analysis to accurately find the lemma. This process often requires understanding the word's part of speech, its meaning in context, and its morphological analysis. Lemmatization is a crucial step in Text Normalization and is often used to improve text analysis and processing tasks, such as text indexing, topic modeling, and semantic analysis, by reducing words to their base form.\",\n",
      "\"latex\": \"Lemmatization\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Sentence Segmentation Techniques\",\n",
      "\"definition\": \"Sentence Segmentation Techniques refer to the methods used in Natural Language Processing (NLP) to accurately identify the boundaries of sentences within a text. This is crucial for further text processing tasks such as Tokenization of Text , Lemmatization , and syntactic parsing. Sentence segmentation is typically achieved using algorithms that identify punctuation marks indicative of sentence boundaries, such as periods, question marks, and exclamation points. More advanced techniques may also consider contextual clues and linguistic rules to handle edge cases where punctuation does not solely determine the end of a sentence, such as abbreviations or dialogue cues. Effective sentence segmentation is a foundational step in the text normalization process, which ensures that the textual data is in a standard form suitable for analysis and processing by various NLP applications.\",\n",
      "\"latex\": \"Sentence Segmentation Techniques\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Tokenization\",\n",
      "\"definition\": \"Tokenization is a fundamental Natural Language Processing (NLP) technique that involves breaking down a string of text into smaller units, called tokens. This process is essential for many NLP tasks as it facilitates the understanding and manipulation of text by converting a continuous stream of characters into discrete elements. Tokens usually correspond to words, phrases, or other meaningful elements in the text, and the process often involves the removal of punctuation and separation based on whitespace and other delimiters. Tokenization serves as the basis for further text analysis operations, such as Lemmatization , part-of-speech tagging , and syntax parsing . It is also a precursor to Text Normalization and the calculation of Edit Distance in NLP applications. Effective tokenization is critical for accurate text representation and plays a significant role in the performance of NLP systems.\",\n",
      "\"latex\": \"Tokenization\",\n",
      "}\n",
      "\n",
      "{\"name\": \"part-of-speech tagging\",\n",
      "\"definition\": \"Part-of-speech tagging, also known as POS tagging or grammatical tagging, is a process in Natural Language Processing (NLP) where words in a text are marked with their corresponding part of speech, such as nouns, verbs, adjectives, etc. This information is crucial for many NLP tasks because it helps in understanding the syntax and meaning of sentences. POS tagging is often performed using algorithms that consider both the definition of the word and the context in which it is used. Techniques such as machine learning , Regular Expressions , and contextual clues are employed to accurately assign parts of speech to each word. The outcome of POS tagging is beneficial for tasks like Text Normalization , syntactic parsing , and information extraction .\",\n",
      "\"latex\": \"part-of-speech tagging\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Syntax Parsing\",\n",
      "\"definition\": \"Syntax parsing, also known as syntactic analysis, is a process in Natural Language Processing (NLP) that involves analyzing the structure of sentences to understand their constituent parts, such as phrases and sub-phrases, and the grammatical relationship between them. It enables the decomposition of natural language text into its grammatical components, providing a way to understand and represent the syntactic structure of language. Syntax parsing is used to build parse trees, which illustrate the hierarchical structure of sentence syntax and are crucial for further NLP tasks such as semantic analysis , machine translation , and information extraction . This process relies on formal grammars, such as context-free grammars, and algorithms like chart parsing, shift-reduce parsing, and dependency parsing to accurately model the syntax of language.\",\n",
      "\"latex\": \"Syntax Parsing\",\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Concept generation from AITutor:\n",
    "import pickle as pkl\n",
    "notebank = NoteBank()\n",
    "[notebank.add_note(n) for n in current_plan.split(\"\\n\")]\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(f\"Research/temp_data/temp_concepts_{current_topic}.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(f\"Research/temp_data/temp_concepts_{current_topic}.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        concept = pkl.load(f)\n",
    "        concept_db = ConceptDatabase(main_concept, notebank.env_string(), False)\n",
    "        concept_db.Concepts = concept\n",
    "else:\n",
    "    concept_db = ConceptDatabase(main_concept,notebank.env_string())\n",
    "    with open(f\"Research/temp_data/temp_concepts_{current_topic}.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(concept_db.Concepts, f)\n",
    "\n",
    "print(\"\\n\\n\".join([slide.format_json() for slide in concept_db.Concepts]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slides: \n",
    "- Preprocessing\n",
    "- Generation of Document for (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SLIDE OBJ PROMPTs\n",
    "prompt = ''' #Your task is to create a JSON object from a slide string. View the example Input and output, and then repeat the same for the provided input. \n",
    "Perform the conversion for each slide s in the input string such that s->json_object(s). You should be able to figure out which is the title and which is the description.\n",
    "IMPORTANT: Escape Characters in JSON Data can cause errors if the JSON Object or JSON data contains backslashes, which means they need to be properly escaped\n",
    "Avoid these errors: Invalid \\escape: line 24 column 72 (char 2199)\n",
    "By properly escaping your backslashes ('\\\\')\n",
    "IMPORTANT: If there is two words together, such as \"functionwhere\", without being separated with a white space, that most probably means that there is a new line ('\\n') or space (' ') in between them, e.g. \"function where\".\n",
    "\n",
    "// Input:\n",
    "Page 1 Content:\n",
    "Natural Language ProcessingProfessor John LicatoUniversity of South FloridaChapter 2:RegEx, Edit Distance\n",
    "\n",
    "----------------------------------------\n",
    "Page 2 Content:\n",
    "\"Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When youâ€™re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.\"Regular Expressions\n",
    "----------------------------------------\n",
    "Page 3 Content:\n",
    "The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))Regular Expressions\n",
    "----------------------------------------\n",
    "Page 4 Content:\n",
    "The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')Regular Expressions\n",
    "----------------------------------------\n",
    "Page 5 Content:\n",
    "Creating regex objectsrâ€™ = raw string\\d â€“ placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(râ€™\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\dâ€™)\n",
    "----------------------------------------\n",
    "Page 6 Content:\n",
    "Matching regex objects\n",
    "mo = match object â€“ contains the result of our search>>> import re>>> phoneNumRegex = re.compile(râ€™\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\dâ€™)>>> mo = phoneNumRegex.search(â€˜My number is 415-555-4242.â€™)>>> print(â€˜Phone number found: â€™ + mo.group())Phone number found: 415-555-4242\n",
    "----------------------------------------\n",
    "Page 7 Content:\n",
    "Text Normalizationâ€¢We will work a lot with large datasets / corporaâ€¢We often need to pre-process textâ€¢Tokenizing (segmenting) wordsâ€¢Normalizing word formatsâ€¢Segmenting sentences (e.g. by using punctuation)\n",
    "----------------------------------------\n",
    "Page 8 Content:\n",
    "Tokenization â€“ segmenting running text into words (or word-like units)>>> text = 'That U.S.A. poster-print costs $12.40...'>>> pattern = r\\'\\'\\', (?x)  # set flag to allow verbose regexps...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A....     | \\w+(-\\w+)*      # words with optional internal hyphens...     | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%...     | \\.\\.\\.          # ellipsis...     | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [... \\'\\'\\'>>> nltk.regexp_tokenize(text, pattern)['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n",
    "----------------------------------------\n",
    "Page 9 Content:\n",
    "Subword tokenizationâ€¢How do we capture relations between words like:â€“new, newerâ€“blow, blowingâ€“precipitation, precipitateâ€¢Often useful to break tokens into *sub*wordsâ€¢Usually split into token learners, and token segmenters\n",
    "----------------------------------------\n",
    "Page 10 Content:\n",
    "Byte-pair encoding (BPE)â€¢A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab VV <- all unique characters in C                  # initial set of tokens is charactersfor i = 1 to k do                                # merge tokens til k times    t_L, t_R <- Most frequent pair of adjacent tokens in C    t_new <- t_L + t_R                           # make new token by concatenating    V <- V + t_new                               # update the vocabulary    Replace each occurrence of t_L, t_R in C with t_new # and update the corpusreturn Vcorpus5 low_2 lowest_6 newer_3 wider_2 new_vocabulary_, d, e, i, l, n, o, r, s, t, wcorpus5 low _2 lowest _6 newer _3 wider _2 new _vocabulary_, d, e, i, l, n, o, r, s, t, w, er\n",
    "----------------------------------------\n",
    "...\n",
    "        \n",
    "// Output:\n",
    "        { \n",
    "                \\\"slides\\\":[\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Natural Language Processing\\\", \n",
    "                                \\\"Description\\\": \\\"Professor John Licato University of South Florida Chapter 2:RegEx, Edit Distance\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When youâ€™re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Creating regex objects\\\", \n",
    "                                \\\"Description\\\": \\\"râ€™ = raw string\\d â€“ placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(râ€™\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\dâ€™)\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Matching regex objects\\\", \n",
    "                                \\\"Description\\\": \\\">>> import re>>> phoneNumRegex = re.compile(râ€™\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\dâ€™)>>> mo = phoneNumRegex.search(â€˜My number is 415-555-4242.â€™)>>> print(â€˜Phone number found: â€™ + mo.group())Phone number found: 415-555-4242 mo = match object â€“ contains the result of our search\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Text Normalization\\\", \n",
    "                                \\\"Description\\\": \\\"â€¢We will work a lot with large datasets / corpora\\nâ€¢We often need to pre-process text\\nâ€¢Tokenizing (segmenting) words\\nâ€¢Normalizing word formats\\nâ€¢Segmenting sentences (e.g. by using punctuation) )\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Tokenization â€“ segmenting running text into words (or word-like units)\\\", \n",
    "                                \\\"Description\\\": \\\">>> text = 'That U.S.A. poster-print costs $12.40...'\\n>>> pattern = r\\'\\'\\', (?x)  # set flag to allow verbose regexps\\n...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A\\n....     | \\w+(-\\w+)*      # words with optional internal hyphens\\n...     | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\\n...     | \\.\\.\\.          # ellipsis...     | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [\\n... \\'\\'\\'\\n>>> nltk.regexp_tokenize(text, pattern)\\n['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Subword tokenization\\\", \n",
    "                                \\\"Description\\\": \\\"â€¢How do we capture relations between words like:\\nâ€“new, newer\\nâ€“blow, blowing\\nâ€“precipitation, precipitate\\nâ€¢Often useful to break tokens into *sub*wordsâ€¢Usually split into token learners, and token segmenters\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Byte-pair encoding (BPE)\\\", \n",
    "                                \\\"Description\\\": \\\"â€¢A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab V\\nV <- all unique characters in C                  # initial set of tokens is characters\\nfor i = 1 to k do                                # merge tokens til k times    \\nt_L, t_R <- Most frequent pair of adjacent tokens in C    \\nt_new <- t_L + t_R                           # make new token by concatenating    \\nV <- V + t_new                               # update the vocabulary    \\nReplace each occurrence of t_L, t_R in C with t_new # and update the corpus\\nreturn V\\ncorpus\\n5 low_\\n2 lowest_\\n6 newer_\\n3 wider_\\n2 new_\\nvocabulary\\n_, d, e, i, l, n, o, r, s, t, w\\ncorpus\\n5 low _\\n2 lowest _\\n6 newer _\\n3 wider _\\n2 new _\\nvocabulary\\n_, d, e, i, l, n, o, r, s, t, w, er\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        ...\n",
    "                ]\n",
    "        }\n",
    "Remember! Escape Characters in JSON Data: If the JSON Object or JSON data contains backslashes, they need to be properly escaped.\n",
    "Avoid these errors: Invalid \\escape: line 24 column 72 (char 2199)\n",
    "\n",
    "// Input:\n",
    "        $SLIDE$\n",
    "\n",
    "// Output:\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Slide helper functions\n",
    "import PyPDF2\n",
    "from pptx import Presentation\n",
    "import json\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"Reads a PDF file and prints the content of each page\"\"\"\n",
    "    slide_str = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "\n",
    "        for i in range(num_pages):\n",
    "            page = reader.pages[i]\n",
    "            text = page.extract_text()\n",
    "            slide_str += f\"Page {i+1} Content:\\n{text}\"\n",
    "            slide_str += \"\\n\" + (\"-\" * 40) + \"\\n\"\n",
    "    return slide_str\n",
    "\n",
    "def extract_text_from_slide(slide):\n",
    "    \"\"\"Extracts title and content from a slide\"\"\"\n",
    "    title = slide.shapes.title.text if slide.shapes.title else \"No Title\"\n",
    "    content = []\n",
    "\n",
    "    for shape in slide.shapes:\n",
    "        if hasattr(shape, \"text\"):\n",
    "            content.append(shape.text)\n",
    "\n",
    "    return title, content\n",
    "\n",
    "def read_pptx(file_path):\n",
    "    \"\"\"Reads a pptx file and prints the title and content of each slide\"\"\"\n",
    "    prs = Presentation(file_path)\n",
    "\n",
    "    for slide in prs.slides:\n",
    "        title, content = extract_text_from_slide(slide)\n",
    "        print(f\"Title: {title}\")\n",
    "        print(\"Content:\", \"\\n\".join(content))\n",
    "        print(\"-\" * 40)\n",
    "def get_slide_prompt(slide_template, data):\n",
    "    return slide_template.replace(\"$SLIDE$\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Research/generation_data/slides/Expert/codingSlides_expert_RAW.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/oem/Development/AITutor-Backend/research_metrics.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oem/Development/AITutor-Backend/research_metrics.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m### TEST SLIDE OBJ GEN FROM GPT FOR EXPERT\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/oem/Development/AITutor-Backend/research_metrics.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m slide_str \u001b[39m=\u001b[39m read_pdf(\u001b[39m'\u001b[39;49m\u001b[39mResearch/generation_data/slides/Expert/codingSlides_expert_RAW.pdf\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oem/Development/AITutor-Backend/research_metrics.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m curr_prompt \u001b[39m=\u001b[39m get_slide_prompt(prompt, slide_str)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oem/Development/AITutor-Backend/research_metrics.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m/home/oem/Development/AITutor-Backend/research_metrics.ipynb Cell 12\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oem/Development/AITutor-Backend/research_metrics.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Reads a PDF file and prints the content of each page\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oem/Development/AITutor-Backend/research_metrics.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m slide_str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/oem/Development/AITutor-Backend/research_metrics.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_path, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/oem/Development/AITutor-Backend/research_metrics.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     reader \u001b[39m=\u001b[39m PyPDF2\u001b[39m.\u001b[39mPdfReader(file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/oem/Development/AITutor-Backend/research_metrics.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     num_pages \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(reader\u001b[39m.\u001b[39mpages)\n",
      "File \u001b[0;32m~/miniconda/envs/aitutor/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Research/generation_data/slides/Expert/codingSlides_expert_RAW.pdf'"
     ]
    }
   ],
   "source": [
    "### TEST SLIDE OBJ GEN FROM GPT FOR EXPERT\n",
    "slide_str = read_pdf('Research/generation_data/slides/Expert/codingSlides_expert_RAW.pdf')\n",
    "\n",
    "\n",
    "curr_prompt = get_slide_prompt(prompt, slide_str)\n",
    "try:\n",
    "    json_data = request_output_from_llm(prompt=curr_prompt, model=\"gpt-3.5-turbo-16k\")\n",
    "    slide_obj = json.loads(json_data)\n",
    "    print(slide_obj)\n",
    "\n",
    "    # Convert the dictionary to a JSON-formatted string\n",
    "    json_str = json.dumps(slide_obj, indent=4)  # indent for pretty-printing\n",
    "\n",
    "    # Write the JSON string to a file\n",
    "    with open(\"Research/generation_data/slides/Expert/codingSlides_expert.json\", \"w\") as f:\n",
    "        f.write(json_str)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Introduction to Natural Language Processing (NLP) and Regular Expressions', 'purpose': 0, 'purpose_statement': 'To give the student an initial overview of NLP with a focus on Regular Expressions and how they play a foundational role in text analysis.', 'concepts': ['Natural Language Processing (NLP)', 'Regular Expressions', 'Text Normalization', 'Tokenization of Text', 'Edit Distance']}\n",
      "\n",
      "{'title': 'Decoding Patterns with Regular Expressions in NLP', 'purpose': 0, 'purpose_statement': 'This slide will serve as an initial deep dive into the world of Regular Expressions, enabling students to understand their syntax, basic operators, and fundamental uses in NLP, setting the groundwork for more sophisticated text processing tasks.', 'concepts': ['Regular Expressions']}\n",
      "\n",
      "{'title': 'Discovering Lemmatization: Enhancing Text Analysis in NLP', 'purpose': 0, 'purpose_statement': \"This slide aims to introduce Lemmatization as an essential NLP text preprocessing technique, building on the student's understanding of Text Normalization and Tokenization, to further enhance their text analysis skills.\", 'concepts': ['Lemmatization', 'Text Normalization', 'Tokenization of Text']}\n",
      "\n",
      "{'title': 'Engaging with Part-of-Speech Tagging in NLP', 'purpose': 0, 'purpose_statement': 'To introduce the foundational concept of part-of-speech tagging within Natural Language Processing (NLP), elucidating its definition, importance, and the role it plays in understanding the grammatical structure of language for text analysis and other NLP applications.', 'concepts': ['part-of-speech tagging', 'Tokenization of Text', 'Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Unveiling the Layers of Language: Syntax Parsing and Sentence Segmentation', 'purpose': 0, 'purpose_statement': \"This slide is designed to introduce the foundational concepts of 'Syntax Parsing' and 'Sentence Segmentation Techniques' in NLP, underscoring their roles in text analysis, their importance in machine understanding of language, and how they pave the way for advanced NLP tasks such as dependency parsing and named entity recognition.\", 'concepts': ['Syntax Parsing', 'Sentence Segmentation Techniques']}\n",
      "\n",
      "{'title': 'Transforming Texts: Mastering Edit Distance & Regular Expressions in NLP', 'purpose': 3, 'purpose_statement': \"This slide is tailored to solidify the student's foundational knowledge of Regular Expressions, focusing on their practical applications in text normalization and edit distance calculations in NLP. The content will bridge theoretical concepts with real-world tools to enhance the student's performance in an academic setting, and facilitate the application of these concepts through interactive examples and aligned practice problems.\", 'concepts': ['Regular Expressions', 'Edit Distance', 'Text Normalization']}\n",
      "\n",
      "{'title': 'Diving Deeper: Practical Applications of Edit Distance in NLP', 'purpose': 2, 'purpose_statement': 'This slide uncovers the significant role of Edit Distance in the field of Natural Language Processing, with a focus on real-world applications like spelling correction in digital text analysis and comparative genomics through DNA sequencing. The aim is to provide the student with an understanding of Edit Distance that bridges the gap between academic learning and practical implementation in modern computational linguistics and bioinformatics.', 'concepts': ['Edit Distance']}\n",
      "\n",
      "{'title': 'Enhancing Natural Language Processing with Tokenization Techniques', 'purpose': 2, 'purpose_statement': \"This slide aims to delve into the specifics of 'Tokenization'â€”a fundamental concept in NLP not yet fully explored in our slide set. By exploring different tokenization techniques and their pertinence in various contexts of NLP, the student will gain a deeper understanding of how text is processed at the foundational level for more complex tasks.\", 'concepts': ['Tokenization', 'Regular Expressions', 'Text Normalization']}\n",
      "\n",
      "{'title': 'Enhancing Natural Language Processing with Tokenization Techniques', 'purpose': 2, 'purpose_statement': \"This slide aims to delve into the specifics of 'Tokenization'â€”a fundamental concept in NLP not yet fully explored in our slide set. By exploring different tokenization techniques and their pertinence in various contexts of NLP, the student will gain a deeper understanding of how text is processed at the foundational level for more complex tasks.\", 'concepts': ['Tokenization', 'Regular Expressions', 'Text Normalization']}\n",
      "\n",
      "{'title': 'Navigating the Grammar of Language: part-of-speech tagging in NLP', 'purpose': 2, 'purpose_statement': 'This slide aims to deepen the studentâ€™s understanding of the part-of-speech tagging process within the broader scope of NLP, demonstrating the connection between tagging, sentence structure, and linguistic meaning, as well as its application in real-world text analysis challenges.', 'concepts': ['part-of-speech tagging', 'Natural Language Processing (NLP)', 'Tokenization of Text', 'Syntax Parsing']}\n",
      "\n",
      "{'title': 'Practical Insights into Lemmatization and Sentence Segmentation in NLP', 'purpose': 4, 'purpose_statement': \"To consolidate the student's theoretical understanding of Lemmatization and Sentence Segmentation Techniques through practical examples and exercises, demonstrating their vital roles in NLP applications such as machine translation and information retrieval systems.\", 'concepts': ['Lemmatization', 'Sentence Segmentation Techniques']}\n",
      "\n",
      "{'title': 'Advanced Utilization of Regular Expressions in NLP', 'purpose': 3, 'purpose_statement': \"To expand the student's knowledge of Regular Expressions beyond basics, covering advanced pattern matching and efficiency in text parsing, and to provide practical applications in NLP.\", 'concepts': ['Regular Expressions']}\n",
      "\n",
      "{'title': 'Mastery Through Practice: Regular Expressions in Real-World NLP Applications', 'purpose': 4, 'purpose_statement': \"This slide aims to reinforce and enhance the student's theoretical understanding of Regular Expressions through curated examples and exercises, focusing on their implementation in real-world Natural Language Processing applications.\", 'concepts': ['Regular Expressions', 'Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Synergy of Patterns and Distance: Regular Expressions Meet Edit Distances in NLP', 'purpose': 3, 'purpose_statement': 'To explain the synergistic relationship between Regular Expressions and Edit Distance in the context of Natural Language Processing (NLP) and demonstrate their application in text analysis, error detection, and correction.', 'concepts': ['Regular Expressions', 'Edit Distance', 'Text Normalization']}\n",
      "{\n",
      "    \"slides\": [\n",
      "        {\n",
      "            \"title\": \"Introduction to Natural Language Processing (NLP) and Regular Expressions\",\n",
      "            \"presentation\": \"Hello! Today, we're starting our adventure into the world of Natural Language Processing, or NLP for short. This is really the magic behind how computers can seem to understand us, humans. You know when you talk to Siri or Alexa and they actually get what you're saying? Well, that's NLP in action. There's a ton of exciting stuff we can do with it, like translating languages, figuring out whether people's comments are happy or sad, digging out key information from large texts, or building chatbots. Now, let's introduce an incredibly useful tool in our NLP toolkit called Regular Expressions, often referred to as regex. Think of regex like a hunter. It's really good at tracking down and capturing specific patterns in text. It's kind of like playing Where's Waldo with words! If we want to tidy up text before we make a computer process it, or perhaps we're trying to spot all the times a specific word or number appears, regex is our go-to. You'll see how we can use this super tool to do things like checking if an email address is written correctly, or finding all the hashtags in a tweet. The actual symbols and commands used in regex can look a bit like a secret code at first, but don't worry, we'll go through some examples together. By mastering regex and the broader field of NLP, we're wading into the waters of computational linguistics and the vast ocean of AI. This knowledge is like a golden ticket in today's tech-centric world, so ready your mind's sails \\u2013 we're off!\",\n",
      "            \"content\": \"Introduction to Natural Language Processing (NLP) and Regular Expressions\\n\\n- NLP is the field that enables computers to understand, interpret, and generate human languages\\n\\n- Applications of NLP include machine translation, sentiment analysis, information extraction, and conversational agents\\n\\n- Regular Expressions (regex) play a foundational role in text analysis\\n\\n- Regex is a versatile tool for pattern matching and text parsing\\n\\n- Examples of regex usage: searching for patterns, text normalization\\n\\n- Syntax and use of regex in simple NLP tasks\\n\\n- Importance of mastering NLP and regex in the modern landscape of computational linguistics and AI\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 0,\n",
      "            \"purpose_statement\": \"To give the student an initial overview of NLP with a focus on Regular Expressions and how they play a foundational role in text analysis.\",\n",
      "            \"concepts\": [\n",
      "                \"Natural Language Processing (NLP)\",\n",
      "                \"Regular Expressions\",\n",
      "                \"Text Normalization\",\n",
      "                \"Tokenization of Text\",\n",
      "                \"Edit Distance\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Decoding Patterns with Regular Expressions in NLP\",\n",
      "            \"presentation\": \"Alright, let's dive into the fascinating world of Regular Expressions! Think of them as a secret code for finding patterns in text \\u2014 essential for processing languages in computers. Now, imagine you have a string of text and you want to find specific information, like email addresses or dates. That's where Regular Expressions, or regex for short, come into play. They're sequences of characters that define a search pattern, and they're incredibly powerful in NLP tasks. For instance, when you're using a search function, it's regex that's working behind the scenes to find your match. Now, let's break down the syntax of regex. We have wildcards like '*' which means 'any number of any character', and '?' representing 'one or any character'. Then, there are character classes; for example, '[a-z]' specifies any lowercase letter. Quantifiers like '{m,n}' set the minimum and maximum number of occurrences for a character. And anchors, '^' signifies the start of a string, while '$' indicates the end. Let's put it into a practical perspective. Say you want to verify if a string is a valid email address. By combining these operators, regex can specify the pattern of a standard email, identifying if our string fits that pattern. It's like a puzzle where each piece of syntax helps us form the bigger picture. In the realm of NLP, regex plays a crucial role in text normalization. It helps in tokenization \\u2014 chopping up text into pieces like words or sentences. Then there's lemmatization, where words are trimmed down to their base form, like turning 'running' into 'run'. And for sentence segmentation, regex helps in spotting the ends of sentences so they can be separated properly. These steps are all part of preparing text for more complex processing. The relationship between regex, text normalization, and edit distance is like a team that works together to polish text data, making it ready for NLP applications. You see, NLP isn't just theoretical; it has very practical applications like data mining, where regex can extract information from large volumes of text. Or in chatbot development, it helps in understanding and responding to user inputs accurately. What we're setting up here is the groundwork for you to conquer text processing tasks in your NLP course and your future adventures in computer science. Isn't it exciting to think about all the possibilities?\",\n",
      "            \"content\": \"- Definition of Regular Expressions: sequences of characters that form a search pattern\\n- Syntax of Regular Expressions: basic operators such as wildcards (* and ?), character classes ([a-z]), quantifiers ({m,n}), and anchors (^ and $)\\n- Practical Examples of Regular Expressions: email address validation as an example that showcases the use of different operators\\n- Uses of Regular Expressions in NLP: tokenization, lemmatization, and sentence segmentation\\n- Relationship between Regular Expressions, Text Normalization, and Edit Distance in NLP\\n- Importance and Applications of Regular Expressions in NLP: data mining, chatbot development, etc.\\n- Objective of the slide: to provide an initial deep dive into Regular Expressions and their fundamental uses in NLP, setting the groundwork for future text processing tasks\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 0,\n",
      "            \"purpose_statement\": \"This slide will serve as an initial deep dive into the world of Regular Expressions, enabling students to understand their syntax, basic operators, and fundamental uses in NLP, setting the groundwork for more sophisticated text processing tasks.\",\n",
      "            \"concepts\": [\n",
      "                \"Regular Expressions\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Discovering Lemmatization: Enhancing Text Analysis in NLP\",\n",
      "            \"presentation\": \"We've touched on the importance of Text Normalization in making sense of the ocean of words we encounter in digital text. Now, let's wade a little deeper and uncover the elegance of Lemmatization, a technique at the heart of any thorough text analysis. Picture this: you have different forms of a word, say 'run', 'runner', 'running'. Your computer sees each as a unique word, but you know they are connected. That's where Lemmatization steps in. It peels away the varying endings and transforms each of these into their simplest form, 'run'. Unlike stemming, which might chop off the end to get 'run', Lemmatization understands that 'running' is a verb in action, while 'runner' is a noun\\u2014a person. By using this technique, you refine your text to its most communicative elements. Let's consider a practical case: Imagine you're analyzing tweets to gauge public opinion on a new tech gadget. The word 'impress' might appear as 'impressed', 'impressive', or 'impressively'. Lemmatization ensures that all these are recognized as variations of 'impress', giving you a more accurate sentiment analysis. This process follows Tokenization, another technique we explored, where text gets divided into usable pieces or tokens. Lemmatization builds on this, scrutinizing each word's role in a sentence, its grammatical nuances, and its essential meaning. By interweaving Tokenization with Lemmatization, we lay down a solid foundation for any NLP task, paving the way to a world where machines understand text with a near-human finesse. There's a real beauty to Lemmatization\\u2014its blend of linguistic elegance with algorithmic precision makes it a fundamental tool in your journey through Natural Language Processing. And each step of this journey brings you closer to excelling in your NLP class and understanding how conversational agents, like chatbots, get trained to comprehend our questions and provide informative answers.\",\n",
      "            \"content\": \"- Lemmatization is a technique used in NLP to reduce words to their base or dictionary form\\n- It goes beyond stemming by considering the word's function in context\\n- Lemmatization is vital in Text Normalization to standardize text for analysis\\n- It improves NLP tasks such as topic modeling and semantic analysis\\n- Lemmatization follows Tokenization, which breaks text into tokens\\n- Lemmatization takes into account the part of speech, role within a sentence, and morphological makeup\\n- Practical examples will demonstrate the importance of lemmatization in transforming raw text\\n- Lemmatization is a foundational concept in text processing within NLP\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 0,\n",
      "            \"purpose_statement\": \"This slide aims to introduce Lemmatization as an essential NLP text preprocessing technique, building on the student's understanding of Text Normalization and Tokenization, to further enhance their text analysis skills.\",\n",
      "            \"concepts\": [\n",
      "                \"Lemmatization\",\n",
      "                \"Text Normalization\",\n",
      "                \"Tokenization of Text\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Engaging with Part-of-Speech Tagging in NLP\",\n",
      "            \"presentation\": \"Alright, diving into slide number four, today's topic is Part-of-Speech Tagging in NLP, a fundamental concept that's going to make text understanding much clearer for you. Let's kick things off with what exactly this process is all about. Imagine you have a sentence, and you want to associate each word with its role in the language, like whether it's a noun, verb, adjective, and so on. That's where Part-of-Speech tagging comes in\\u2014it systematically identifies each word's function, setting the stage for deeper language analysis. Think of a simple sentence like 'The quick brown fox jumps over the lazy dog.' We tag 'The' and 'the' as articles, 'quick', 'brown', and 'lazy' as adjectives, 'fox' and 'dog' as nouns, and 'jumps' as a verb. This spotlighting of roles paves the way to understanding the structure, which is fundamental in NLP for tasks such as text normalization\\u2014cleaning up text data for processing, syntactic parsing\\u2014analyzing sentences' grammatical structure, and even information extraction. Moving on to the meat of POS Tagging, we have a workflow that begins with raw text. Before we can tag anything, we need to break the text down into chunks, or tokens\\u2014this step is crucial and is known as Tokenization. For example, our fox sentence becomes a series of tokens: 'The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.' Now we can start tagging each token with its respective part of speech. Here's where Regular Expressions, or regex for short, make a surprise entry. By using regex patterns, we can program algorithms to recognize parts of speech within our tokenized text. It's like having a smart filter that can pick out nouns, verbs, adjectives, etc., from a jumble of words. I'll show you how regex can be used to identify verbs in sentences, enhancing our POS tagging capacity. On the screen, you'll see diagrams and color-coded text that lay out these concepts visually. It's all about making the information pop and stick in your memory. Plus, these visual aids are going to be super handy when you're dissecting language for your NLP course exams. Lastly, why bother with all this tagging business, you might ask? Well, it's at the heart of modern NLP applications. Take chatbots, for instance; they use POS tagging to grasp what we're asking them and come up with coherent replies. It's a stepping stone to the much more sophisticated world of NLP that you're gearing up to tackle. Keep these points in mind, and you'll be analyzing and tagging text like a pro in no time.\",\n",
      "            \"content\": \"Slide Title: Engaging with Part-of-Speech Tagging in NLP\\n\\n- Introduction to Part-of-Speech (POS) Tagging\\n  - Definition: Process of associating each word in text with its corresponding part of speech\\n  - Example sentence with tagged parts of speech\\n\\n- Importance of POS Tagging in NLP\\n  - Understanding the grammatical structure of language\\n  - Text normalization, syntactic parsing, and information extraction\\n\\n- Workflow of POS Tagging\\n  - Raw text \\u27a1 Tokenization of Text \\u27a1 POS Tagging\\n  - Examples of tokenized text\\n\\n- Application of Regular Expressions in POS Tagging\\n  - Regex pattern to identify parts of speech patterns in text\\n\\n- Visual aids and examples\\n  - Diagrams and color-coded text\\n  - Multisensory approach for enhanced learning\\n\\n- Relevance of POS Tagging in Modern NLP Applications\\n  - Chatbots and other advanced NLP concepts\\n\\nObjective: Introduce the foundational concept of part-of-speech tagging within NLP, explaining its definition, importance, and role in understanding language structure for text analysis and other NLP applications\\n\\nStudent's Interest: Find NLP interesting and important since taking it as a course and will be tested\\n\\nStudent's Preference: Taught by information and examples\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 0,\n",
      "            \"purpose_statement\": \"To introduce the foundational concept of part-of-speech tagging within Natural Language Processing (NLP), elucidating its definition, importance, and the role it plays in understanding the grammatical structure of language for text analysis and other NLP applications.\",\n",
      "            \"concepts\": [\n",
      "                \"part-of-speech tagging\",\n",
      "                \"Tokenization of Text\",\n",
      "                \"Natural Language Processing (NLP)\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Unveiling the Layers of Language: Syntax Parsing and Sentence Segmentation\",\n",
      "            \"presentation\": \"Great to see you're getting into the swing of things with NLP! So far, we've covered some fundamental concepts. Now, let's dive into the mechanics of how NLP deals with understanding sentence structure and meaning. We'll be looking at two crucial concepts: syntax parsing and sentence segmentation techniques. Syntax parsing is a bit like the grammatical analysis you've done in school, but here, it's all about teaching a machine to recognize the parts of a sentence and how they link together. Imagine you tell a friend, 'I love learning about NLP,' and want a computer to understand that 'I' is the subject and 'learning about NLP' is what you enjoy. This is precisely what syntax parsing aims to do by breaking down sentences into smaller components. This decomposition is visually represented through parse trees, which outline the sentence structure in branches showing relationships between words and phrases. Let's visualize a parse tree with our example sentence. At the top, we have the entire sentence, and then it branches out to 'I' as the noun subject and 'love learning about NLP' as the predicate, and it gets more detailed as we go further down the branches. Next up, we have sentence segmentation which is the process of figuring out where one sentence ends and another begins within a text. Think of it as the programming equivalent of taking a breath when reading aloud. While the period at the end of a sentence is a clear cue, it's not always that simple. For instance, a dot could appear in abbreviations or numbers, and that's where sentence segmentation techniques come in, using advanced methods to handle these nuances. Edge cases like 'Dr. Smith arrived at 10 a.m.' can trip up basic algorithms, so it's crucial for more sophisticated approaches to determine that this single sentence has internal punctuation. Why are these important, you ask? Well, they lay down the groundwork for more complex NLP tasks like understanding the relationships and dependencies between words or recognizing names, places, and other entities. Plus, considering your academic goals, mastering these will be critical for your NLP class. And they're not just theoretical; they're employed in systems you interact with, like chatbots, which use these techniques to comprehend and generate natural-sounding responses. On this slide, we've introduced the basics to set the stage for your journey into deeper NLP waters. With this knowledge, you're well on your way to grasp how AI manages to parse and segment text, a vital step towards actual language understanding.\",\n",
      "            \"content\": \"Definition: Syntax Parsing - the process of analyzing the structure of sentences in NLP\\nSyntax parsing allows us to decompose language into syntactic components\\nParse trees represent sentence structure in a hierarchical manner\\nExample of a simple parse tree\\nDefinition: Sentence Segmentation Techniques - determining sentence boundaries within a body of text\\nSentence segmentation sets the stage for tokenization and parsing\\nPunctuation is a primary signal for sentence boundaries\\nAdvanced techniques consider contextual and linguistic cues\\nExamples of edge cases that pose challenges to sentence segmentation\\nSyntax parsing and sentence segmentation are crucial for text analysis\\nThey lay the foundation for advanced NLP tasks like dependency parsing and named entity recognition\\nThese concepts are essential for machines to understand human language\\nSyntax parsing and sentence segmentation find applications in modern NLP applications\\nExamples include chatbots\\nThis slide introduces the foundational concepts of syntax parsing and sentence segmentation\\nIt aims to provide a core understanding of how machines interpret textual data\\nThe student's interest in NLP and academic success is considered in the presentation\\nExamples will be used to enhance understanding\\nStudents will gain an understanding of the role of syntax and sentence boundaries in NLP tasks\\nStudents will recognize the importance of syntax parsing and sentence segmentation in text analysis\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 0,\n",
      "            \"purpose_statement\": \"This slide is designed to introduce the foundational concepts of 'Syntax Parsing' and 'Sentence Segmentation Techniques' in NLP, underscoring their roles in text analysis, their importance in machine understanding of language, and how they pave the way for advanced NLP tasks such as dependency parsing and named entity recognition.\",\n",
      "            \"concepts\": [\n",
      "                \"Syntax Parsing\",\n",
      "                \"Sentence Segmentation Techniques\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Transforming Texts: Mastering Edit Distance & Regular Expressions in NLP\",\n",
      "            \"presentation\": \"Alright, let's take a deep dive into 'Transforming Texts: Mastering Edit Distance & Regular Expressions in NLP'. You've been doing great progress so far, but now it's time to solidify your knowledge of regular expressions in the context of NLP, especially their use in text normalization and edit distance calculations. So, starting with regular expressions\\u2014they're like the Swiss Army knife for text processing. The basic syntax and operators of regex are critical as they enable us to craft patterns for almost any text manipulation task you can think of. Think wildcards on steroids! You have your character classes that let you define a set of characters to match, quantifiers to specify how many times a character can repeat, and anchors to nail down the position of a pattern within the text. Imagine you're sifting through a giant haystack of text to find specific needles, like email addresses or dates, regex is your magnet. Next, see how these are utilized in NLP tasks by simplifying tokenization and lemmatization processes, which are essential for understanding the structure and meaning of the text. Now, moving on to the concept of edit distance, which is about measuring how similar or different two strings of text are. It's calculated based on the minimum number of operations\\u2014like insertions, deletions, or substitutions\\u2014needed to transform one string into another. Think of it as the least number of steps you'd need to take in order to match two patterns. This has tons of applications like spell checking or syncing text data across documents. Okay, we can enhance the efficiency of these applications by preprocessing the text with regex before applying the edit distance. For practical use, consider a chatbot interacting with users. We want it to understand the input even if it's misspelled, right? By applying regex, we can normalize user input before employing edit distance, ensuring our chatbot is not tripped up by minor typos. Let's wrap this up with some hands-on practice problems to solidify your learning. Remember, regex and edit distance are foundational NLP tools that will serve you well, not just in academic scenarios but whenever you're faced with text processing challenges. And there we have it, with a firm grip on these concepts, you're well on your way to acing your NLP class and beyond. Next, we'll tackle some problems to see these concepts in action!\",\n",
      "            \"content\": \"Slide Title: Transforming Texts: Mastering Edit Distance & Regular Expressions in NLP\\nPurpose: Explanative (3)\\n- Introduction to Regular Expressions\\n   - Syntax and operators of regex\\n   - Character classes, quantifiers, and anchors\\n   - Examples of regex application for tokenization and lemmatization\\n- Uses of Regular Expressions in NLP\\n   - Harnessing regex for text normalization\\n   - Enhancing Edit Distance algorithms through regex\\n- Introduction to Edit Distance\\n   - Measuring text similarity\\n   - Minimum operations for string transformation\\n   - Applications in spell checking and alignment algorithms\\n- Practical Examples and Exercises Using Regular Expressions\\n   - Applying regex to improve text normalization\\n   - Real-world applications and hands-on practice problems\\n- Conclusion\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 3,\n",
      "            \"purpose_statement\": \"This slide is tailored to solidify the student's foundational knowledge of Regular Expressions, focusing on their practical applications in text normalization and edit distance calculations in NLP. The content will bridge theoretical concepts with real-world tools to enhance the student's performance in an academic setting, and facilitate the application of these concepts through interactive examples and aligned practice problems.\",\n",
      "            \"concepts\": [\n",
      "                \"Regular Expressions\",\n",
      "                \"Edit Distance\",\n",
      "                \"Text Normalization\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Diving Deeper: Practical Applications of Edit Distance in NLP\",\n",
      "            \"presentation\": \"In our journey through Natural Language Processing, we've seen how edit distance helps us understand patterns in text data. Now, let's explore how it connects to the real world. Imagine you're frantically typing a message or searching online, and you mistype a word. Almost like magic, your search engine or word processor corrects it or suggests the right spelling. That's edit distance in action, right there beneath our fingertips. Algorithms based on edit distance determine the fewest number of insertions, deletions, or substitutions needed to change your mistyped word into the correct one. Now, what might surprise you is that the same principle is applied in biology, specifically in comparative genomics. Scientists compare DNA sequences from different organisms to track evolutionary changes. The smaller the edit distance between the DNA sequences, the closer the genetic relationship is. So, in a way, edit distance helps us read both the story of our languages and the story of life itself. To solidify our understanding, let's visualize it. Imagine a diagram showing the word 'recieve' transforming into 'receive' by swapping the 'i' and 'e'. In the realm of DNA, it could be a chart showcasing the alignment of sequences that might lead to groundbreaking insights about our genetic history. Isn't it fascinating how a concept from NLP can be so versatile?\",\n",
      "            \"content\": \"This slide explores the practical applications of Edit Distance in Natural Language Processing (NLP). We will focus on two key applications: spelling correction in digital text analysis and comparative genomics through DNA sequencing.\\n\\n- Spelling Correction: Edit Distance algorithms play a crucial role in word processors and search engines, providing suggestions for mistyped words. By measuring the number of insertions, deletions, and substitutions needed to transform a mistyped word into a dictionary word, Edit Distance determines the best fit suggestion.\\n\\n- Comparative Genomics: Edit Distance is used to align DNA sequences from different organisms, allowing researchers to compare and analyze similarities and differences. This provides insights into evolutionary relationships and gene functions.\\n\\nThrough the use of visual aids, such as diagrams illustrating the editing steps and DNA alignment charts, we will provide a clear and engaging understanding of Edit Distance in practical applications.\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 2,\n",
      "            \"purpose_statement\": \"This slide uncovers the significant role of Edit Distance in the field of Natural Language Processing, with a focus on real-world applications like spelling correction in digital text analysis and comparative genomics through DNA sequencing. The aim is to provide the student with an understanding of Edit Distance that bridges the gap between academic learning and practical implementation in modern computational linguistics and bioinformatics.\",\n",
      "            \"concepts\": [\n",
      "                \"Edit Distance\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Enhancing Natural Language Processing with Tokenization Techniques\",\n",
      "            \"presentation\": \"Alright, moving on to our current topic, we're going to dive into the heart of slicing and dicing text using Tokenization techniques in natural language processing. Now, Tokenization\\u2014you might wonder\\u2014is like turning a raw diamond into a polished gem. It's how we take a big block of text and break it down into smaller, more manageable pieces. Picture a long run-on sentence. Tokenization helps chop it up into words or phrases that we can easily digest and analyze individually. And these individual pieces, these tokens, lay the groundwork for all the complex tasks we aim to perform in NLP. Think about it like Lego blocks; without individual pieces, we can't build our castle. Now, regex or regular expressions are the magic wand here. They\\u2019re a series of characters that define search patterns, which we can use in Tokenization. For instance, we can craft a regex that discerns words separated by spaces or punctuation. Let's consider, for example, the common task of identifying hashtags in a tweet. With the right regex pattern, we could quickly sift through millions of tweets to find, analyze, and understand trending topics. Now, to make sure that our tokens are clean and uniform, we apply Text Normalization. This is like ironing out all the creases so that words like 'U.S.A.' and 'USA' or 'run' and 'running' are treated the same when we process them. This fine-tuning is critical for machines to treat similar words consistently, easing the path to accurate language comprehension. Then there's the Edit Distance, which is a bit like a ruler to measure how 'far apart' different words are by counting the minimum number of operations needed to transform one word into another. How does it all come together? Imagine programming your own chatbot for customer service. By applying tokenization using regex, enforcing text normalization, and understanding edit distance, you can train your bot to comprehend customer queries better and provide relevant answers. To solidify your understanding, how about we roll up our sleeves and try a couple of exercises? We'll write a regex pattern to tokenize sentences, then compare different versions of a text for practice.\",\n",
      "            \"content\": \"Enhancing Natural Language Processing with Tokenization Techniques\\n\\n- Definition and significance of Tokenization in NLP\\n- Role of Tokenization as foundation for advanced NLP tasks\\n- Use of Regular Expressions (regex) in Tokenization\\n- Practical importance of Text Normalization\\n- Introduction to Edit Distance and its correlation with Tokenization\\n- Real-world examples and exercises for hands-on practice\\n\\n\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 2,\n",
      "            \"purpose_statement\": \"This slide aims to delve into the specifics of 'Tokenization'\\u2014a fundamental concept in NLP not yet fully explored in our slide set. By exploring different tokenization techniques and their pertinence in various contexts of NLP, the student will gain a deeper understanding of how text is processed at the foundational level for more complex tasks.\",\n",
      "            \"concepts\": [\n",
      "                \"Tokenization\",\n",
      "                \"Regular Expressions\",\n",
      "                \"Text Normalization\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Enhancing Natural Language Processing with Tokenization Techniques\",\n",
      "            \"presentation\": \"Alright, let's dig into the concept of tokenization in NLP which is, in essence, the process of chopping up text into smaller units known as tokens. Now, you can think of tokens as the Lego blocks of language processing; they are the essential ingredients that we'll use to build up to more complex operations. Imagine sorting all your Lego pieces before creating your model, that's what tokenization is to NLP tasks. This foundational process allows us to analyze texts, understand them, and eventually teach our computers to do the same. So, when we talk about breaking down sentences into words or even larger text bodies, like paragraphs into sentences, we are tokenizing them. The idea here is to simplify the large chunk of text into manipulable pieces that we can then study and work with. Regular Expressions, or Regex, are like smart scissors in the toolkit of tokenization. They allow us to cut the text in very specific patterns and are particularly helpful when dealing with various languages and complex text structures. By mastering Regex, you'll be able to wield a powerful tool that can cut through text in the exact way you need. Next up is text normalization, which is kind of like preparing the Lego pieces before using them. It's about making text uniform, making sure everything is in the right form and format to be processed efficiently. This could be something as simple as converting all characters to lowercase, or stripping away extraneous punctuation, ensuring all tokens are consistent and ready for action. Let's translate this into real-world scenarios. Consider chatbots; they rely heavily on tokenization to understand and respond to user queries. Every time you ask a bot a question, it's using tokenization to break down your sentence and make sense of it to provide a relevant response. Or take automated translation systems, which tokenize sentences into words or phrases before translating them into another language. What we're aiming for here is for you to walk away with the ability to not just grasp the theory but also jump into applying these concepts practically in both academia and state-of-the-art NLP systems. Remember, the tools and techniques you're learning can be readily applied to the projects and assignments you'll encounter in your NLP class, setting you up for success in your course assessments and beyond.\",\n",
      "            \"content\": \"Main Points:\\n\\n- Tokenization is a fundamental concept in NLP\\n- Tokenization breaks down text into smaller units called tokens\\n- Tokens serve as the building blocks for NLP operations\\n- Regular Expressions are powerful tools for tokenization\\n- Text Normalization complements tokenization\\n- Practical examples of tokenization techniques\\n- Importance of understanding context in tokenization\\n- Applications of tokenization in NLP tasks\\n\\nSupporting Examples:\\n\\n- Breaking down sentences into words\\n- Breaking down paragraphs into sentences\\n- Using regular expressions to tokenize text\\n- Text normalization techniques like converting to lowercase and removing punctuation\\n- Examples of tokenization used in chatbots and automated translation systems\\n\\nEnd Goal:\\n\\n- Solid understanding of tokenization and its applications in NLP\\n- Ability to apply tokenization concepts in real-world and academic scenarios\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 2,\n",
      "            \"purpose_statement\": \"This slide aims to delve into the specifics of 'Tokenization'\\u2014a fundamental concept in NLP not yet fully explored in our slide set. By exploring different tokenization techniques and their pertinence in various contexts of NLP, the student will gain a deeper understanding of how text is processed at the foundational level for more complex tasks.\",\n",
      "            \"concepts\": [\n",
      "                \"Tokenization\",\n",
      "                \"Regular Expressions\",\n",
      "                \"Text Normalization\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Navigating the Grammar of Language: part-of-speech tagging in NLP\",\n",
      "            \"presentation\": \"So far, we've delved into the fascinating world of Natural Language Processing, or NLP, and examined how computers process and analyze large volumes of textual data. Now, let's navigate further into the intricate details of language structure with Slide 10, where we'll discuss part-of-speech tagging, a critical step in understanding the grammar of text in NLP.\\n\\nPart-of-speech tagging, essentially, assigns words in a text to grammatical categories like nouns, verbs, adjectives, and more. By doing this, we start to unravel the sentence structure, helping computers comprehend the roles different words play in a sentence. It's like giving each word a specific identity badge that says, 'Hey, I'm a verb, and I'm crucial to the action taking place here!'\\n\\nUnderstanding the connection between POS tagging and sentence structure is like looking at a scaffold that holds the building blocks of language. It determines how words come together to form coherent meaning. For example, in the sentence, 'The quick brown fox jumps over the lazy dog,' POS tagging would identify 'jumps' as a verb, which is the action taking place. Knowing this helps in many NLP tasks, including language translation and sentiment analysis.\\n\\nNow, imagine you are designing a chatbot, and you want it to understand and respond to customer inquiries effectively. This is where POS tagging comes in handy. It helps the chatbot to identify the key action words and respond appropriately. The bot needs to distinguish queries from commands or greetings from feedback to interact effectively, right?\\n\\nBefore diving into POS tagging, it's crucial to talk about tokenization. Think of it as chopping up a sentence into individual pieces, like cutting a string of beads into separate beads. This step is vital because it sets everything up for assigning those grammatical badges we talked about.\\n\\nRegular Expressions are like secret codes or patterns that can help recognize text efficiently. They can expedite the POS tagging process by picking out recurring patterns corresponding to different parts of speech. It's a bit like a word detective recognizing suspects based on certain clues.\\n\\nAnd let's not forget Syntax Parsing. This goes beyond POS tagging\\u2014it's like creating a family tree for a sentence, showing who's related to whom, and how they're connected grammatically. Syntax parsing reveals the hierarchy of words and phrases, further unraveling the complexity of language.\\n\\nChallenges? Of course, there are. Sometimes words are like chameleons; they change their grammatical role based on the context. In such cases, POS tagging can be tricky, and this is where knowing the context becomes crucial for accurate tagging.\\n\\nAll of these elements, from tokenization to syntax parsing, demonstrate the relevance of POS tagging to a broad range of NLP tasks, including text normalization, where we clean and prepare text for analysis, and, of course, synthesis or grammar analysis.\\n\\nLastly, to help solidify these concepts, let's look at some examples and visuals demonstrating POS tagging in action. By understanding these concepts and seeing them applied, you're not just learning for your NLP class, but you're gaining tools that are used in real-world NLP applications, like developing sophisticated chatbots or even improving search engine algorithms.\\n\\nWith the foundation from today's slide, you'll be well-equipped to dive into more advanced NLP studies, and go from understanding the basic grammar of language to crafting systems that can interact and interpret human language much like we do. Alright, let's move on and see how all these pieces of language puzzles you're learning come together in the world of NLP.\",\n",
      "            \"content\": \"\\u2022 Defining Part-of-Speech (POS) tagging as the process of assigning words to their grammatical categories\\n\\u2022 Understanding the connection between POS tagging and sentence structure\\n\\u2022 Exploring the application of POS tagging in real-world text analysis challenges\\n\\u2022 Highlighting the significance of Tokenization of Text as a preliminary step in POS tagging\\n\\u2022 Leveraging Regular Expressions for pattern recognition in POS tagging\\n\\u2022 Introducing Syntax Parsing to decompose sentences and reveal grammatical relationships\\n\\u2022 Addressing challenges in POS tagging, including handling ambiguous cases\\n\\u2022 Demonstrating the relevance of POS tagging to tasks like text normalization and syntax parsing\\n\\u2022 Providing examples and visuals to illustrate POS tagging in action\\n\\u2022 Ensuring a solid foundation for advanced studies in Natural Language Processing (NLP)\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 2,\n",
      "            \"purpose_statement\": \"This slide aims to deepen the student\\u2019s understanding of the part-of-speech tagging process within the broader scope of NLP, demonstrating the connection between tagging, sentence structure, and linguistic meaning, as well as its application in real-world text analysis challenges.\",\n",
      "            \"concepts\": [\n",
      "                \"part-of-speech tagging\",\n",
      "                \"Natural Language Processing (NLP)\",\n",
      "                \"Tokenization of Text\",\n",
      "                \"Syntax Parsing\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Practical Insights into Lemmatization and Sentence Segmentation in NLP\",\n",
      "            \"presentation\": \"So far, we have built a solid grasp of Natural Language Processing's theoretical framework. Now, let's make that knowledge more concrete with today's focus on lemmatization and sentence segmentation. Picture yourself working on a sophisticated machine translation system; understanding these concepts is key to making it function accurately. Lemmatization is about stripping down words to their core, their lemma. Think of all the different forms of the verb 'to be': 'am,' 'is,' 'are,' 'was,' 'were,' and so forth. In lemmatization, they all transform to 'be,' making it easier for the computer to process underlying meanings. Let me give you another example, the words 'running,' 'runs,' and 'ran' all link back to 'run.' By normalizing these to their base form, an NLP system can maintain the semantic integrity of the text it analyzes. Now, sentence segmentation is equally intriguing. It's the process of carving texts into digestible sentences, which is vital before any advanced NLP task can take place. It sounds straightforward when you think of periods, question marks, or exclamation points as signals for sentence boundaries. But what about 'Dr.' or 'U.S.A.'? Here, periods don't imply the end of a sentence. So, we employ algorithms to decipher these nuances, considering the context and specific linguistic rules. To solidify your understanding, you'll work on some text exercises where you'll practice identifying sentence boundaries and applying lemmatization yourself. And as we progress, keep in mind how these elements are essential in creating fluent chatbots, smooth search engines, and other NLP features you interact with daily. By the end of this session, the goal is for you to not only grasp the theory behind lemmatization and segmentation but also to see them in action, bridging the gap between your academic studies and the real-world applications you're passionate about.\",\n",
      "            \"content\": \"The slide titled 'Practical Insights into Lemmatization and Sentence Segmentation in NLP' aims to present an examplative perspective on these two fundamental concepts within the field of Natural Language Processing (NLP). The main goal is to deepen the student's theoretical knowledge through carefully selected practical examples that illustrate how lemmatization and sentence segmentation are essential in real-world NLP applications, such as machine translation and information retrieval systems.\\n\\n- Definition of lemmatization as the process of reducing words to their lemma or base form\\n- Examples demonstrating the conversion of different inflected forms of a word to its base lemma\\n- Explanation of sentence segmentation techniques to identify sentence boundaries\\n- Examples of standard and complex cases where sentence boundaries are determined\\n- Engaging exercises on applying lemmatization and sentence segmentation to real-language data snippets\\n\\nThe content will be organized to ensure a logical flow from theory to application. Real-world relevance will be established by tying examples and exercises to applications like chatbots or other common NLP systems. By the end of the slide, the student should have a reinforced understanding of lemmatization and sentence segmentation, accompanied by practical insights that offer a glimpse into their application in modern NLP.\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 4,\n",
      "            \"purpose_statement\": \"To consolidate the student's theoretical understanding of Lemmatization and Sentence Segmentation Techniques through practical examples and exercises, demonstrating their vital roles in NLP applications such as machine translation and information retrieval systems.\",\n",
      "            \"concepts\": [\n",
      "                \"Lemmatization\",\n",
      "                \"Sentence Segmentation Techniques\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Advanced Utilization of Regular Expressions in NLP\",\n",
      "            \"presentation\": \"We've laid a solid foundation thus far, but let's dive deeper into the world of Regular Expressions, particularly how they're wielding their power in Natural Language Processing. Now, imagine you're sifting through a large dataset of social media posts to analyze sentiment. Without refined tools, it's like looking for a needle in a haystack. This is where our advanced regex skills come into play. Let's start by talking about lookahead and lookbehind assertions. These are like your text detective's magnifying glass, allowing you to match a string that is followed or preceded by another string without including it in the match. For instance, if we're interested in words that are often mentioned before a brand name, lookahead will be our go-to tool. Now, capture groups and backreferences take our pattern matching to a new level of sophistication, akin to having bookmarks in a novel. They allow us to remember and reuse parts of the matched string, which is incredibly handy for tasks like rearranging date formats or extracting specific data points. Non-capture groups are the unsung heroes here, they let us use part of the regex for grouping without storing it, keeping our matches clean and efficient. Speaking of efficiency, remember coding your first regex pattern? It probably felt like it worked magic until it came across a massive wall of text. Minimizing backtracking \\u2013 essentially, reducing the steps the regex engine takes \\u2013 and using non-greedy quantifiers, which match as little as possible, ensure our patterns are not just powerful but also swift and resource-conscious. Now, let's put theory into action with some coding. Consider chatbots; they need to parse human input efficiently. With regex, we can swiftly identify intent and extract information by recognizing patterns in natural language. Sentiment analysis is another domain where regex shines. By identifying keywords and their modifiers, we can categorize text as positive, negative, or neutral at scale. And let's not forget about the practical hands-on exercises. Applying these advanced regex concepts in real-world NLP tasks will not only reinforce your understanding but will also prepare you to tackle any complex text parsing challenge with confidence. So, ready to start coding some patterns and see these concepts light up in action?\",\n",
      "            \"content\": \"Deepening understanding of Regular Expressions in NLP\\n- Introduction to advanced concepts in Regular Expressions\\n  - Lookahead and lookbehind assertions\\n  - Capturing groups and backreferences\\n  - Non-capture groups\\n- Efficiency considerations in text parsing\\n  - Minimizing backtracking\\n  - Using non-greedy quantifiers\\n- Practical applications of Regular Expressions in NLP\\n  - Text normalization\\n  - Pattern recognition\\n- Bridging foundational regex knowledge to NLP tasks\\n- Building on primary regex concepts from chapter summary\\n- Examples of practical usage in modern NLP applications\\n  - Chatbots\\n  - Sentiment analysis\\n- Hands-on exercises and demonstrations\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 3,\n",
      "            \"purpose_statement\": \"To expand the student's knowledge of Regular Expressions beyond basics, covering advanced pattern matching and efficiency in text parsing, and to provide practical applications in NLP.\",\n",
      "            \"concepts\": [\n",
      "                \"Regular Expressions\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Mastery Through Practice: Regular Expressions in Real-World NLP Applications\",\n",
      "            \"presentation\": \"So far, we've laid a solid foundation on Regular Expressions in Natural Language Processing, and now it's time to solidify that knowledge through practical application. Picture this: you're working with a massive amount of text data, and you need to quickly find specific information like email addresses, or perhaps you're pre-processing text data to help your computer understand human language better. This is where our hero, Regular Expressions, comes to the rescue, saving time and making your NLP tasks much more manageable. Let's dig into some real-world applications you might encounter, especially in your NLP course or even in developing tomorrow's AI. For starters, consider a common NLP task such as text normalization. With regex, you can design patterns to systematically extract meaningful pieces from a labyrinth of words or even transform text into a uniform format. Tokenization, which chops a text into bite-sized pieces called tokens, and sentence segmentation techniques are also an arena where regex skills are invaluable. Think about our chatbot example; using regex, the bot could effortlessly identify different types of user input to keep the conversation flowing smoothly. Now, let's try something hands-on to demonstrate these concepts. Imagine you're tasked to create a chatbot for a local event. Your first challenge is to design regex patterns so that whenever someone mentions the word 'schedule,' the bot understands they\\u2019re looking for event times, not locations or tickets. Try this: write a regex pattern that finds any variations of the word 'schedule' \\u2013 be it 'schedules', 'scheduled', or just 'schedule'. And remember, NLP isn't just about finding matches; it's about ironing out the differences too. That's where the Edit Distance Algorithm comes in, helping us determine how similar or different two strings of text are, which is quite crucial in tasks such as spell checkers or DNA sequencing. Every concept we've covered \\u2013 from Regular Expressions to Edit Distance \\u2013 isn't just academic; they're the building blocks for complex NLP systems that are changing the world as we know it. As we wrap up, I encourage you to reflect on how these techniques can be applied in real NLP projects and stay curious about the potential that these skills unlock for future innovations.\",\n",
      "            \"content\": \"Slide Title: Mastery Through Practice: Regular Expressions in Real-World NLP Applications\\n\\n- Regular Expressions in NLP:\\n  - Introduction to Regular Expressions\\n    - Efficient search patterns within strings\\n    - Character classes, quantifiers, basic syntax\\n  - Uses of Regular Expressions in NLP\\n    - Text normalization\\n    - Tokenization of text\\n    - Sentence segmentation techniques\\n  - Practical Examples and Exercises Using Regular Expressions\\n    - Filtering data from logs\\n    - Extracting dates and entities from unstructured text\\n    - Preprocessing for sentiment analysis\\n  - Applications of Edit Distance Algorithm in NLP\\n    - Alignment of strings and calculating edit distance\\n  - Hands-on Practice Problems\\n    -Designing patterns for chatbot communication\\n    - Identifying intents and extracting information from user input\\n  - Foundational Understanding of Main Points\\n  - Academic Application of Concepts\\n  - Theoretical Basis Followed by Practical Application\\n- Call to Action:\\n  - Reflect on the practical importance of Regular Expressions in NLP\\n  - Apply gained understanding in developing advanced NLP systems\\n\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 4,\n",
      "            \"purpose_statement\": \"This slide aims to reinforce and enhance the student's theoretical understanding of Regular Expressions through curated examples and exercises, focusing on their implementation in real-world Natural Language Processing applications.\",\n",
      "            \"concepts\": [\n",
      "                \"Regular Expressions\",\n",
      "                \"Natural Language Processing (NLP)\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Synergy of Patterns and Distance: Regular Expressions Meet Edit Distances in NLP\",\n",
      "            \"presentation\": \"So far, we've laid a strong foundation in understanding how computers process human language. Now, let\\u2019s dive into something that's akin to a Swiss Army knife in the world of text processing: Regular Expressions, commonly known as regex. Regular Expressions are essentially patterns that help us in identifying, matching, and managing text. For example, they make it possible to find all email addresses in a document or replace every occurrence of 'color' with 'colour' to adapt to British English spelling. Think of it like a special code that hunts down the exact text you need in a sea of words. Moving on, the rule book for writing these patterns is what we refer to as the basic syntax and operators of Regular Expressions. It\\u2019s composed of various characters that, when combined, can create powerful search patterns. For instance, the asterisk (*) is used to indicate that the preceding element can be matched zero or more times, which is really useful in dealing with variations in text. To bring this to life, imagine you're programming a chatbot for your NLP class project. With Regular Expressions, you can ensure it understands different user input formats and can respond accurately. Next up is Text Normalization. This is where we polish the text to a form that our programs or algorithms can more easily process. It involves several key tasks: tokenization - which cuts text into pieces like words or sentences, lemmatization - boiling down a word to its basic form, and sentence segmentation - identifying and separating sentences. Transitioning to Edit Distance, this is a fascinating concept that measures how alike or different two strings of text are. It can help our chatbot understand that when someone types 'recieve', they likely meant 'receive', and help correct it. We quantify this similarity or dissimilarity by calculating how many single-character edits are needed to change one word into another. Putting these tools into action is where the magic happens. As a computational linguist in the making, you will use these techniques not just to pass your NLP class, but to potentially craft more intuitive and mistake-tolerant applications. To wrap up, let's engage with some hands-on practice problems that will test your understanding of these concepts from theory to implementation. Always remember, mastering these skills are steps toward achieving the fluency in the language of NLP. Are you ready for the challenge?\",\n",
      "            \"content\": \"- Introduction to Regular Expressions\\n- Uses of Regular Expressions in NLP\\n- Basic Syntax and Operators of Regular Expressions\\n- Practical Examples and Exercises Using Regular Expressions\\n- Introduction to Text Normalization\\n- Tokenization of Text\\n- Lemmatization and its Importance\\n- Sentence Segmentation Techniques\\n- Introduction to Edit Distance\\n- Applications of Edit Distance Algorithm in NLP\\n- Calculation of Edit Distance and String Alignment\\n- Practical Examples and Exercises in Modern NLP Applications (e.g., Chatbots)\\n- Hands-on Practice Problems\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 3,\n",
      "            \"purpose_statement\": \"To explain the synergistic relationship between Regular Expressions and Edit Distance in the context of Natural Language Processing (NLP) and demonstrate their application in text analysis, error detection, and correction.\",\n",
      "            \"concepts\": [\n",
      "                \"Regular Expressions\",\n",
      "                \"Edit Distance\",\n",
      "                \"Text Normalization\"\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"current_obj_idx\": 0,\n",
      "    \"num_slides\": 14\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "### Slide generation from AITutor\n",
    "notebank = NoteBank()\n",
    "[notebank.add_note(n) for n in current_plan.split(\"\\n\")]\n",
    "slide_planner = SlidePlanner(notebank, concept_db)\n",
    "# Check if the file exists\n",
    "if os.path.exists(f\"Research/temp_data/temp_slideplan_{current_topic}.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(f\"Research/temp_data/temp_slideplan_{current_topic}.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        slide_plans = pkl.load(f)\n",
    "        print(\"\\n\\n\".join([str(slide.format_json()) for slide in slide_plans]))\n",
    "        slide_planner.SlidePlans = slide_plans\n",
    "else:\n",
    "    slide_planner.generate_slide_plan()\n",
    "    with open(f\"Research/temp_data/temp_slideplan_{current_topic}.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(slide_planner.SlidePlans, f)\n",
    "\n",
    "if os.path.exists(f\"Research/temp_data/temp_slides_{current_topic}.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(f\"Research/temp_data/temp_slides_{current_topic}.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        slides = pkl.load(f)\n",
    "        slide_planner.Slides = slides\n",
    "else:\n",
    "    slide_planner.generate_slide_deque()\n",
    "    with open(f\"Research/temp_data/temp_slides_{current_topic}.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(slide_planner.Slides, f)\n",
    "import json\n",
    "print(json.dumps(slide_planner.format_json(), indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "- Preprocessing\n",
    "- Generation of questions from (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Introduction to Natural Language Processing (NLP) and Regular Expressions', 'purpose': 0, 'purpose_statement': 'To give the student an initial overview of NLP with a focus on Regular Expressions and how they play a foundational role in text analysis.', 'concepts': ['Natural Language Processing (NLP)', 'Regular Expressions', 'Text Normalization', 'Tokenization of Text', 'Edit Distance']}\n",
      "\n",
      "{'title': 'Decoding Patterns with Regular Expressions in NLP', 'purpose': 0, 'purpose_statement': 'This slide will serve as an initial deep dive into the world of Regular Expressions, enabling students to understand their syntax, basic operators, and fundamental uses in NLP, setting the groundwork for more sophisticated text processing tasks.', 'concepts': ['Regular Expressions']}\n",
      "\n",
      "{'title': 'Discovering Lemmatization: Enhancing Text Analysis in NLP', 'purpose': 0, 'purpose_statement': \"This slide aims to introduce Lemmatization as an essential NLP text preprocessing technique, building on the student's understanding of Text Normalization and Tokenization, to further enhance their text analysis skills.\", 'concepts': ['Lemmatization', 'Text Normalization', 'Tokenization of Text']}\n",
      "\n",
      "{'title': 'Engaging with Part-of-Speech Tagging in NLP', 'purpose': 0, 'purpose_statement': 'To introduce the foundational concept of part-of-speech tagging within Natural Language Processing (NLP), elucidating its definition, importance, and the role it plays in understanding the grammatical structure of language for text analysis and other NLP applications.', 'concepts': ['part-of-speech tagging', 'Tokenization of Text', 'Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Unveiling the Layers of Language: Syntax Parsing and Sentence Segmentation', 'purpose': 0, 'purpose_statement': \"This slide is designed to introduce the foundational concepts of 'Syntax Parsing' and 'Sentence Segmentation Techniques' in NLP, underscoring their roles in text analysis, their importance in machine understanding of language, and how they pave the way for advanced NLP tasks such as dependency parsing and named entity recognition.\", 'concepts': ['Syntax Parsing', 'Sentence Segmentation Techniques']}\n",
      "\n",
      "{'title': 'Transforming Texts: Mastering Edit Distance & Regular Expressions in NLP', 'purpose': 3, 'purpose_statement': \"This slide is tailored to solidify the student's foundational knowledge of Regular Expressions, focusing on their practical applications in text normalization and edit distance calculations in NLP. The content will bridge theoretical concepts with real-world tools to enhance the student's performance in an academic setting, and facilitate the application of these concepts through interactive examples and aligned practice problems.\", 'concepts': ['Regular Expressions', 'Edit Distance', 'Text Normalization']}\n",
      "\n",
      "{'title': 'Diving Deeper: Practical Applications of Edit Distance in NLP', 'purpose': 2, 'purpose_statement': 'This slide uncovers the significant role of Edit Distance in the field of Natural Language Processing, with a focus on real-world applications like spelling correction in digital text analysis and comparative genomics through DNA sequencing. The aim is to provide the student with an understanding of Edit Distance that bridges the gap between academic learning and practical implementation in modern computational linguistics and bioinformatics.', 'concepts': ['Edit Distance']}\n",
      "\n",
      "{'title': 'Enhancing Natural Language Processing with Tokenization Techniques', 'purpose': 2, 'purpose_statement': \"This slide aims to delve into the specifics of 'Tokenization'â€”a fundamental concept in NLP not yet fully explored in our slide set. By exploring different tokenization techniques and their pertinence in various contexts of NLP, the student will gain a deeper understanding of how text is processed at the foundational level for more complex tasks.\", 'concepts': ['Tokenization', 'Regular Expressions', 'Text Normalization']}\n",
      "\n",
      "{'title': 'Enhancing Natural Language Processing with Tokenization Techniques', 'purpose': 2, 'purpose_statement': \"This slide aims to delve into the specifics of 'Tokenization'â€”a fundamental concept in NLP not yet fully explored in our slide set. By exploring different tokenization techniques and their pertinence in various contexts of NLP, the student will gain a deeper understanding of how text is processed at the foundational level for more complex tasks.\", 'concepts': ['Tokenization', 'Regular Expressions', 'Text Normalization']}\n",
      "\n",
      "{'title': 'Navigating the Grammar of Language: part-of-speech tagging in NLP', 'purpose': 2, 'purpose_statement': 'This slide aims to deepen the studentâ€™s understanding of the part-of-speech tagging process within the broader scope of NLP, demonstrating the connection between tagging, sentence structure, and linguistic meaning, as well as its application in real-world text analysis challenges.', 'concepts': ['part-of-speech tagging', 'Natural Language Processing (NLP)', 'Tokenization of Text', 'Syntax Parsing']}\n",
      "\n",
      "{'title': 'Practical Insights into Lemmatization and Sentence Segmentation in NLP', 'purpose': 4, 'purpose_statement': \"To consolidate the student's theoretical understanding of Lemmatization and Sentence Segmentation Techniques through practical examples and exercises, demonstrating their vital roles in NLP applications such as machine translation and information retrieval systems.\", 'concepts': ['Lemmatization', 'Sentence Segmentation Techniques']}\n",
      "\n",
      "{'title': 'Advanced Utilization of Regular Expressions in NLP', 'purpose': 3, 'purpose_statement': \"To expand the student's knowledge of Regular Expressions beyond basics, covering advanced pattern matching and efficiency in text parsing, and to provide practical applications in NLP.\", 'concepts': ['Regular Expressions']}\n",
      "\n",
      "{'title': 'Mastery Through Practice: Regular Expressions in Real-World NLP Applications', 'purpose': 4, 'purpose_statement': \"This slide aims to reinforce and enhance the student's theoretical understanding of Regular Expressions through curated examples and exercises, focusing on their implementation in real-world Natural Language Processing applications.\", 'concepts': ['Regular Expressions', 'Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Synergy of Patterns and Distance: Regular Expressions Meet Edit Distances in NLP', 'purpose': 3, 'purpose_statement': 'To explain the synergistic relationship between Regular Expressions and Edit Distance in the context of Natural Language Processing (NLP) and demonstrate their application in text analysis, error detection, and correction.', 'concepts': ['Regular Expressions', 'Edit Distance', 'Text Normalization']}\n",
      "{\n",
      "    \"questions\": [\n",
      "        {\n",
      "            \"subject\": 1,\n",
      "            \"type\": 3,\n",
      "            \"data\": {\n",
      "                \"data\": \"In the context of processing tweets or social media text, write a Python function `extract_hashtags` that takes a string as an input and returns a list of hashtags in the text. You are required to use regular expressions in your solution. Consider that a hashtag is defined as a string that begins with a hash symbol (#) and is followed by alphanumeric characters without spaces.\",\n",
      "                \"boilerplate\": \"def extract_hashtags(text):\\n    # TODO: Your code here\\n    pass\",\n",
      "                \"test_cases_script\": \"assert extract_hashtags(\\\"Loving the #AI and #NLP talks at the conference!\\\") == [\\\"#AI\\\", \\\"#NLP\\\"]\\nassert extract_hashtags(\\\"This is a #great_day!\\\") == [\\\"#great_day\\\"]\\nassert extract_hashtags(\\\"Hello world!\\\") == []\",\n",
      "                \"concepts\": [\n",
      "                    \"Regular Expressions\",\n",
      "                    \"Applications of Regular Expressions in NLP\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"subject\": 1,\n",
      "            \"type\": 0,\n",
      "            \"data\": {\n",
      "                \"data\": \"Implement a function `count_vowels` in Python that takes a string input and returns the number of vowels in the string. Consider the following vowels: a, e, i, o, u, and their uppercase counterparts.\",\n",
      "                \"rubric\": \"Rubric: [1 Point] Correct implementation of the function; [1 Point] Correct count of vowels; [1 Point] Correct handling of uppercase vowels; [1 Point] Clean, readable code with appropriate comments.\",\n",
      "                \"concepts\": [\n",
      "                    \"Regular Expressions\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"subject\": 1,\n",
      "            \"type\": 3,\n",
      "            \"data\": {\n",
      "                \"data\": \"Write a Python function to calculate the Edit Distance between two strings using the Levenshtein distance algorithm. Your function should take two arguments (string1, string2) and return the minimum number of operations required to convert string1 into string2. You can limit your solution to insertions, deletions, and substitutions as the allowed edit operations.\",\n",
      "                \"boilerplate\": \"def edit_distance(str1, str2):\\n    # Your code here\\n    pass\",\n",
      "                \"test_cases_script\": \"assert edit_distance('kitten', 'sitting') == 3\\nassert edit_distance('intention', 'execution') == 5\\nassert edit_distance('algorithm', 'altruistic') == 6\",\n",
      "                \"concepts\": [\n",
      "                    \"Edit Distance\",\n",
      "                    \"Applications of Edit Distance Algorithm in NLP\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"subject\": 1,\n",
      "            \"type\": 3,\n",
      "            \"data\": {\n",
      "                \"data\": \"Write a Python function named `lemmatize_tokens` that takes a string as input and returns a list of lemmatized tokens. Your implementation should tokenize the text and then apply lemmatization to each token.\",\n",
      "                \"boilerplate\": \"def lemmatize_tokens(text):\\n    # Your code here\\n    # You may use NLTK library for tokenizing and lemmatization\\n\\n    pass  # Replace with your implementation\",\n",
      "                \"test_cases_script\": \"sample_text = \\\"The striped bats are hanging on their feet for best\\\"\\nassert lemmatize_tokens(sample_text) == [\\\"The\\\", \\\"striped\\\", \\\"bat\\\", \\\"be\\\", \\\"hanging\\\", \\\"on\\\", \\\"their\\\", \\\"foot\\\", \\\"for\\\", \\\"best\\\"], \\\"Test case failed!\\\"\",\n",
      "                \"concepts\": [\n",
      "                    \"Natural Language Processing (NLP)\",\n",
      "                    \"Tokenization of Text\",\n",
      "                    \"Lemmatization\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"subject\": 1,\n",
      "            \"type\": 3,\n",
      "            \"data\": {\n",
      "                \"data\": \"Write a Python function that uses regular expressions to count the number of times a specific word appears in a given text.\",\n",
      "                \"boilerplate\": \"import re\\n\\ndef count_word_occurrences(word, text):\\n\\t# TODO: Complete the function\\n\\tpass\",\n",
      "                \"test_cases_script\": \"text = '''Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum auctor euismod nunc, nec lacinia libero ultrices nec. Nulla vitae sagittis ipsum. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Morbi vitae neque viverra, euismod nisl in, tristique odio. Nulla facilisi. Sed vehicula lorem massa, a ultrices quam iaculis et. Donec pretium viverra accumsan. Proin in lorem velit. Integer nec interdum enim, sit amet condimentum libero. Phasellus sagittis, sapien sed dapibus imperdiet, purus metus bibendum mauris, quis congue ipsum eros quis nibh.'''\\n\\nassert count_word_occurrences('Lorem', text) == 1\\nassert count_word_occurrences('semper', text) == 0\\nassert count_word_occurrences('amet', text) == 2\",\n",
      "                \"concepts\": [\n",
      "                    \"Regular Expressions\",\n",
      "                    \"Uses of Regular Expressions in NLP\",\n",
      "                    \"Basic Syntax and Operators of Regular Expressions\"\n",
      "                ],\n",
      "                \"teaching_note\": \"Regular expressions are a powerful tool for pattern matching in text. In this question, you will use regular expressions to count the number of times a specific word appears in a given text. Remember to use the re module in Python to access the regular expression functions. You can use the re.findall() function to find all occurrences of the word in the text and then use the len() function to count the number of matches.\"\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"current_obj_idx\": -1,\n",
      "    \"num_questions\": 5\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "### Slide generation from AITutor\n",
    "notebank = NoteBank()\n",
    "[notebank.add_note(n) for n in current_plan.split(\"\\n\")]\n",
    "q_suite = QuestionSuite(5, notebank, concept_db)\n",
    "# Check if the file exists\n",
    "if os.path.exists(f\"Research/temp_data/temp_questions_{current_topic}.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(f\"Research/temp_data/temp_questions_{current_topic}.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        questions = pkl.load(f)\n",
    "        print(\"\\n\\n\".join([str(slide.format_json()) for slide in slide_plans]))\n",
    "        q_suite.Questions = questions\n",
    "else:\n",
    "    q_suite.generate_question_data()\n",
    "    with open(f\"Research/temp_data/temp_questions_{current_topic}.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(q_suite.Questions, f)\n",
    "        \n",
    "print(json.dumps(q_suite.format_json(), indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERRORS\n",
    "\n",
    "\n",
    "the thing we are checking for errors is number of api calls per errors. api calls during translation / errors during translation\n",
    "gpt-4 and gpt-3.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCEPTS RATIO OF NUMBER OF RELEVANT CONCEPTS OVER NUMBER OF CONCEPS\n",
    "GPT-3.5 vs GPT-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
