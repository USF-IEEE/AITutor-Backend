{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricizing LLMaAiTB-E\n",
    "\n",
    "- Our Focus: Generation Quality\n",
    "- Measurement Techniques: \n",
    "    - Vector Comparison\n",
    "    - Human Preference Sample (A/B/C Testing)\n",
    "    - ???\n",
    "- Iterative documents to measure:\n",
    "    - Concepts (Generation phase)\n",
    "    - Slides (Teaching phase)\n",
    "    - Questions (Testing phase)\n",
    "- Resources for testing:\n",
    "    - Expert (From classes)\n",
    "    - GPT4 (Generation)\n",
    "    - LLMaAiTB-E (Teachabull)\n",
    "- Main concepts to cover:\n",
    "    - Object Oriented Programming\n",
    "    - Programming Language Semantics\n",
    "    - Math\n",
    "    - History\n",
    "    - \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Helper Functions\n",
    "We will demonstrate our metrics using OpenAI's Vector Embeddings on our generated documents. We decided to use OpenAI's embeddings due to their large document size capacity. We agreed that this method would prove to be the best while comparing large documents.\n",
    "\n",
    "## LLM Prompt/Text Completion\n",
    "\n",
    "\n",
    "## Vector Comparison\n",
    "Embeddings: OpenAIâ€™s text embeddings measure the relatedness of text strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import myenv\n",
    "import os\n",
    "import pickle as pkl\n",
    "from AITutor_Backend.src.TutorUtils.concepts import *\n",
    "from AITutor_Backend.src.TutorUtils.notebank import NoteBank\n",
    "from AITutor_Backend.src.TutorUtils.slides import SlidePlan, Slide, SlidePlanner, Purpose\n",
    "from AITutor_Backend.src.TutorUtils.questions import Question, QuestionSuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPENAI HELPER FUNCTIONS \n",
    "def request_output_from_llm(prompt, model: str):\n",
    "    \"\"\"Requests the Concept information from an LLM.\n",
    "\n",
    "    Args:\n",
    "        prompt: (str) - string to get passed to the model\n",
    "        model: (str) - \n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI() \n",
    "    \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "    {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": prompt,\n",
    "    },\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=8000,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Embeddings (1, 1536): [[-0.00125975  0.00951579  0.0048352  ... -0.01831474 -0.00588236\n",
      "  -0.04087433]]\n",
      "Cosine Similarity: 1.0\n"
     ]
    }
   ],
   "source": [
    "### Vector Functions\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "def tokenizer(text):\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    return tokens\n",
    "\n",
    "def process_in_batches(tokens, batch_size=8000):\n",
    "    for i in range(0, len(tokens), batch_size):\n",
    "        yield tokens[i:i + batch_size]\n",
    "\n",
    "def create_embeddings(text):\n",
    "    tokens = tokenizer(text)\n",
    "    embeddings = []\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")  # Reuse the encoding for decoding\n",
    "\n",
    "    for token_batch in process_in_batches(tokens):\n",
    "        # Convert token batch back to string\n",
    "        batch_text = encoding.decode(token_batch)\n",
    "        batch_embedding = get_embedding(batch_text)\n",
    "        embeddings.append(batch_embedding)\n",
    "\n",
    "    return np.mean([embeddings], axis=1)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n",
    "# Test works\n",
    "embeddings = create_embeddings(\"Research/generation_data/slides/Expert/codingSlides_expert.json\")\n",
    "print(f\"Vector Embeddings {embeddings.shape}:\", embeddings, )\n",
    "\n",
    "# Test = 1\n",
    "vec1 = np.array([1, 2, 3])\n",
    "vec2 = np.array([2, 4, 6])\n",
    "similarity = cosine_similarity(vec1, vec2)\n",
    "print(f\"Cosine Similarity: {similarity}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts\n",
    "- Preprocessing\n",
    "- Generation of Graph for (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notebanks from AI Tutor\n",
    "current_topic = \"NLP\"\n",
    "main_concept = \"Natural Language Processing: Regular Expressions, Text Normalization, Edit Distance\"\n",
    "tutor_plan_nlp = '''Main Concept: Regular Expressions, Text Normalization, Edit Distance in Natural Language Processing (NLP)\n",
    "Student is a computer science student with no prior knowledge of the topic, requiring an introductory lesson.\n",
    "Student is taking an NLP class, suggesting the lessons are for academic purposes and should cover necessary conceptual detail.\n",
    "Student provided a chapter summary that includes key subtopics; this will be a guide in structuring the lesson plan.\n",
    "Tutor shall educate on the following concepts:\n",
    "Subconcept: Introduction to Regular Expressions\n",
    "Subconcept: Uses of Regular Expressions in NLP\n",
    "Subconcept: Basic Syntax and Operators of Regular Expressions\n",
    "Subconcept: Practical Examples and Exercises Using Regular Expressions\n",
    "Subconcept: Introduction to Text Normalization\n",
    "Subconcept: Tokenization of Text\n",
    "Subconcept: Lemmatization and its Importance\n",
    "Subconcept: Sentence Segmentation Techniques\n",
    "Subconcept: Introduction to Edit Distance\n",
    "Subconcept: Applications of Edit Distance Algorithm in NLP\n",
    "Subconcept: Calculation of Edit Distance and String Alignment\n",
    "Tutor will apply practical examples relevant to modern NLP applications, such as chatbots, using the chapter summary as a conversational context.\n",
    "Tutor will provide hands-on practice problems and ensure the student understands the implementation of the concepts.\n",
    "Student's objective: To gain a foundational understanding of the chapter's main points, to apply this understanding in an academic setting, and to perform well in the NLP class.\n",
    "Since the student might need to have a deep understanding of the class material, the lesson should provide a solid theoretical basis, followed by practical application.\n",
    "Concept: Regular Expressions, Text Normalization, Edit Distance in Natural Language Processing (NLP)\n",
    "Concept: Introduction to Regular Expressions\n",
    "Concept: Uses of Regular Expressions in NLP\n",
    "Concept: Basic Syntax and Operators of Regular Expressions\n",
    "Concept: Practical Examples and Exercises Using Regular Expressions\n",
    "Concept: Introduction to Text Normalization\n",
    "Concept: Tokenization of Text\n",
    "Concept: Lemmatization and its Importance\n",
    "Concept: Sentence Segmentation Techniques\n",
    "Concept: Introduction to Edit Distance\n",
    "Concept: Applications of Edit Distance Algorithm in NLP\n",
    "Concept: Calculation of Edit Distance and String Alignment\n",
    "Concept: Practical Examples and Exercises in Modern NLP Applications (e.g., Chatbots)\n",
    "Concept: Hands-on Practice Problems\n",
    "Concept: Foundational Understanding of Main Points\n",
    "Concept: Academic Application of Concepts\n",
    "Concept: Theoretical Basis Followed by Practical Application\n",
    "Student's Interest Statement: I find natural language processing interesting and important since I am taking it as a course in college where I will be tested\n",
    "Student's Slides Preference Statement: I want to be taught by information and examples\n",
    "Student's Questions Preference Statement: 2 of multiple choice, 2 of free response and 2 coding questions'''\n",
    "\n",
    "tutor_plan_economics = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "tutor_plan_calc = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "tutor_plan_art = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "tutor_plan_history = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "current_plan = {'NLP': tutor_plan_nlp, \"history\": tutor_plan_history, \"art\":tutor_plan_art, \"econ\": tutor_plan_economics, \"calc\": tutor_plan_calc}[current_topic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concept generation from AITutor:\n",
    "import pickle as pkl\n",
    "notebank = NoteBank()\n",
    "[notebank.add_note(n) for n in current_plan.split(\"\\n\")]\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(f\"Research/temp_data/temp_concepts_{current_topic}.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(f\"Research/temp_data/temp_concepts_{current_topic}.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        concept = pkl.load(f)\n",
    "        concept_db = ConceptDatabase(main_concept, notebank.env_string(), False)\n",
    "        concept_db.Concepts = concept\n",
    "else:\n",
    "    concept_db = ConceptDatabase(main_concept,notebank.env_string())\n",
    "    with open(f\"Research/temp_data/temp_concepts_{current_topic}.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(concept_db.Concepts, f)\n",
    "\n",
    "print(\"\\n\\n\".join([slide.format_json() for slide in concept_db.Concepts]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slides: \n",
    "- Preprocessing\n",
    "- Generation of Document for (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SLIDE OBJ PROMPTs\n",
    "prompt = ''' #Your task is to create a JSON object from a slide string. View the example Input and output, and then repeat the same for the provided input. \n",
    "Perform the conversion for each slide s in the input string such that s->json_object(s). You should be able to figure out which is the title and which is the description.\n",
    "IMPORTANT: Escape Characters in JSON Data can cause errors if the JSON Object or JSON data contains backslashes, which means they need to be properly escaped\n",
    "Avoid these errors: Invalid \\escape: line 24 column 72 (char 2199)\n",
    "By properly escaping your backslashes ('\\\\')\n",
    "IMPORTANT: If there is two words together, such as \"functionwhere\", without being separated with a white space, that most probably means that there is a new line ('\\n') or space (' ') in between them, e.g. \"function where\".\n",
    "\n",
    "// Input:\n",
    "Page 1 Content:\n",
    "Natural Language ProcessingProfessor John LicatoUniversity of South FloridaChapter 2:RegEx, Edit Distance\n",
    "\n",
    "----------------------------------------\n",
    "Page 2 Content:\n",
    "\"Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When youâ€™re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.\"Regular Expressions\n",
    "----------------------------------------\n",
    "Page 3 Content:\n",
    "The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))Regular Expressions\n",
    "----------------------------------------\n",
    "Page 4 Content:\n",
    "The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')Regular Expressions\n",
    "----------------------------------------\n",
    "Page 5 Content:\n",
    "Creating regex objectsrâ€™ = raw string\\d â€“ placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(râ€™\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\dâ€™)\n",
    "----------------------------------------\n",
    "Page 6 Content:\n",
    "Matching regex objects\n",
    "mo = match object â€“ contains the result of our search>>> import re>>> phoneNumRegex = re.compile(râ€™\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\dâ€™)>>> mo = phoneNumRegex.search(â€˜My number is 415-555-4242.â€™)>>> print(â€˜Phone number found: â€™ + mo.group())Phone number found: 415-555-4242\n",
    "----------------------------------------\n",
    "Page 7 Content:\n",
    "Text Normalizationâ€¢We will work a lot with large datasets / corporaâ€¢We often need to pre-process textâ€¢Tokenizing (segmenting) wordsâ€¢Normalizing word formatsâ€¢Segmenting sentences (e.g. by using punctuation)\n",
    "----------------------------------------\n",
    "Page 8 Content:\n",
    "Tokenization â€“ segmenting running text into words (or word-like units)>>> text = 'That U.S.A. poster-print costs $12.40...'>>> pattern = r\\'\\'\\', (?x)  # set flag to allow verbose regexps...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A....     | \\w+(-\\w+)*      # words with optional internal hyphens...     | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%...     | \\.\\.\\.          # ellipsis...     | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [... \\'\\'\\'>>> nltk.regexp_tokenize(text, pattern)['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n",
    "----------------------------------------\n",
    "Page 9 Content:\n",
    "Subword tokenizationâ€¢How do we capture relations between words like:â€“new, newerâ€“blow, blowingâ€“precipitation, precipitateâ€¢Often useful to break tokens into *sub*wordsâ€¢Usually split into token learners, and token segmenters\n",
    "----------------------------------------\n",
    "Page 10 Content:\n",
    "Byte-pair encoding (BPE)â€¢A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab VV <- all unique characters in C                  # initial set of tokens is charactersfor i = 1 to k do                                # merge tokens til k times    t_L, t_R <- Most frequent pair of adjacent tokens in C    t_new <- t_L + t_R                           # make new token by concatenating    V <- V + t_new                               # update the vocabulary    Replace each occurrence of t_L, t_R in C with t_new # and update the corpusreturn Vcorpus5 low_2 lowest_6 newer_3 wider_2 new_vocabulary_, d, e, i, l, n, o, r, s, t, wcorpus5 low _2 lowest _6 newer _3 wider _2 new _vocabulary_, d, e, i, l, n, o, r, s, t, w, er\n",
    "----------------------------------------\n",
    "...\n",
    "        \n",
    "// Output:\n",
    "        { \n",
    "                \\\"slides\\\":[\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Natural Language Processing\\\", \n",
    "                                \\\"Description\\\": \\\"Professor John Licato University of South Florida Chapter 2:RegEx, Edit Distance\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When youâ€™re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Creating regex objects\\\", \n",
    "                                \\\"Description\\\": \\\"râ€™ = raw string\\d â€“ placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(râ€™\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\dâ€™)\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Matching regex objects\\\", \n",
    "                                \\\"Description\\\": \\\">>> import re>>> phoneNumRegex = re.compile(râ€™\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\dâ€™)>>> mo = phoneNumRegex.search(â€˜My number is 415-555-4242.â€™)>>> print(â€˜Phone number found: â€™ + mo.group())Phone number found: 415-555-4242 mo = match object â€“ contains the result of our search\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Text Normalization\\\", \n",
    "                                \\\"Description\\\": \\\"â€¢We will work a lot with large datasets / corpora\\nâ€¢We often need to pre-process text\\nâ€¢Tokenizing (segmenting) words\\nâ€¢Normalizing word formats\\nâ€¢Segmenting sentences (e.g. by using punctuation) )\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Tokenization â€“ segmenting running text into words (or word-like units)\\\", \n",
    "                                \\\"Description\\\": \\\">>> text = 'That U.S.A. poster-print costs $12.40...'\\n>>> pattern = r\\'\\'\\', (?x)  # set flag to allow verbose regexps\\n...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A\\n....     | \\w+(-\\w+)*      # words with optional internal hyphens\\n...     | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\\n...     | \\.\\.\\.          # ellipsis...     | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [\\n... \\'\\'\\'\\n>>> nltk.regexp_tokenize(text, pattern)\\n['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Subword tokenization\\\", \n",
    "                                \\\"Description\\\": \\\"â€¢How do we capture relations between words like:\\nâ€“new, newer\\nâ€“blow, blowing\\nâ€“precipitation, precipitate\\nâ€¢Often useful to break tokens into *sub*wordsâ€¢Usually split into token learners, and token segmenters\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Byte-pair encoding (BPE)\\\", \n",
    "                                \\\"Description\\\": \\\"â€¢A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab V\\nV <- all unique characters in C                  # initial set of tokens is characters\\nfor i = 1 to k do                                # merge tokens til k times    \\nt_L, t_R <- Most frequent pair of adjacent tokens in C    \\nt_new <- t_L + t_R                           # make new token by concatenating    \\nV <- V + t_new                               # update the vocabulary    \\nReplace each occurrence of t_L, t_R in C with t_new # and update the corpus\\nreturn V\\ncorpus\\n5 low_\\n2 lowest_\\n6 newer_\\n3 wider_\\n2 new_\\nvocabulary\\n_, d, e, i, l, n, o, r, s, t, w\\ncorpus\\n5 low _\\n2 lowest _\\n6 newer _\\n3 wider _\\n2 new _\\nvocabulary\\n_, d, e, i, l, n, o, r, s, t, w, er\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        ...\n",
    "                ]\n",
    "        }\n",
    "Remember! Escape Characters in JSON Data: If the JSON Object or JSON data contains backslashes, they need to be properly escaped.\n",
    "Avoid these errors: Invalid \\escape: line 24 column 72 (char 2199)\n",
    "\n",
    "// Input:\n",
    "        $SLIDE$\n",
    "\n",
    "// Output:\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Slide helper functions\n",
    "import PyPDF2\n",
    "from pptx import Presentation\n",
    "import json\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"Reads a PDF file and prints the content of each page\"\"\"\n",
    "    slide_str = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "\n",
    "        for i in range(num_pages):\n",
    "            page = reader.pages[i]\n",
    "            text = page.extract_text()\n",
    "            slide_str += f\"Page {i+1} Content:\\n{text}\"\n",
    "            slide_str += \"\\n\" + (\"-\" * 40) + \"\\n\"\n",
    "    return slide_str\n",
    "\n",
    "def extract_text_from_slide(slide):\n",
    "    \"\"\"Extracts title and content from a slide\"\"\"\n",
    "    title = slide.shapes.title.text if slide.shapes.title else \"No Title\"\n",
    "    content = []\n",
    "\n",
    "    for shape in slide.shapes:\n",
    "        if hasattr(shape, \"text\"):\n",
    "            content.append(shape.text)\n",
    "\n",
    "    return title, content\n",
    "\n",
    "def read_pptx(file_path):\n",
    "    \"\"\"Reads a pptx file and prints the title and content of each slide\"\"\"\n",
    "    prs = Presentation(file_path)\n",
    "\n",
    "    for slide in prs.slides:\n",
    "        title, content = extract_text_from_slide(slide)\n",
    "        print(f\"Title: {title}\")\n",
    "        print(\"Content:\", \"\\n\".join(content))\n",
    "        print(\"-\" * 40)\n",
    "def get_slide_prompt(slide_template, data):\n",
    "    return slide_template.replace(\"$SLIDE$\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 21 column 38 (char 1854)\n"
     ]
    }
   ],
   "source": [
    "### TEST SLIDE OBJ GEN FROM GPT FOR EXPERT\n",
    "slide_str = read_pdf('Research/generation_data/slides/Expert/codingSlides_expert.pdf')\n",
    "\n",
    "\n",
    "curr_prompt = get_slide_prompt(prompt, slide_str)\n",
    "try:\n",
    "    json_data = request_output_from_llm(prompt=curr_prompt, model=\"gpt-3.5-turbo-16k\")\n",
    "    slide_obj = json.loads(json_data)\n",
    "    print(slide_obj)\n",
    "\n",
    "    # Convert the dictionary to a JSON-formatted string\n",
    "    json_str = json.dumps(slide_obj, indent=4)  # indent for pretty-printing\n",
    "\n",
    "    # Write the JSON string to a file\n",
    "    with open(\"Research/generation_data/slides/Expert/codingSlides_expert.json\", \"w\") as f:\n",
    "        f.write(json_str)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'concept_db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m notebank \u001b[39m=\u001b[39m NoteBank()\n\u001b[1;32m      3\u001b[0m [notebank\u001b[39m.\u001b[39madd_note(n) \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m current_plan\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)]\n\u001b[0;32m----> 4\u001b[0m slide_planner \u001b[39m=\u001b[39m SlidePlanner(notebank, concept_db)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Check if the file exists\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResearch/temp_data/temp_slideplan_\u001b[39m\u001b[39m{\u001b[39;00mcurrent_topic\u001b[39m}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[39m# Load the object from the file\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'concept_db' is not defined"
     ]
    }
   ],
   "source": [
    "### Slide generation from AITutor\n",
    "notebank = NoteBank()\n",
    "[notebank.add_note(n) for n in current_plan.split(\"\\n\")]\n",
    "slide_planner = SlidePlanner(notebank, concept_db)\n",
    "# Check if the file exists\n",
    "if os.path.exists(f\"Research/temp_data/temp_slideplan_{current_topic}.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(f\"Research/temp_data/temp_slideplan_{current_topic}.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        slide_plans = pkl.load(f)\n",
    "        print(\"\\n\\n\".join([str(slide.format_json()) for slide in slide_plans]))\n",
    "        slide_planner.SlidePlans = slide_plans\n",
    "else:\n",
    "    slide_planner.generate_slide_plan()\n",
    "    with open(f\"Research/temp_data/temp_slideplan_{current_topic}.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(slide_planner.SlidePlans, f)\n",
    "\n",
    "if os.path.exists(f\"Research/temp_data/temp_slides_{current_topic}.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(f\"Research/temp_data/temp_slides_{current_topic}.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        slides = pkl.load(f)\n",
    "        slide_planner.Slides = slides\n",
    "else:\n",
    "    slide_planner.generate_slide_deque()\n",
    "    with open(f\"Research/temp_data/temp_slides_{current_topic}.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(slide_planner.Slides, f)\n",
    "import json\n",
    "print(json.dumps(slide_planner.format_json(), indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "- Preprocessing\n",
    "- Generation of questions from (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Introduction to Natural Language Processing (NLP) and Regular Expressions', 'purpose': 0, 'purpose_statement': 'To give the student an initial overview of NLP with a focus on Regular Expressions and how they play a foundational role in text analysis.', 'concepts': ['Natural Language Processing (NLP)', 'Regular Expressions', 'Text Normalization', 'Tokenization of Text', 'Edit Distance']}\n",
      "\n",
      "{'title': 'Decoding Patterns with Regular Expressions in NLP', 'purpose': 0, 'purpose_statement': 'This slide will serve as an initial deep dive into the world of Regular Expressions, enabling students to understand their syntax, basic operators, and fundamental uses in NLP, setting the groundwork for more sophisticated text processing tasks.', 'concepts': ['Regular Expressions']}\n",
      "\n",
      "{'title': 'Discovering Lemmatization: Enhancing Text Analysis in NLP', 'purpose': 0, 'purpose_statement': \"This slide aims to introduce Lemmatization as an essential NLP text preprocessing technique, building on the student's understanding of Text Normalization and Tokenization, to further enhance their text analysis skills.\", 'concepts': ['Lemmatization', 'Text Normalization', 'Tokenization of Text']}\n",
      "\n",
      "{'title': 'Engaging with Part-of-Speech Tagging in NLP', 'purpose': 0, 'purpose_statement': 'To introduce the foundational concept of part-of-speech tagging within Natural Language Processing (NLP), elucidating its definition, importance, and the role it plays in understanding the grammatical structure of language for text analysis and other NLP applications.', 'concepts': ['part-of-speech tagging', 'Tokenization of Text', 'Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Unveiling the Layers of Language: Syntax Parsing and Sentence Segmentation', 'purpose': 0, 'purpose_statement': \"This slide is designed to introduce the foundational concepts of 'Syntax Parsing' and 'Sentence Segmentation Techniques' in NLP, underscoring their roles in text analysis, their importance in machine understanding of language, and how they pave the way for advanced NLP tasks such as dependency parsing and named entity recognition.\", 'concepts': ['Syntax Parsing', 'Sentence Segmentation Techniques']}\n",
      "\n",
      "{'title': 'Transforming Texts: Mastering Edit Distance & Regular Expressions in NLP', 'purpose': 3, 'purpose_statement': \"This slide is tailored to solidify the student's foundational knowledge of Regular Expressions, focusing on their practical applications in text normalization and edit distance calculations in NLP. The content will bridge theoretical concepts with real-world tools to enhance the student's performance in an academic setting, and facilitate the application of these concepts through interactive examples and aligned practice problems.\", 'concepts': ['Regular Expressions', 'Edit Distance', 'Text Normalization']}\n",
      "\n",
      "{'title': 'Diving Deeper: Practical Applications of Edit Distance in NLP', 'purpose': 2, 'purpose_statement': 'This slide uncovers the significant role of Edit Distance in the field of Natural Language Processing, with a focus on real-world applications like spelling correction in digital text analysis and comparative genomics through DNA sequencing. The aim is to provide the student with an understanding of Edit Distance that bridges the gap between academic learning and practical implementation in modern computational linguistics and bioinformatics.', 'concepts': ['Edit Distance']}\n",
      "\n",
      "{'title': 'Enhancing Natural Language Processing with Tokenization Techniques', 'purpose': 2, 'purpose_statement': \"This slide aims to delve into the specifics of 'Tokenization'â€”a fundamental concept in NLP not yet fully explored in our slide set. By exploring different tokenization techniques and their pertinence in various contexts of NLP, the student will gain a deeper understanding of how text is processed at the foundational level for more complex tasks.\", 'concepts': ['Tokenization', 'Regular Expressions', 'Text Normalization']}\n",
      "\n",
      "{'title': 'Enhancing Natural Language Processing with Tokenization Techniques', 'purpose': 2, 'purpose_statement': \"This slide aims to delve into the specifics of 'Tokenization'â€”a fundamental concept in NLP not yet fully explored in our slide set. By exploring different tokenization techniques and their pertinence in various contexts of NLP, the student will gain a deeper understanding of how text is processed at the foundational level for more complex tasks.\", 'concepts': ['Tokenization', 'Regular Expressions', 'Text Normalization']}\n",
      "\n",
      "{'title': 'Navigating the Grammar of Language: part-of-speech tagging in NLP', 'purpose': 2, 'purpose_statement': 'This slide aims to deepen the studentâ€™s understanding of the part-of-speech tagging process within the broader scope of NLP, demonstrating the connection between tagging, sentence structure, and linguistic meaning, as well as its application in real-world text analysis challenges.', 'concepts': ['part-of-speech tagging', 'Natural Language Processing (NLP)', 'Tokenization of Text', 'Syntax Parsing']}\n",
      "\n",
      "{'title': 'Practical Insights into Lemmatization and Sentence Segmentation in NLP', 'purpose': 4, 'purpose_statement': \"To consolidate the student's theoretical understanding of Lemmatization and Sentence Segmentation Techniques through practical examples and exercises, demonstrating their vital roles in NLP applications such as machine translation and information retrieval systems.\", 'concepts': ['Lemmatization', 'Sentence Segmentation Techniques']}\n",
      "\n",
      "{'title': 'Advanced Utilization of Regular Expressions in NLP', 'purpose': 3, 'purpose_statement': \"To expand the student's knowledge of Regular Expressions beyond basics, covering advanced pattern matching and efficiency in text parsing, and to provide practical applications in NLP.\", 'concepts': ['Regular Expressions']}\n",
      "\n",
      "{'title': 'Mastery Through Practice: Regular Expressions in Real-World NLP Applications', 'purpose': 4, 'purpose_statement': \"This slide aims to reinforce and enhance the student's theoretical understanding of Regular Expressions through curated examples and exercises, focusing on their implementation in real-world Natural Language Processing applications.\", 'concepts': ['Regular Expressions', 'Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Synergy of Patterns and Distance: Regular Expressions Meet Edit Distances in NLP', 'purpose': 3, 'purpose_statement': 'To explain the synergistic relationship between Regular Expressions and Edit Distance in the context of Natural Language Processing (NLP) and demonstrate their application in text analysis, error detection, and correction.', 'concepts': ['Regular Expressions', 'Edit Distance', 'Text Normalization']}\n",
      "{\n",
      "    \"questions\": [\n",
      "        {\n",
      "            \"subject\": 1,\n",
      "            \"type\": 3,\n",
      "            \"data\": {\n",
      "                \"data\": \"In the context of processing tweets or social media text, write a Python function `extract_hashtags` that takes a string as an input and returns a list of hashtags in the text. You are required to use regular expressions in your solution. Consider that a hashtag is defined as a string that begins with a hash symbol (#) and is followed by alphanumeric characters without spaces.\",\n",
      "                \"boilerplate\": \"def extract_hashtags(text):\\n    # TODO: Your code here\\n    pass\",\n",
      "                \"test_cases_script\": \"assert extract_hashtags(\\\"Loving the #AI and #NLP talks at the conference!\\\") == [\\\"#AI\\\", \\\"#NLP\\\"]\\nassert extract_hashtags(\\\"This is a #great_day!\\\") == [\\\"#great_day\\\"]\\nassert extract_hashtags(\\\"Hello world!\\\") == []\",\n",
      "                \"concepts\": [\n",
      "                    \"Regular Expressions\",\n",
      "                    \"Applications of Regular Expressions in NLP\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"subject\": 1,\n",
      "            \"type\": 0,\n",
      "            \"data\": {\n",
      "                \"data\": \"Implement a function `count_vowels` in Python that takes a string input and returns the number of vowels in the string. Consider the following vowels: a, e, i, o, u, and their uppercase counterparts.\",\n",
      "                \"rubric\": \"Rubric: [1 Point] Correct implementation of the function; [1 Point] Correct count of vowels; [1 Point] Correct handling of uppercase vowels; [1 Point] Clean, readable code with appropriate comments.\",\n",
      "                \"concepts\": [\n",
      "                    \"Regular Expressions\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"subject\": 1,\n",
      "            \"type\": 3,\n",
      "            \"data\": {\n",
      "                \"data\": \"Write a Python function to calculate the Edit Distance between two strings using the Levenshtein distance algorithm. Your function should take two arguments (string1, string2) and return the minimum number of operations required to convert string1 into string2. You can limit your solution to insertions, deletions, and substitutions as the allowed edit operations.\",\n",
      "                \"boilerplate\": \"def edit_distance(str1, str2):\\n    # Your code here\\n    pass\",\n",
      "                \"test_cases_script\": \"assert edit_distance('kitten', 'sitting') == 3\\nassert edit_distance('intention', 'execution') == 5\\nassert edit_distance('algorithm', 'altruistic') == 6\",\n",
      "                \"concepts\": [\n",
      "                    \"Edit Distance\",\n",
      "                    \"Applications of Edit Distance Algorithm in NLP\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"subject\": 1,\n",
      "            \"type\": 3,\n",
      "            \"data\": {\n",
      "                \"data\": \"Write a Python function named `lemmatize_tokens` that takes a string as input and returns a list of lemmatized tokens. Your implementation should tokenize the text and then apply lemmatization to each token.\",\n",
      "                \"boilerplate\": \"def lemmatize_tokens(text):\\n    # Your code here\\n    # You may use NLTK library for tokenizing and lemmatization\\n\\n    pass  # Replace with your implementation\",\n",
      "                \"test_cases_script\": \"sample_text = \\\"The striped bats are hanging on their feet for best\\\"\\nassert lemmatize_tokens(sample_text) == [\\\"The\\\", \\\"striped\\\", \\\"bat\\\", \\\"be\\\", \\\"hanging\\\", \\\"on\\\", \\\"their\\\", \\\"foot\\\", \\\"for\\\", \\\"best\\\"], \\\"Test case failed!\\\"\",\n",
      "                \"concepts\": [\n",
      "                    \"Natural Language Processing (NLP)\",\n",
      "                    \"Tokenization of Text\",\n",
      "                    \"Lemmatization\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"subject\": 1,\n",
      "            \"type\": 3,\n",
      "            \"data\": {\n",
      "                \"data\": \"Write a Python function that uses regular expressions to count the number of times a specific word appears in a given text.\",\n",
      "                \"boilerplate\": \"import re\\n\\ndef count_word_occurrences(word, text):\\n\\t# TODO: Complete the function\\n\\tpass\",\n",
      "                \"test_cases_script\": \"text = '''Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum auctor euismod nunc, nec lacinia libero ultrices nec. Nulla vitae sagittis ipsum. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Morbi vitae neque viverra, euismod nisl in, tristique odio. Nulla facilisi. Sed vehicula lorem massa, a ultrices quam iaculis et. Donec pretium viverra accumsan. Proin in lorem velit. Integer nec interdum enim, sit amet condimentum libero. Phasellus sagittis, sapien sed dapibus imperdiet, purus metus bibendum mauris, quis congue ipsum eros quis nibh.'''\\n\\nassert count_word_occurrences('Lorem', text) == 1\\nassert count_word_occurrences('semper', text) == 0\\nassert count_word_occurrences('amet', text) == 2\",\n",
      "                \"concepts\": [\n",
      "                    \"Regular Expressions\",\n",
      "                    \"Uses of Regular Expressions in NLP\",\n",
      "                    \"Basic Syntax and Operators of Regular Expressions\"\n",
      "                ],\n",
      "                \"teaching_note\": \"Regular expressions are a powerful tool for pattern matching in text. In this question, you will use regular expressions to count the number of times a specific word appears in a given text. Remember to use the re module in Python to access the regular expression functions. You can use the re.findall() function to find all occurrences of the word in the text and then use the len() function to count the number of matches.\"\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"current_obj_idx\": -1,\n",
      "    \"num_questions\": 5\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "### Slide generation from AITutor\n",
    "notebank = NoteBank()\n",
    "[notebank.add_note(n) for n in current_plan.split(\"\\n\")]\n",
    "q_suite = QuestionSuite(5, notebank, concept_db)\n",
    "# Check if the file exists\n",
    "if os.path.exists(f\"Research/temp_data/temp_questions_{current_topic}.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(f\"Research/temp_data/temp_questions_{current_topic}.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        questions = pkl.load(f)\n",
    "        print(\"\\n\\n\".join([str(slide.format_json()) for slide in slide_plans]))\n",
    "        q_suite.Questions = questions\n",
    "else:\n",
    "    q_suite.generate_question_data()\n",
    "    with open(f\"Research/temp_data/temp_questions_{current_topic}.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(q_suite.Questions, f)\n",
    "        \n",
    "print(json.dumps(q_suite.format_json(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9867042631283537 0.9690687772047663\n"
     ]
    }
   ],
   "source": [
    "### Testing Concepts\n",
    "\n",
    "coding_concepts_aitutor = \"Research/generation_data/concept_graph/Teachabull/codingConcepts_teachabull.json\"\n",
    "with open(coding_concepts_aitutor, \"r\") as f:\n",
    "    coding_concepts_aitutor = json.load(f)\n",
    "coding_concepts_aitutor = {\"concepts\": [\n",
    "    {\"name\": concept['name']} for concept in coding_concepts_aitutor['concepts']\n",
    "]}\n",
    "\n",
    "# Normalize and create embedding\n",
    "coding_concepts_aitutor = json.dumps(coding_concepts_aitutor, indent=4)\n",
    "coding_concepts_aitutor = create_embeddings(coding_concepts_aitutor)\n",
    "# expert\n",
    "coding_concepts_expert = \"Research/generation_data/concept_graph/Expert/codingConcepts_expert.json\"\n",
    "with open(coding_concepts_expert, \"r\") as f:\n",
    "    coding_concepts_expert = json.load(f)\n",
    "coding_concepts_expert = {\"concepts\": [\n",
    "    {\"name\": concept['name']} for concept in coding_concepts_expert['concepts']\n",
    "]}\n",
    "coding_concepts_expert = json.dumps(coding_concepts_expert, indent=4)\n",
    "coding_concepts_expert = create_embeddings(coding_concepts_expert)\n",
    "\n",
    "# chatgpt\n",
    "coding_concepts_chatgpt = \"Research/generation_data/concept_graph/ChatGPT/codingConcepts_chatgpt4.json\"\n",
    "with open(coding_concepts_chatgpt, \"r\") as f:\n",
    "    coding_concepts_chatgpt = json.load(f)\n",
    "coding_concepts_chatgpt = {\"concepts\": [\n",
    "    {\"name\": concept['name']} for concept in coding_concepts_chatgpt['concepts']\n",
    "]}\n",
    "coding_concepts_chatgpt = json.dumps(coding_concepts_chatgpt, indent=4)\n",
    "coding_concepts_chatgpt = create_embeddings(coding_concepts_chatgpt)\n",
    "\n",
    "coschatgpt = cosine_similarity(coding_concepts_chatgpt[0], coding_concepts_expert[0])\n",
    "cosaitutor = cosine_similarity(coding_concepts_aitutor[0], coding_concepts_expert[0])\n",
    "print(coschatgpt, cosaitutor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9469275623592007 0.9354448040938711\n"
     ]
    }
   ],
   "source": [
    "### Testing Slides\n",
    "\n",
    "coding_slides_aitutor = \"Research/generation_data/slides/Teachabull/codingSlides_aitutor.json\"\n",
    "with open(coding_slides_aitutor, \"r\") as f:\n",
    "    coding_slides_aitutor = json.load(f)\n",
    "coding_slides_aitutor = {\"slides\": [\n",
    "    {\"title\": slide['title'],\"content\": slide['content']} for slide in coding_slides_aitutor['slides']\n",
    "]}\n",
    "\n",
    "# Normalize and create embedding\n",
    "coding_slides_aitutor = json.dumps(coding_slides_aitutor, indent=4)\n",
    "coding_slides_aitutor = create_embeddings(coding_slides_aitutor)\n",
    "# expert\n",
    "coding_slides_expert = \"Research/generation_data/slides/Expert/codingSlides_expert.json\"\n",
    "with open(coding_slides_expert, \"r\") as f:\n",
    "    coding_slides_expert = json.load(f)\n",
    "coding_slides_expert = {\"slides\": [\n",
    "    {\"title\": slide['Title'],\"content\": slide['Description']} for slide in coding_slides_expert['slides']\n",
    "]}\n",
    "coding_slides_expert = json.dumps(coding_slides_expert, indent=4)\n",
    "coding_slides_expert = create_embeddings(coding_slides_expert)\n",
    "\n",
    "# chatgpt\n",
    "coding_slides_chatgpt = \"Research/generation_data/slides/ChatGPT/codingSlides_chatgpt.json\"\n",
    "with open(coding_slides_chatgpt, \"r\") as f:\n",
    "    coding_slides_chatgpt = json.load(f)\n",
    "coding_slides_chatgpt = {\"slides\": [\n",
    "    {\"title\": slide['Title'],\"content\": slide['Description']} for slide in coding_slides_chatgpt['slides']\n",
    "]}\n",
    "coding_slides_chatgpt = json.dumps(coding_slides_chatgpt, indent=4)\n",
    "coding_slides_chatgpt = create_embeddings(coding_slides_chatgpt)\n",
    "\n",
    "coschatgpt = cosine_similarity(coding_slides_chatgpt[0], coding_slides_expert[0])\n",
    "cosaitutor = cosine_similarity(coding_slides_aitutor[0], coding_slides_expert[0])\n",
    "print(coschatgpt, cosaitutor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8775049669925067 0.8033181749423033\n"
     ]
    }
   ],
   "source": [
    "### Testing Questions\n",
    "\n",
    "coding_questions_aitutor = \"Research/generation_data/questions/Teachabull/codingQuestions_aitutor.json\"\n",
    "with open(coding_questions_aitutor, \"r\") as f:\n",
    "    coding_questions_aitutor = json.load(f)\n",
    "s = \"\"\n",
    "for i, question in enumerate(coding_questions_aitutor[\"questions\"]):\n",
    "    s+=f\"{i}.\\n\"\n",
    "    for k, v in question[\"data\"].items():\n",
    "        if isinstance(v, str):\n",
    "            s+=v+\"\\n\"\n",
    "    s+=\"\\n\"\n",
    "coding_questions_aitutor = s\n",
    "\n",
    "# Normalize and create embedding\n",
    "coding_questions_aitutor = create_embeddings(coding_questions_aitutor)\n",
    "\n",
    "# expert\n",
    "coding_questions_expert = \"Research/generation_data/questions/Expert/codingQuestions_expert_RAW.txt\"\n",
    "with open(coding_questions_expert, \"r\") as f:\n",
    "    coding_questions_expert = f.read()\n",
    "\n",
    "coding_questions_expert = create_embeddings(coding_questions_expert)\n",
    "\n",
    "# chatgpt\n",
    "coding_questions_chatgpt = \"Research/generation_data/questions/ChatGPT/codingQuestions_chatgpt_RAW.txt\"\n",
    "with open(coding_questions_chatgpt, \"r\") as f:\n",
    "    coding_questions_chatgpt = f.read()\n",
    "\n",
    "coding_questions_chatgpt = create_embeddings(coding_questions_chatgpt)\n",
    "\n",
    "coschatgpt = cosine_similarity(coding_questions_chatgpt[0], coding_questions_expert[0])\n",
    "cosaitutor = cosine_similarity(coding_questions_aitutor[0], coding_questions_expert[0])\n",
    "print(coschatgpt, cosaitutor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERRORS\n",
    "\n",
    "\n",
    "the thing we are checking for errors is number of api calls per errors. api calls during translation / errors during translation\n",
    "gpt-4 and gpt-3.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCEPTS RATIO OF NUMBER OF RELEVANT CONCEPTS OVER NUMBER OF CONCEPS\n",
    "GPT-3.5 vs GPT-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
