{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricizing LLMaAiTB-E\n",
    "\n",
    "- Our Focus: Generation Quality\n",
    "- Measurement Techniques: \n",
    "    - Vector Comparison\n",
    "    - Human Preference Sample (A/B/C Testing)\n",
    "    - ???\n",
    "- Iterative documents to measure:\n",
    "    - Concepts (Generation phase)\n",
    "    - Slides (Teaching phase)\n",
    "    - Questions (Testing phase)\n",
    "- Resources for testing:\n",
    "    - Expert (From classes)\n",
    "    - GPT4 (Generation)\n",
    "    - LLMaAiTB-E (Teachabull)\n",
    "- Main concepts to cover:\n",
    "    - Coding NLP slides\n",
    "    - Math Derivatives\n",
    "    - History ?\n",
    "- Questions to test:\n",
    "    - 2 Topic\n",
    "    - 2 Multiple choice\n",
    "    - 2 Conceptual\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Helper Functions\n",
    "We will demonstrate our metrics using OpenAI's Vector Embeddings on our generated documents. We decided to use OpenAI's embeddings due to their large document size capacity. We agreed that this method would prove to be the best while comparing large documents.\n",
    "\n",
    "## LLM Prompt/Text Completion\n",
    "\n",
    "\n",
    "## Vector Comparison\n",
    "Embeddings: OpenAI’s text embeddings measure the relatedness of text strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pickle as pkl\n",
    "from AITutor_Backend.src.TutorUtils.concepts import *\n",
    "from AITutor_Backend.src.TutorUtils.notebank import NoteBank\n",
    "from AITutor_Backend.src.TutorUtils.slides import SlidePlan, Slide, SlidePlanner, Purpose, Concept, ConceptDatabase\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPENAI HELPER FUNCTIONS \n",
    "def request_output_from_llm(prompt, model: str):\n",
    "    \"\"\"Requests the Concept information from an LLM.\n",
    "\n",
    "    Args:\n",
    "        prompt: (str) - string to get passed to the model\n",
    "        model: (str) - \n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI() \n",
    "    \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "    {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": prompt,\n",
    "    },\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=8000,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts\n",
    "- Preprocessing\n",
    "- Generation of Graph for (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTEBANK from AI Tutor\n",
    "tutor_plan = '''Main Concept: Regular Expressions, Text Normalization, Edit Distance in Natural Language Processing (NLP)\n",
    "Student is a computer science student with no prior knowledge of the topic, requiring an introductory lesson.\n",
    "Student is taking an NLP class, suggesting the lessons are for academic purposes and should cover necessary conceptual detail.\n",
    "Student provided a chapter summary that includes key subtopics; this will be a guide in structuring the lesson plan.\n",
    "Tutor shall educate on the following concepts:\n",
    "Subconcept: Introduction to Regular Expressions\n",
    "Subconcept: Uses of Regular Expressions in NLP\n",
    "Subconcept: Basic Syntax and Operators of Regular Expressions\n",
    "Subconcept: Practical Examples and Exercises Using Regular Expressions\n",
    "Subconcept: Introduction to Text Normalization\n",
    "Subconcept: Tokenization of Text\n",
    "Subconcept: Lemmatization and its Importance\n",
    "Subconcept: Sentence Segmentation Techniques\n",
    "Subconcept: Introduction to Edit Distance\n",
    "Subconcept: Applications of Edit Distance Algorithm in NLP\n",
    "Subconcept: Calculation of Edit Distance and String Alignment\n",
    "Tutor will apply practical examples relevant to modern NLP applications, such as chatbots, using the chapter summary as a conversational context.\n",
    "Tutor will provide hands-on practice problems and ensure the student understands the implementation of the concepts.\n",
    "Student's objective: To gain a foundational understanding of the chapter's main points, to apply this understanding in an academic setting, and to perform well in the NLP class.\n",
    "Since the student might need to have a deep understanding of the class material, the lesson should provide a solid theoretical basis, followed by practical application.\n",
    "Concept: Regular Expressions, Text Normalization, Edit Distance in Natural Language Processing (NLP)\n",
    "Concept: Introduction to Regular Expressions\n",
    "Concept: Uses of Regular Expressions in NLP\n",
    "Concept: Basic Syntax and Operators of Regular Expressions\n",
    "Concept: Practical Examples and Exercises Using Regular Expressions\n",
    "Concept: Introduction to Text Normalization\n",
    "Concept: Tokenization of Text\n",
    "Concept: Lemmatization and its Importance\n",
    "Concept: Sentence Segmentation Techniques\n",
    "Concept: Introduction to Edit Distance\n",
    "Concept: Applications of Edit Distance Algorithm in NLP\n",
    "Concept: Calculation of Edit Distance and String Alignment\n",
    "Concept: Practical Examples and Exercises in Modern NLP Applications (e.g., Chatbots)\n",
    "Concept: Hands-on Practice Problems\n",
    "Concept: Foundational Understanding of Main Points\n",
    "Concept: Academic Application of Concepts\n",
    "Concept: Theoretical Basis Followed by Practical Application\n",
    "Student's Interest Statement: I find natural language processing interesting and important since I am taking it as a course in college where I will be tested\n",
    "Student's Slides Preference Statement: I want to be taught by information and examples\n",
    "Student's Questions Preference Statement: 2 of multiple choice, 2 of free response and 2 coding questions'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"Natural Language Processing: Regular Expressions, Text Normalization, Edit Distance\",\n",
      "\"definition\": \"Natural Language Processing (NLP) encompasses a suite of techniques for enabling computers to understand and process human languages. Within this field, Regular Expressions are a powerful tool for text pattern recognition, allowing the automation of searching, editing, and manipulation of text. Text Normalization is a preprocessing step which involves transforming text into a consistent format, often through Tokenization of Text , Lemmatization , and Sentence Segmentation Techniques . Edit Distance is a way of quantifying how dissimilar two strings (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other, which has various Applications of Edit Distance Algorithm in NLP .\",\n",
      "\"latex\": \"Natural Language Processing: Regular Expressions, Text Normalization, Edit Distance\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Regular Expressions\",\n",
      "\"definition\": \"Regular Expressions, often abbreviated as regex or regexp, are a sequence of characters that define a search pattern, primarily for use in pattern matching with strings, or string matching, in Natural Language Processing (NLP) . They serve as a powerful tool for text processing, allowing the specification of complex search patterns with various Basic Syntax and Operators of Regular Expressions . Uses of Regular Expressions in NLP include text searching, text substitution, data validation, and is foundational for various Text Normalization techniques such as Tokenization of Text and Sentence Segmentation Techniques .\",\n",
      "\"latex\": \"Regular Expressions\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Natural Language Processing (NLP)\",\n",
      "\"definition\": \"Natural Language Processing (NLP) is a field of computer science and artificial intelligence concerned with the interactions between computers and human (natural language</Concept>). The ultimate objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both meaningful and useful. NLP encompasses a range of techniques and tools that allow for text analysis , language modeling , machine translation , sentiment analysis , and more, making it essential in the development of applications such as speech recognition systems, chatbots , and text-to-speech systems.\",\n",
      "\"latex\": \"Natural Language Processing (NLP)\",\n",
      "}\n",
      "\n",
      "{\"name\": \"(natural\",\n",
      "\"definition\": \"The concept of (natural might refer to an incorrectly formatted or incomplete token, typically encountered in programming or data parsing where the string literal '(natural' may appear as part of a larger expression or construct. This concept may relate to syntax errors, string literals, and the importance of proper tokenization in programming and Natural Language Processing (NLP).\",\n",
      "\"latex\": \"(natural\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Text Analysis\",\n",
      "\"definition\": \"Text Analysis is a broad term for various processes that involve extracting meaningful information from natural language text. It encompasses a range of methodologies and technologies that Natural Language Processing (NLP) employs, such as Regular Expressions , Text Normalization , and understanding the Edit Distance between strings. These techniques allow for the structuring of unstructured text data, enabling tasks like sentiment analysis, topic modeling, and named entity recognition.\",\n",
      "\"latex\": \"Text Analysis\",\n",
      "}\n",
      "\n",
      "{\"name\": \"language modeling\",\n",
      "\"definition\": \"Language modeling is the task of predicting the likelihood of a sequence of words in a natural language . It is a fundamental concept in Natural Language Processing (NLP) , which involves developing probabilistic models that can generate or determine the probability distribution of linguistic units, usually in the form of words or sentences. These models are the basis for various NLP applications such as speech recognition , machine translation , and text generation .\",\n",
      "\"latex\": \"language modeling\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Machine Translation\",\n",
      "\"definition\": \"Machine Translation (MT) is a subfield of Natural Language Processing (NLP) that focuses on the problem of automatically translating text or speech from one language to another. It involves the use of software to translate text or speech, often employing complex algorithms and language modeling . MT can be approached in several ways, including rule-based, statistical, and neural methods, each with its own strengths and applications.\",\n",
      "\"latex\": \"Machine Translation\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Sentiment Analysis\",\n",
      "\"definition\": \"Sentiment Analysis, often referred to as opinion mining, is a subfield of Natural Language Processing (NLP) that involves the use of algorithms and techniques to extract subjective information—such as opinions, attitudes, and emotions—from textual data. This process often involves the classification of texts into categories like positive, negative, or neutral sentiment. Sentiment Analysis is widely used in fields such as business analytics, social media monitoring, and customer feedback to understand consumer attitudes and to inform decision-making.\",\n",
      "\"latex\": \"Sentiment Analysis\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Speech Recognition\",\n",
      "\"definition\": \"Speech recognition is a technology that allows computers to recognize and interpret human speech as a means of input. It is a subfield of Natural Language Processing (NLP) and involves the conversion of spoken language into text. This technology utilizes language modeling , Machine Translation , and Sentiment Analysis to understand and process the speech data accurately.\",\n",
      "\"latex\": \"Speech Recognition\",\n",
      "}\n",
      "\n",
      "{\"name\": \"chatbots\",\n",
      "\"definition\": \"Chatbots are automated, typically text-based dialogue systems that interact with humans using Natural Language Processing (NLP) techniques. They are designed to simulate conversational experiences and can be found in customer service, personal assistants, and language modeling applications. Key elements in the functioning of chatbots include Regular Expressions for pattern matching, Text Normalization for preparing user input for processing, and Edit Distance algorithms to understand and correct misspellings or errors in user input.\",\n",
      "\"latex\": \"chatbots\",\n",
      "}\n",
      "\n",
      "{\"name\": \"text-to-speech\",\n",
      "\"definition\": \"Text-to-speech (TTS) is a type of speech synthesis application that converts written text into spoken words using voice generation . TTS is often used in Natural Language Processing (NLP) for applications such as assistive technology , Speech Recognition , and chatbots . It involves processes such as Text Analysis , language modeling , and the synthesis of speech waveforms .\",\n",
      "\"latex\": \"text-to-speech\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Basic Syntax and Operators of Regular Expressions\",\n",
      "\"definition\": \"The basic syntax and operators of Regular Expressions constitute the foundational grammar used to describe patterns in text for tasks in Natural Language Processing (NLP) . This syntax includes a set of special characters that enable the matching of a wide variety of string patterns. Common operators include literals, wildcards, character classes, quantifiers, anchors, and grouping constructs, each serving a specific purpose in pattern matching.\",\n",
      "\"latex\": \"Basic Syntax and Operators of Regular Expressions\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Uses of Regular Expressions in NLP\",\n",
      "\"definition\": \"The uses of Regular Expressions in Natural Language Processing (NLP) are diverse and crucial. Regular expressions are utilized for tasks such as searching and matching patterns within text, data extraction and cleaning, Text Analysis , Sentiment Analysis , and parsing text data efficiently. They are also fundamental in Text Normalization processes like Tokenization of Text , Sentence Segmentation Techniques , and preparing text for more complex tasks such as language modeling , Machine Translation , Speech Recognition , and interacting with chatbots .\",\n",
      "\"latex\": \"Uses of Regular Expressions in NLP\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Text Normalization\",\n",
      "\"definition\": \"Text Normalization is a process in Natural Language Processing (NLP) that involves converting text into a more uniform format. It includes tasks such as Tokenization of Text , Lemmatization and its Importance , and Sentence Segmentation Techniques . Text Normalization is essential for preparing raw text for further processing like text-to-speech synthesis, Machine Translation , and Sentiment Analysis . It helps in reducing lexical variety and complexity, thus making NLP tasks more efficient and accurate.\",\n",
      "\"latex\": \"Text Normalization\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Tokenization of Text\",\n",
      "\"definition\": \"Tokenization of Text is the process in Natural Language Processing (NLP) of converting a sequence of characters into a sequence of tokens. A token is typically a word, but can also be a phrase, symbol, or other meaningful element called a lexeme within the text. Tokenization is a fundamental step before further processing such as parsing , Part-of-Speech tagging , and Text Analysis . It is crucial for Text Normalization as it helps in reducing inflectional forms and sometimes derivationally related forms of a word to a common base form.\",\n",
      "\"latex\": \"Tokenization of Text\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Sentence Segmentation Techniques\",\n",
      "\"definition\": \"Sentence Segmentation Techniques involve methods and algorithms used in Natural Language Processing (NLP) to divide a text into its constituent sentences. The goal is to accurately identify sentence boundaries, which can be complicated by the use of punctuation for other purposes, such as decimal points or abbreviations. Common techniques include the use of punctuation cues, capitalization, machine learning models, and rules-based systems that consider the linguistic and contextual structure of the text.\",\n",
      "\"latex\": \"Sentence Segmentation Techniques\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Lemmatization\",\n",
      "\"definition\": \"Lemmatization is a process in Natural Language Processing (NLP) that involves reducing a word to its base or root form, called a lemma. Unlike stemming, which crudely chops off word endings, Lemmatization considers the vocabulary and morphological analysis of words, which makes it more sophisticated and accurate. It is an essential step in Text Normalization and plays a crucial role in various NLP tasks like text-to-speech , Machine Translation , and Sentiment Analysis .\",\n",
      "\"latex\": \"Lemmatization\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Edit Distance\",\n",
      "\"definition\": \"In the context of Natural Language Processing (NLP) , Edit Distance is a way of quantifying how dissimilar two strings (e.g., words) are by counting the minimum number of operations required to transform one string into the other. The operations typically include insertions, deletions, or substitutions of a single character. Understanding Edit Distance is important for various NLP tasks such as spell checking , plagiarism detection , and genome sequence analysis . The algorithm used for computing edit distance is known as the Levenshtein algorithm .\",\n",
      "\"latex\": \"Edit Distance\",\n",
      "}\n",
      "\n",
      "{\"name\": \"spell checking\",\n",
      "\"definition\": \"Spell checking is the process of detecting and correcting spelling errors in text. This process often involves the use of algorithms such as the Edit Distance algorithm to compare words against a dictionary of correct spellings. Spell checking is an essential component in various Natural Language Processing (NLP) applications like text-to-speech , chatbots , and Sentiment Analysis , where accurate text representation is crucial.\",\n",
      "\"latex\": \"spell checking\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Algorithms\",\n",
      "\"definition\": \"In computer science, an Algorithms is a finite sequence of well-defined, computer-implementable instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are essential for processing data, automating tasks, and providing decision-making logic for software and systems.\",\n",
      "\"latex\": \"Algorithms\",\n",
      "}\n",
      "\n",
      "{\"name\": \"plagiarism detection\",\n",
      "\"definition\": \"Plagiarism detection is the process of identifying instances where text or work has been copied without authorization and proper attribution. This concept is closely related to Natural Language Processing (NLP) , as it involves the analysis and comparison of texts using computational techniques. Tools for plagiarism detection often employ Regular Expressions to identify similar strings of text, utilize Edit Distance algorithms to measure the similarity between documents, and may apply Machine Learning models to discern patterns that indicate plagiarism. The effectiveness of plagiarism detection software can be augmented by understanding the principles of Text Normalization and Tokenization of Text to standardize and break down the text for more accurate comparisons.\",\n",
      "\"latex\": \"plagiarism detection\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Machine Learning\",\n",
      "\"definition\": \"Machine Learning is a subset of Artificial Intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It involves the development of algorithms that can learn from and make predictions or decisions based on data. This field intersects with various other concepts such as statistics , data mining , and predictive modeling .\",\n",
      "\"latex\": \"Machine Learning\",\n",
      "}\n",
      "\n",
      "{\"name\": \"genome sequence analysis\",\n",
      "\"definition\": \"Genome sequence analysis is the process of examining the sequence of DNA in a genome to understand its structure, function, and evolution. This involves various techniques and tools such as DNA sequencing , bioinformatics , genetic mapping , and comparative genomics . Through genome sequence analysis, scientists can identify genome , determine their functions, and detect mutations that may lead to diseases.\",\n",
      "\"latex\": \"genome sequence analysis\",\n",
      "}\n",
      "\n",
      "{\"name\": \"genome\",\n",
      "\"definition\": \"In biology, a genome refers to the complete set of genetic material present in an organism or a cell. This includes all of its genome , which are the basic units of heredity, as well as the non-coding sequences of DNA that have various regulatory functions. The study of genomes is a central part of genomics , which often involves the use of Natural Language Processing (NLP) techniques like Regular Expressions , Text Normalization , and Edit Distance in genome sequence analysis .\",\n",
      "\"latex\": \"genome\",\n",
      "}\n",
      "\n",
      "{\"name\": \"DNA sequencing\",\n",
      "\"definition\": \"DNA sequencing is the process of determining the precise order of nucleotides within a DNA molecule. It includes any method or technology that is used to determine the order of the four bases—adenine, guanine, cytosine, and thymine—in a strand of DNA. This process has revolutionized the field of genomics and is essential for a wide range of applications, including genome sequence analysis , genetic testing , biomedical research , and forensic biology . With advancements in Machine Learning and Algorithms , DNA sequencing technology has become faster, cheaper, and more accurate, leading to significant breakthroughs in biology and medicine.\",\n",
      "\"latex\": \"DNA sequencing\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Bioinformatics\",\n",
      "\"definition\": \"Bioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data. It combines computer science , biology , chemistry , statistics , and mathematics to analyze and interpret biological data. Applications include genome sequence analysis and DNA sequencing .\",\n",
      "\"latex\": \"Bioinformatics\",\n",
      "}\n",
      "\n",
      "{\"name\": \"genetic mapping\",\n",
      "\"definition\": \"Genetic mapping is the process of determining the location and chemical sequence of specific genes on a DNA strand. This process involves techniques such as genome sequence analysis , which allows scientists to associate particular segments of DNA with specific characteristics or diseases. Genetic mapping is a key tool in Bioinformatics , aiding in tasks like genome editing, plagiarism detection in academic research, and understanding the genetic basis of diseases for better medical interventions.\",\n",
      "\"latex\": \"genetic mapping\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Comparative Genomics\",\n",
      "\"definition\": \"Comparative genomics is a field of biological research in which the genomic features of different organisms are compared. It is often used in genome sequence analysis to understand the structure, function, and evolutionary processes that affect genomes. Comparative genomics involves the alignment of genome sequences and the identification of similarities and differences to infer the evolutionary relationships between species. This field intersects with other areas such as Bioinformatics , genetic mapping , and DNA sequencing .\",\n",
      "\"latex\": \"Comparative Genomics\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Levenshtein algorithm\",\n",
      "\"definition\": \"The Levenshtein algorithm, also known as the Levenshtein distance, is a measure of the difference between two sequences. It is the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one word into the other. This algorithm is a key part of Edit Distance calculations in various applications such as spell checking , plagiarism detection , and genome sequence analysis in Bioinformatics .\",\n",
      "\"latex\": \"Levenshtein algorithm\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Applications of Edit Distance Algorithm in NLP\",\n",
      "\"definition\": \"The Edit Distance algorithm is utilized in Natural Language Processing (NLP) for various tasks, such as spell checking , plagiarism detection , and DNA sequence analysis in Bioinformatics . It measures the minimum number of operations required to transform one string into another, which is particularly useful in tasks involving string comparison and correction within NLP.\",\n",
      "\"latex\": \"Applications of Edit Distance Algorithm in NLP\",\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Concept generation from AITutor:\n",
    "import pickle as pkl\n",
    "notebank = NoteBank()\n",
    "[notebank.add_note(n) for n in tutor_plan.split(\"\\n\")]\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"./temp_concepts.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(\"./temp_concepts.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        concept = pkl.load(f)\n",
    "        concept_db = ConceptDatabase(\"Natural Language Processing: Regular Expressions, Text Normalization, Edit Distance\",notebank.env_string(), False)\n",
    "        concept_db.Concepts = concept\n",
    "else:\n",
    "    concept_db = ConceptDatabase(\"Natural Language Processing: Regular Expressions, Text Normalization, Edit Distance\",notebank.env_string())\n",
    "    with open(\"./temp_concepts.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(concept_db.Concepts, f)\n",
    "\n",
    "print(\"\\n\\n\".join([slide.format_json() for slide in concept_db.Concepts]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slides: \n",
    "- Preprocessing\n",
    "- Generation of Document for (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SLIDE OBJ PROMPTs\n",
    "prompt = ''' #Your task is to create a JSON object from a slide string. View the example Input and output, and then repeat the same for the provided input. \n",
    "Perform the conversion for each slide s in the input string such that s->json_object(s). You should be able to figure out which is the title and which is the description.\n",
    "IMPORTANT: Escape Characters in JSON Data can cause errors if the JSON Object or JSON data contains backslashes, which means they need to be properly escaped\n",
    "Avoid these errors: Invalid \\escape: line 24 column 72 (char 2199)\n",
    "By properly escaping your backslashes ('\\\\')\n",
    "IMPORTANT: If there is two words together, such as \"functionwhere\", without being separated with a white space, that most probably means that there is a new line ('\\n') or space (' ') in between them, e.g. \"function where\".\n",
    "\n",
    "// Input:\n",
    "Page 1 Content:\n",
    "Natural Language ProcessingProfessor John LicatoUniversity of South FloridaChapter 2:RegEx, Edit Distance\n",
    "\n",
    "----------------------------------------\n",
    "Page 2 Content:\n",
    "\"Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When you’re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.\"Regular Expressions\n",
    "----------------------------------------\n",
    "Page 3 Content:\n",
    "The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))Regular Expressions\n",
    "----------------------------------------\n",
    "Page 4 Content:\n",
    "The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')Regular Expressions\n",
    "----------------------------------------\n",
    "Page 5 Content:\n",
    "Creating regex objectsr’ = raw string\\d – placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)\n",
    "----------------------------------------\n",
    "Page 6 Content:\n",
    "Matching regex objects\n",
    "mo = match object – contains the result of our search>>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)>>> mo = phoneNumRegex.search(‘My number is 415-555-4242.’)>>> print(‘Phone number found: ’ + mo.group())Phone number found: 415-555-4242\n",
    "----------------------------------------\n",
    "Page 7 Content:\n",
    "Text Normalization•We will work a lot with large datasets / corpora•We often need to pre-process text•Tokenizing (segmenting) words•Normalizing word formats•Segmenting sentences (e.g. by using punctuation)\n",
    "----------------------------------------\n",
    "Page 8 Content:\n",
    "Tokenization – segmenting running text into words (or word-like units)>>> text = 'That U.S.A. poster-print costs $12.40...'>>> pattern = r\\'\\'\\', (?x)  # set flag to allow verbose regexps...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A....     | \\w+(-\\w+)*      # words with optional internal hyphens...     | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%...     | \\.\\.\\.          # ellipsis...     | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [... \\'\\'\\'>>> nltk.regexp_tokenize(text, pattern)['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n",
    "----------------------------------------\n",
    "Page 9 Content:\n",
    "Subword tokenization•How do we capture relations between words like:–new, newer–blow, blowing–precipitation, precipitate•Often useful to break tokens into *sub*words•Usually split into token learners, and token segmenters\n",
    "----------------------------------------\n",
    "Page 10 Content:\n",
    "Byte-pair encoding (BPE)•A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab VV <- all unique characters in C                  # initial set of tokens is charactersfor i = 1 to k do                                # merge tokens til k times    t_L, t_R <- Most frequent pair of adjacent tokens in C    t_new <- t_L + t_R                           # make new token by concatenating    V <- V + t_new                               # update the vocabulary    Replace each occurrence of t_L, t_R in C with t_new # and update the corpusreturn Vcorpus5 low_2 lowest_6 newer_3 wider_2 new_vocabulary_, d, e, i, l, n, o, r, s, t, wcorpus5 low _2 lowest _6 newer _3 wider _2 new _vocabulary_, d, e, i, l, n, o, r, s, t, w, er\n",
    "----------------------------------------\n",
    "...\n",
    "        \n",
    "// Output:\n",
    "        { \n",
    "                \\\"slides\\\":[\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Natural Language Processing\\\", \n",
    "                                \\\"Description\\\": \\\"Professor John Licato University of South Florida Chapter 2:RegEx, Edit Distance\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When you’re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Creating regex objects\\\", \n",
    "                                \\\"Description\\\": \\\"r’ = raw string\\d – placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Matching regex objects\\\", \n",
    "                                \\\"Description\\\": \\\">>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)>>> mo = phoneNumRegex.search(‘My number is 415-555-4242.’)>>> print(‘Phone number found: ’ + mo.group())Phone number found: 415-555-4242 mo = match object – contains the result of our search\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Text Normalization\\\", \n",
    "                                \\\"Description\\\": \\\"•We will work a lot with large datasets / corpora\\n•We often need to pre-process text\\n•Tokenizing (segmenting) words\\n•Normalizing word formats\\n•Segmenting sentences (e.g. by using punctuation) )\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Tokenization – segmenting running text into words (or word-like units)\\\", \n",
    "                                \\\"Description\\\": \\\">>> text = 'That U.S.A. poster-print costs $12.40...'\\n>>> pattern = r\\'\\'\\', (?x)  # set flag to allow verbose regexps\\n...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A\\n....     | \\w+(-\\w+)*      # words with optional internal hyphens\\n...     | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\\n...     | \\.\\.\\.          # ellipsis...     | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [\\n... \\'\\'\\'\\n>>> nltk.regexp_tokenize(text, pattern)\\n['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Subword tokenization\\\", \n",
    "                                \\\"Description\\\": \\\"•How do we capture relations between words like:\\n–new, newer\\n–blow, blowing\\n–precipitation, precipitate\\n•Often useful to break tokens into *sub*words•Usually split into token learners, and token segmenters\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Byte-pair encoding (BPE)\\\", \n",
    "                                \\\"Description\\\": \\\"•A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab V\\nV <- all unique characters in C                  # initial set of tokens is characters\\nfor i = 1 to k do                                # merge tokens til k times    \\nt_L, t_R <- Most frequent pair of adjacent tokens in C    \\nt_new <- t_L + t_R                           # make new token by concatenating    \\nV <- V + t_new                               # update the vocabulary    \\nReplace each occurrence of t_L, t_R in C with t_new # and update the corpus\\nreturn V\\ncorpus\\n5 low_\\n2 lowest_\\n6 newer_\\n3 wider_\\n2 new_\\nvocabulary\\n_, d, e, i, l, n, o, r, s, t, w\\ncorpus\\n5 low _\\n2 lowest _\\n6 newer _\\n3 wider _\\n2 new _\\nvocabulary\\n_, d, e, i, l, n, o, r, s, t, w, er\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        ...\n",
    "                ]\n",
    "        }\n",
    "Remember! Escape Characters in JSON Data: If the JSON Object or JSON data contains backslashes, they need to be properly escaped.\n",
    "Avoid these errors: Invalid \\escape: line 24 column 72 (char 2199)\n",
    "\n",
    "// Input:\n",
    "        $SLIDE$\n",
    "\n",
    "// Output:\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 Content:\n",
      "Natural Language ProcessingProfessor John LicatoUniversity of South FloridaChapter 2:RegEx, Edit Distance\n",
      "\n",
      "----------------------------------------\n",
      "Page 2 Content:\n",
      "\"Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When you’re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.\"Regular Expressions\n",
      "----------------------------------------\n",
      "Page 3 Content:\n",
      "The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))Regular Expressions\n",
      "----------------------------------------\n",
      "Page 4 Content:\n",
      "The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')Regular Expressions\n",
      "----------------------------------------\n",
      "Page 5 Content:\n",
      "Creating regex objectsr’ = raw string\\d – placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)\n",
      "----------------------------------------\n",
      "Page 6 Content:\n",
      "Matching regex objects\n",
      "mo = match object – contains the result of our search>>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)>>> mo = phoneNumRegex.search(‘My number is 415-555-4242.’)>>> print(‘Phone number found: ’ + mo.group())Phone number found: 415-555-4242\n",
      "----------------------------------------\n",
      "Page 7 Content:\n",
      "Text Normalization•We will work a lot with large datasets / corpora•We often need to pre-process text•Tokenizing (segmenting) words•Normalizing word formats•Segmenting sentences (e.g. by using punctuation)\n",
      "----------------------------------------\n",
      "Page 8 Content:\n",
      "Tokenization – segmenting running text into words (or word-like units)>>> text = 'That U.S.A. poster-print costs $12.40...'>>> pattern = r''', (?x)  # set flag to allow verbose regexps...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A....     | \\w+(-\\w+)*      # words with optional internal hyphens...     | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%...     | \\.\\.\\.          # ellipsis...     | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [... '''>>> nltk.regexp_tokenize(text, pattern)['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n",
      "----------------------------------------\n",
      "Page 9 Content:\n",
      "Subword tokenization•How do we capture relations between words like:–new, newer–blow, blowing–precipitation, precipitate•Often useful to break tokens into *sub*words•Usually split into token learners, and token segmenters\n",
      "----------------------------------------\n",
      "Page 10 Content:\n",
      "Byte-pair encoding (BPE)•A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab VV <- all unique characters in C                  # initial set of tokens is charactersfor i = 1 to k do                                # merge tokens til k times    t_L, t_R <- Most frequent pair of adjacent tokens in C    t_new <- t_L + t_R                           # make new token by concatenating    V <- V + t_new                               # update the vocabulary    Replace each occurrence of t_L, t_R in C with t_new # and update the corpusreturn Vcorpus5 low_2 lowest_6 newer_3 wider_2 new_vocabulary_, d, e, i, l, n, o, r, s, t, wcorpus5 low _2 lowest _6 newer _3 wider _2 new _vocabulary_, d, e, i, l, n, o, r, s, t, w, er\n",
      "----------------------------------------\n",
      "Page 11 Content:\n",
      "Word normalization•Case folding – e.g., making everything lowercase•Lemmatization – folding lemmas together if they have the same root (dinner / dinners, am / are / is, etc.). •Stemming – performing lemmatization by removing all but the roots of words (running / runner -> run)\n",
      "----------------------------------------\n",
      "Page 12 Content:\n",
      "Minimum Edit DistanceDefinition of Minimum Edit Distance\n",
      "----------------------------------------\n",
      "Page 13 Content:\n",
      "How similar are two strings?•Spell correction–The user typed “graffe”Which is closest? •graf•graft•grail•giraffe•Computational Biology•Align two sequences of nucleotides•Resulting alignment:•Also for Machine Translation, Information Extraction, Speech RecognitionAGGCTATCACCTGACCTCCAGGCCGATGCCCTAGCTATCACGACCGCGGTCGATTTGCCCGAC-AGGCTATCACCTGACCTCCAGGCCGA--TGCCC---TAG-CTATCAC--GACCGC--GGTCGATTTGCCCGAC\n",
      "----------------------------------------\n",
      "Page 14 Content:\n",
      "Edit Distance•The minimum edit distance between two strings•Is the minimum number of editing operations–Insertion–Deletion–Substitution•Needed to transform one into the other\n",
      "----------------------------------------\n",
      "Page 15 Content:\n",
      "Minimum Edit Distance•Two strings and their alignment:INTENTION| | | | | | |*EXECUTION\n",
      "----------------------------------------\n",
      "Page 16 Content:\n",
      "Minimum Edit Distance•If each operation has cost of 1–Distance between these is 5•If substitutions cost 2 (Levenshtein)–Distance between them is 8INTENTION| | | | | | |*EXECUTIONd s s   is s\n",
      "----------------------------------------\n",
      "Page 17 Content:\n",
      "Alignment in Computational Biology•Given a sequence of bases•An alignment:•Given two sequences, align each letter to a letter or gap-AGGCTATCACCTGACCTCCAGGCCGA--TGCCC---TAG-CTATCAC--GACCGC--GGTCGATTTGCCCGACAGGCTATCACCTGACCTCCAGGCCGATGCCCTAGCTATCACGACCGCGGTCGATTTGCCCGAC\n",
      "----------------------------------------\n",
      "Page 18 Content:\n",
      "Other uses of Edit Distance in NLP•Evaluating Machine Translation and speech recognitionR Spokesman confirms    senior government adviser was shotH Spokesman said    the senior            adviser was shot dead              S      I              D                        I•Named Entity Extraction and Entity Coreference–IBM Inc. announced today–IBM profits–Stanford President John Hennessy announced yesterday–for Stanford University President John Hennessy\n",
      "----------------------------------------\n",
      "Page 19 Content:\n",
      "How to find the Min Edit Distance?•Searching for a path (sequence of edits) from the start string to the final string:–Initial state: the word we’re transforming–Operators: insert, delete, substitute–Goal state:  the word we’re trying to get to–Path cost: what we want to minimize: the number of edits37\n",
      "\n",
      "----------------------------------------\n",
      "Page 20 Content:\n",
      "Minimum Edit as Search•But the space of all edit sequences is huge!–We can’t afford to navigate naïvely–Lots of distinct paths wind up at the same state.•We don’t have to keep track of all of them•Just the shortest path to each of those revisted states.\n",
      "38\n",
      "----------------------------------------\n",
      "Page 21 Content:\n",
      "Defining Min Edit Distance•For two strings–X of length n –Y of length m•We define D(i,j)–the edit distance between X[1..i] and Y[1..j] •i.e., the first i characters of X and the first j characters of Y–The edit distance between X and Y is thus D(n,m)\n",
      "----------------------------------------\n",
      "Page 22 Content:\n",
      "Dynamic Programming forMinimum Edit Distance•Dynamic programming: A tabular computation of D(n,m)•Solving problems by combining solutions to subproblems.•Bottom-up–We compute D(i,j) for small i,j –And compute larger D(i,j) based on previously computed smaller values–i.e., compute D(i,j) for all i (0 < i < n)  and j (0 < j < m)\n",
      "----------------------------------------\n",
      "Page 23 Content:\n",
      "Defining Min Edit Distance (Levenshtein)•InitializationD(i,0) = iD(0,j) = j•Recurrence Relation:For each  i = 1…M   For each  j = 1…N                       D(i-1,j) + 1          D(i,j)= min  D(i,j-1) + 1                       D(i-1,j-1) +   2; if X(i) ≠ Y(j)                                         0; if X(i) = Y(j)•Termination:D(N,M) is distance \n",
      "----------------------------------------\n",
      "Page 24 Content:\n",
      "The Edit Distance TableThe minimum edit distance between two strings is the minimum number of editing operations needed to transform one string into the other. The typical operations allowed are:1.Insertion (Ins): Add one character to the string.2.Deletion (Del): Remove one character from the string.3.Substitution (Sub): Replace one character with another.\n",
      "----------------------------------------\n",
      "Page 25 Content:\n",
      "Computing alignments•Edit distance isn’t sufficient–We often need to align each character of the two strings to each other•We do this by keeping a “backtrace”•Every time we enter a cell, remember where we came from•When we reach the end, –Trace back the path from the upper right corner to read off the alignment\n",
      "----------------------------------------\n",
      "Page 26 Content:\n",
      "MinEdit with Backtrace\n",
      "\n",
      "----------------------------------------\n",
      "Page 27 Content:\n",
      "Adding Backtrace to Minimum Edit Distance•Base conditions:                                                        Termination:D(i,0) = i         D(0,j) = j         D(N,M) is distance •Recurrence Relation:For each  i = 1…M  For each  j = 1…N                      D(i-1,j) + 1         D(i,j)= min  D(i,j-1) + 1                      D(i-1,j-1) +  2; if X(i) ≠ Y(j)                                       0; if X(i) = Y(j)                     LEFT         ptr(i,j)=   DOWN                     DIAGinsertiondeletionsubstitutioninsertiondeletionsubstitution\n",
      "----------------------------------------\n",
      "Page 28 Content:\n",
      "The Distance Matrix\n",
      "Slide adapted from Serafim Batzoglouy0 ………………………………  yMx0 ……………………  xNEvery non-decreasing path from (0,0) to (M, N) corresponds to an alignment of the two sequencesAn optimal alignment is composed of optimal subalignments\n",
      "----------------------------------------\n",
      "Page 29 Content:\n",
      "Result of Backtrace•Two strings and their alignment:INTENTION| | | | | | |*EXECUTION\n",
      "----------------------------------------\n",
      "Page 30 Content:\n",
      "Performance•Time:    O(nm)•Space:    O(nm)•Backtrace    O(n+m)\n",
      "----------------------------------------\n",
      "Page 31 Content:\n",
      "Hearst Patterns for Hypernymy•Hyponym – “Is-A” relationship•Hypernym – Opposite of hypernym•Color is a hypernym of red; cat is a hypernym of white cat. •A rule-based way of detecting hypernym relationships in text is through Hearst patterns\n",
      "----------------------------------------\n",
      "Page 32 Content:\n",
      "Some Hearst Patterns•All bolded symbols (a, b, …) are noun phrases•Type 1 - Extract (b,a)–“a is b”–“a is a type of b”–“a is a kind of b”–“a was b”–“a was a type of b”–“a was a kind of b”–“a are b”–“a are a type of b”–“a are a kind of b”•Type 2 - Extract (a,b), (a,c), …, (a,d)–“a, including b”–“a, including b, c, …, and d”–“a, including b, c, …, or d”–“a, such as b”–“a, such as b, c, …, and d”–“a, such as b, c, …, or d”•There are many others!\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Slide helper functions\n",
    "import PyPDF2\n",
    "from pptx import Presentation\n",
    "import json\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"Reads a PDF file and prints the content of each page\"\"\"\n",
    "    slide_str = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "\n",
    "        for i in range(num_pages):\n",
    "            page = reader.pages[i]\n",
    "            text = page.extract_text()\n",
    "            slide_str += f\"Page {i+1} Content:\\n{text}\"\n",
    "            slide_str += \"\\n\" + (\"-\" * 40) + \"\\n\"\n",
    "    return slide_str\n",
    "\n",
    "def extract_text_from_slide(slide):\n",
    "    \"\"\"Extracts title and content from a slide\"\"\"\n",
    "    title = slide.shapes.title.text if slide.shapes.title else \"No Title\"\n",
    "    content = []\n",
    "\n",
    "    for shape in slide.shapes:\n",
    "        if hasattr(shape, \"text\"):\n",
    "            content.append(shape.text)\n",
    "\n",
    "    return title, content\n",
    "\n",
    "def read_pptx(file_path):\n",
    "    \"\"\"Reads a pptx file and prints the title and content of each slide\"\"\"\n",
    "    prs = Presentation(file_path)\n",
    "\n",
    "    for slide in prs.slides:\n",
    "        title, content = extract_text_from_slide(slide)\n",
    "        print(f\"Title: {title}\")\n",
    "        print(\"Content:\", \"\\n\".join(content))\n",
    "        print(\"-\" * 40)\n",
    "def get_slide_prompt(slide_template, data):\n",
    "    return slide_template.replace(\"$SLIDE$\", data)\n",
    "slide_str = read_pdf('Research/generation_data/slides/Expert/codingSlides_Expert.pdf')\n",
    "print(slide_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'slides': [{'Title': 'Natural Language Processing', 'Description': 'Professor John Licato University of South Florida Chapter 2:RegEx, Edit Distance', 'Latex': []}, {'Title': 'Regular Expressions', 'Description': 'Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When you’re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.', 'Latex': []}, {'Title': 'Regular Expressions', 'Description': \"The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))\", 'Latex': []}, {'Title': 'Regular Expressions', 'Description': \"The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')\", 'Latex': []}, {'Title': 'Creating regex objects', 'Description': 'r’ = raw string\\\\d – placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(r’\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d\\\\d’)', 'Latex': []}, {'Title': 'Matching regex objects', 'Description': '>>> import re>>> phoneNumRegex = re.compile(r’\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d\\\\d’)>>> mo = phoneNumRegex.search(‘My number is 415-555-4242.’)>>> print(‘Phone number found: ’ + mo.group())Phone number found: 415-555-4242 mo = match object – contains the result of our search', 'Latex': []}, {'Title': 'Text Normalization', 'Description': '•We will work a lot with large datasets / corpora\\n•We often need to pre-process text\\n•Tokenizing (segmenting) words\\n•Normalizing word formats\\n•Segmenting sentences (e.g. by using punctuation)', 'Latex': []}, {'Title': 'Tokenization – segmenting running text into words (or word-like units)', 'Description': '>>> text = \\'That U.S.A. poster-print costs $12.40...\\'\\n>>> pattern = r\\'\\'\\', (?x)  # set flag to allow verbose regexps\\n...     ([A-Z]\\\\.)+        # abbreviations, e.g. U.S.A....\\n     | \\\\w+(-\\\\w+)*      # words with optional internal hyphens...\\n     | \\\\$?\\\\d+(\\\\.\\\\d+)?%?  # currency and percentages, e.g. $12.40, 82%...\\n     | \\\\...          # ellipsis...\\n     | [][.,;\"\\'?():-_`]  # these are separate tokens; includes ], [... \\'\\'\\'\\n>>> nltk.regexp_tokenize(text, pattern)[\\'That\\', \\'U.S.A.\\', \\'poster-print\\', \\'costs\\', \\'$12.40\\', \\'...\\']', 'Latex': []}, {'Title': 'Subword tokenization', 'Description': '•How do we capture relations between words like:\\n–new, newer\\n–blow, blowing\\n–precipitation, precipitate\\n•Often useful to break tokens into *sub*words•Usually split into token learners, and token segmenters', 'Latex': []}, {'Title': 'Byte-pair encoding (BPE)', 'Description': '•A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab VV <- all unique characters in C                  # initial set of tokens is charactersfor i = 1 to k do                                # merge tokens til k times    t_L, t_R <- Most frequent pair of adjacent tokens in C    t_new <- t_L + t_R                           # make new token by concatenating    V <- V + t_new                               # update the vocabulary    Replace each occurrence of t_L, t_R in C with t_new # and update the corpusreturn Vcorpus5 low_2 lowest_6 newer_3 wider_2 new_vocabulary_, d, e, i, l, n, o, r, s, t, wcorpus5 low _2 lowest _6 newer _3 wider _2 new _vocabulary_, d, e, i, l, n, o, r, s, t, w, er', 'Latex': []}, {'Title': 'Word normalization', 'Description': '•Case folding – e.g., making everything lowercase•Lemmatization – folding lemmas together if they have the same root (dinner / dinners, am / are / is, etc.). •Stemming – performing lemmatization by removing all but the roots of words (running / runner -> run)', 'Latex': []}, {'Title': 'Minimum Edit Distance', 'Description': 'Definition of Minimum Edit Distance', 'Latex': []}, {'Title': 'How similar are two strings?', 'Description': '•Spell correction–The user typed “graffe”Which is closest? •graf•graft•grail•giraffe•Computational Biology•Align two sequences of nucleotides•Resulting alignment:•Also for Machine Translation, Information Extraction, Speech RecognitionAGGCTATCACCTGACCTCCAGGCCGATGCCCTAGCTATCACGACCGCGGTCGATTTGCCCGAC-AGGCTATCACCTGACCTCCAGGCCGA--TGCCC---TAG-CTATCAC--GACCGC--GGTCGATTTGCCCGAC', 'Latex': []}, {'Title': 'Edit Distance', 'Description': '•The minimum edit distance between two strings•Is the minimum number of editing operations–Insertion–Deletion–Substitution•Needed to transform one into the other', 'Latex': []}, {'Title': 'Minimum Edit Distance', 'Description': '•Two strings and their alignment:INTENTION| | | | | | |*EXECUTION', 'Latex': []}, {'Title': 'Minimum Edit Distance', 'Description': '•If each operation has cost of 1–Distance between these is 5•If substitutions cost 2 (Levenshtein)–Distance between them is 8INTENTION| | | | | | |*EXECUTIONd s s   is s', 'Latex': []}, {'Title': 'Alignment in Computational Biology', 'Description': '•Given a sequence of bases•An alignment:•Given two sequences, align each letter to a letter or gap-AGGCTATCACCTGACCTCCAGGCCGA--TGCCC---TAG-CTATCAC--GACCGC--GGTCGATTTGCCCGAC', 'Latex': []}, {'Title': 'Other uses of Edit Distance in NLP', 'Description': '•Evaluating Machine Translation and speech recognitionR Spokesman confirms    senior government adviser was shotH Spokesman said    the senior            adviser was shot dead              S      I              D                        I•Named Entity Extraction and Entity Coreference–IBM Inc. announced today–IBM profits–Stanford President John Hennessy announced yesterday–for Stanford University President John Hennessy', 'Latex': []}, {'Title': 'How to find the Min Edit Distance?', 'Description': '•Searching for a path (sequence of edits) from the start string to the final string:–Initial state: the word we’re transforming–Operators: insert, delete, substitute–Goal state:  the word we’re trying to get to–Path cost: what we want to minimize: the number of edits37', 'Latex': []}, {'Title': 'Minimum Edit as Search', 'Description': '•But the space of all edit sequences is huge!–We can’t afford to navigate naïvely–Lots of distinct paths wind up at the same state.•We don’t have to keep track of all of them•Just the shortest path to each of those revisted states.\\n38', 'Latex': []}, {'Title': 'Defining Min Edit Distance', 'Description': '•For two strings–X of length n –Y of length m•We define D(i,j)–the edit distance between X[1..i] and Y[1..j] •i.e., the first i characters of X and the first j characters of Y–The edit distance between X and Y is thus D(n,m)', 'Latex': []}, {'Title': 'Dynamic Programming for Minimum Edit Distance', 'Description': '•Dynamic programming: A tabular computation of D(n,m)•Solving problems by combining solutions to subproblems.•Bottom-up–We compute D(i,j) for small i,j –And compute larger D(i,j) based on previously computed smaller values–i.e., compute D(i,j) for all i (0 < i < n)  and j (0 < j < m)', 'Latex': []}, {'Title': 'Defining Min Edit Distance (Levenshtein)', 'Description': '•InitializationD(i,0) = iD(0,j) = j•Recurrence Relation:For each  i = 1…M   For each  j = 1…N                       D(i-1,j) + 1          D(i,j)= min  D(i,j-1) + 1                       D(i-1,j-1) +   2; if X(i) ≠ Y(j)                                         0; if X(i) = Y(j)•Termination:D(N,M) is distance ', 'Latex': []}, {'Title': 'The Edit Distance Table', 'Description': 'The minimum edit distance between two strings is the minimum number of editing operations needed to transform one string into the other. The typical operations allowed are:1.Insertion (Ins): Add one character to the string.2.Deletion (Del): Remove one character from the string.3.Substitution (Sub): Replace one character with another.', 'Latex': []}, {'Title': 'Computing alignments', 'Description': '•Edit distance isn’t sufficient–We often need to align each character of the two strings to each other•We do this by keeping a “backtrace”•Every time we enter a cell, remember where we came from•When we reach the end, –Trace back the path from the upper right corner to read off the alignment', 'Latex': []}, {'Title': 'MinEdit with Backtrace', 'Description': '', 'Latex': []}, {'Title': 'Adding Backtrace to Minimum Edit Distance', 'Description': '•Base conditions:                                                        Termination:D(i,0) = i         D(0,j) = j         D(N,M) is distance •Recurrence Relation:For each  i = 1…M  For each  j = 1…N                      D(i-1,j) + 1         D(i,j)= min  D(i,j-1) + 1                      D(i-1,j-1) +  2; if X(i) ≠ Y(j)                                       0; if X(i) = Y(j)                     LEFT         ptr(i,j)=   DOWN                     DIAGinsertiondeletionsubstitutioninsertiondeletionsubstitution', 'Latex': []}, {'Title': 'The Distance Matrix', 'Description': 'Slide adapted from Serafim Batzoglouy0 ………………………………  yMx0 ……………………  xNEvery non-decreasing path from (0,0) to (M, N) corresponds to an alignment of the two sequencesAn optimal alignment is composed of optimal subalignments', 'Latex': []}, {'Title': 'Result of Backtrace', 'Description': '•Two strings and their alignment:INTENTION| | | | | | |*EXECUTION', 'Latex': []}, {'Title': 'Performance', 'Description': '•Time:    O(nm)•Space:    O(nm)•Backtrace    O(n+m)', 'Latex': []}, {'Title': 'Hearst Patterns for Hypernymy', 'Description': '•Hyponym – “Is-A” relationship•Hypernym – Opposite of hypernym•Color is a hypernym of red; cat is a hypernym of white cat. •A rule-based way of detecting hypernym relationships in text is through Hearst patterns', 'Latex': []}, {'Title': 'Some Hearst Patterns', 'Description': '•All bolded symbols (a, b, …) are noun phrases•Type 1 - Extract (b,a)–“a is b”–“a is a type of b”–“a is a kind of b”–“a was b”–“a was a type of b”–“a was a kind of b”–“a are b”–“a are a type of b”–“a are a kind of b”•Type 2 - Extract (a,b), (a,c), …, (a,d)–“a, including b”–“a, including b, c, …, and d ”–“a, including b, c, …, or d”–“a, such as b”–“a, such as b, c, …, and d”–“a, such as b, c, …, or d”•There are many others!', 'Latex': []}]}\n"
     ]
    }
   ],
   "source": [
    "### TEST SLIDE OBJ GEN FROM GPT FOR EXPERT\n",
    "slide_str = read_pdf('Research/generation_data/slides/Expert/codingSlides_Expert.pdf')\n",
    "\n",
    "\n",
    "curr_prompt = get_slide_prompt(prompt, slide_str)\n",
    "try:\n",
    "    json_data = request_output_from_llm(prompt=curr_prompt, model=\"gpt-3.5-turbo-16k\")\n",
    "    slide_obj = json.loads(json_data)\n",
    "    print(slide_obj)\n",
    "\n",
    "    # Convert the dictionary to a JSON-formatted string\n",
    "    json_str = json.dumps(slide_obj, indent=4)  # indent for pretty-printing\n",
    "\n",
    "    # Write the JSON string to a file\n",
    "    with open(\"Research/generation_data/slides/Expert/codingSlides_expert.json\", \"w\") as f:\n",
    "        f.write(json_str)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Introduction to Regular Expressions in NLP', 'purpose': 0, 'purpose_statement': 'This slide will serve as an entry point to understanding Regular Expressions (Regex) and how they are used in the context of Natural Language Processing (NLP). It aims to establish foundational knowledge for students with no prior experience, as specified in the Notebank. The focus will be on familiarizing the student with the syntax and basic operators of regex, setting the foundation for understanding its applications in NLP tasks.', 'concepts': ['Basic Syntax and Operators of Regular Expressions', 'Regular Expressions']}\n",
      "\n",
      "{'title': 'Fundamentals of Text Normalization in NLP', 'purpose': 0, 'purpose_statement': 'This slide aims to introduce the concept of Text Normalization within the realm of Natural Language Processing (NLP). It will explain what Text Normalization is, why it is critical for processing natural languages, and how it aids in preparing data for further NLP tasks such as Language Modeling or Sentiment Analysis.', 'concepts': ['Text Normalization', 'Tokenization of Text', 'Sentence Segmentation Techniques', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Getting Started with Edit Distance in Natural Language Processing', 'purpose': 0, 'purpose_statement': 'To provide an introduction to the concept of Edit Distance in Natural Language Processing (NLP), emphasizing its definition and utility within the field.', 'concepts': ['Applications of Edit Distance Algorithm in NLP']}\n",
      "\n",
      "{'title': 'Harnessing the Power of Regular Expressions in NLP', 'purpose': 4, 'purpose_statement': \"This slide aims to provide practical examples and exercises demonstrating the application of Regular Expressions within various NLP tasks, reinforcing theoretical knowledge through hands-on learning experiences to solidify the student's understanding.\", 'concepts': ['Uses of Regular Expressions in NLP']}\n",
      "\n",
      "{'title': 'Understanding Edit Distance and Its Relevance in NLP Applications', 'purpose': 0, 'purpose_statement': 'To introduce the concept of Edit Distance in the context of Natural Language Processing (NLP), laying the groundwork for understanding its relevance to real-world NLP applications such as spell checking, plagiarism detection, and genome sequence analysis.', 'concepts': ['Edit Distance', 'Applications of Edit Distance Algorithm in NLP', 'spell checking', 'plagiarism detection', 'genome sequence analysis']}\n",
      "\n",
      "{'title': 'Decoding Edit Distance Calculations in NLP Contexts', 'purpose': 3, 'purpose_statement': \"To provide an in-depth understanding of the Edit Distance concept and its calculation techniques within the Natural Language Processing (NLP) context, focusing on its applications in text analysis problems encountered in the student's academic curriculum.\", 'concepts': ['Edit Distance', 'Levenshtein algorithm', 'Comparative Genomics']}\n",
      "\n",
      "{'title': 'Expanding the Boundaries with Machine Learning in NLP', 'purpose': 2, 'purpose_statement': \"This slide aims to explore the intersection of Machine Learning (ML) and Natural Language Processing (NLP), establishing a foundational understanding of how ML algorithms can enhance NLP tasks such as language modeling, sentiment analysis, and chatbots. The slide also seeks to arouse curiosity and expand the student's knowledge boundaries by introducing the crucial role of ML in modern NLP applications.\", 'concepts': ['Machine Learning', 'language modeling', 'Sentiment Analysis', 'chatbots']}\n",
      "\n",
      "{'title': 'Unveiling the Practicality of Text Normalization in NLP', 'purpose': 4, 'purpose_statement': 'To provide practical examples that demonstrate the application of previous concepts such as Text Normalization and Tokenization and to introduce new related concepts, namely Sentiment Analysis and Speech Recognition, for a deeper theoretical and practical understanding in the context of academic applications.', 'concepts': ['Text Normalization', 'Tokenization of Text', 'Lemmatization', 'Sentiment Analysis', 'Speech Recognition']}\n",
      "\n",
      "{'title': 'Embarking on the Journey of Machine Translation in NLP', 'purpose': 0, 'purpose_statement': 'This slide will serve as a premier introduction to the concept of Machine Translation within the field of Natural Language Processing, setting the stage for students to understand how machines can translate text or speech from one language to another.', 'concepts': ['Machine Translation', 'Algorithms', 'Natural Language Processing (NLP)', 'language modeling', 'text-to-speech']}\n",
      "\n",
      "{'title': 'Mastering the Fundamentals of DNA Sequencing in NLP', 'purpose': 0, 'purpose_statement': 'To introduce the interdisciplinary concepts of DNA sequencing and Bioinformatics and their relation to Natural Language Processing, setting a foundation for understanding the parallels between biological sequence analysis and text processing.', 'concepts': ['DNA sequencing', 'Bioinformatics', 'genetic mapping', 'Comparative Genomics']}\n",
      "\n",
      "{'title': 'Implementing Tokenization and Lemmatization in NLP Workflows', 'purpose': 2, 'purpose_statement': 'This slide aims to provide students with a practical understanding of how Text Tokenization and Lemmatization are implemented in NLP. The focus is on enhancing the student’s practical skills by allowing them to encode these concepts into a workflow, which is crucial for their academic applications and will assist with the NLP class objectives.', 'concepts': ['Tokenization of Text', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Implementing Tokenization and Lemmatization in NLP Workflows', 'purpose': 2, 'purpose_statement': 'This slide aims to provide students with a practical understanding of how Text Tokenization and Lemmatization are implemented in NLP. The focus is on enhancing the student’s practical skills by allowing them to encode these concepts into a workflow, which is crucial for their academic applications and will assist with the NLP class objectives.', 'concepts': ['Tokenization of Text', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Advancing Natural Language Understanding: Text Analysis Essentials', 'purpose': 2, 'purpose_statement': \"This slide aims to bridge foundational NLP concepts with underexplored yet fundamental areas of Text Analysis. It will introduce Text Analysis and elucidate on its various applications in Natural Language Processing, enriching the student's academic curriculum and facilitating better performance in the NLP class through practical, example-driven learning.\", 'concepts': ['Text Analysis', 'language modeling', 'genome', 'Natural Language Processing (NLP)', 'Regular Expressions', 'Text Normalization', 'Edit Distance']}\n",
      "\n",
      "{'title': 'Unlocking Advanced Text Normalization Techniques in NLP', 'purpose': 4, 'purpose_statement': \"To provide practical application and reinforcement of learned concepts in advanced text normalization techniques, enhancing the student's proficiency in text processing for NLP applications.\", 'concepts': ['Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Unlocking Advanced Text Normalization Techniques in NLP', 'purpose': 4, 'purpose_statement': \"To provide practical application and reinforcement of learned concepts in advanced text normalization techniques, enhancing the student's proficiency in text processing for NLP applications.\", 'concepts': ['Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Delving Deeper: Advanced Applications of Edit Distance in NLP', 'purpose': 2, 'purpose_statement': \"This slide aims to build upon the foundational understanding of the Edit Distance concept by exploring its advanced applications in the fields of NLP, specifically focusing on plagiarism detection, genome sequence analysis, and comparative genomics, which have not been fully examined in previous sessions. The goal is to leverage this deeper dive to enhance the student's comprehension for their NLP class and to nourish their interest in wide-ranging practical applications.\", 'concepts': ['Edit Distance', 'plagiarism detection', 'Comparative Genomics', 'genome sequence analysis']}\n",
      "\n",
      "{'title': 'Exploring the Nuances of Sentence Segmentation in Text Analysis', 'purpose': 2, 'purpose_statement': \"This slide will delve into the intricacies of segmenting text into sentences, an important pre-processing step in NLP, and how it paves the way for more advanced text analysis. As we have already touched upon Text Normalization and Tokenization, understanding Sentence Segmentation will enrich the student's comprehension of how texts are prepared for NLP tasks.\", 'concepts': ['Sentence Segmentation Techniques', 'Text Analysis', 'Natural Language Processing (NLP)', 'Regular Expressions']}\n",
      "\n",
      "{'title': 'Exploring the Nuances of Sentence Segmentation in Text Analysis', 'purpose': 2, 'purpose_statement': \"This slide will delve into the intricacies of segmenting text into sentences, an important pre-processing step in NLP, and how it paves the way for more advanced text analysis. As we have already touched upon Text Normalization and Tokenization, understanding Sentence Segmentation will enrich the student's comprehension of how texts are prepared for NLP tasks.\", 'concepts': ['Sentence Segmentation Techniques', 'Text Analysis', 'Natural Language Processing (NLP)', 'Regular Expressions']}\n",
      "{'title': 'Introduction to Regular Expressions in NLP', 'presentation': \"Good day! In this lesson, we will embark on a journey to understand the fascinating world of Regular Expressions (Regex) in the context of Natural Language Processing (NLP). Regular Expressions are incredibly powerful tools used for pattern matching and text processing tasks in NLP. They serve as a sequence of characters that define a search pattern, allowing us to perform complex operations on text. Today, our focus will be on the syntax and operators that form the foundation of Regular Expressions.\\n\\nRegular Expressions consist of various special characters that enable us to describe patterns in text. We will explore some of the most commonly used operators, including literals, wildcards, character classes, quantifiers, anchors, and grouping constructs.\\n\\nFor example, literals are used to search for exact matches of characters or strings. Wildcards allow us to match any character in a certain position. Character classes enable us to define sets of characters to be matched. Quantifiers specify the number of occurrences of a character or a group. Anchors help us identify patterns at the beginning or end of a line. Lastly, grouping constructs allow us to create subexpressions within a larger pattern.\\n\\nTo better understand these concepts, let's consider a practical example that resonates with your interest in chatbots. Imagine we want to create a chatbot that responds to user queries about weather conditions. By using Regular Expressions, we can define specific patterns to recognize weather-related questions. For example, we can use the wildcard operator to match any word followed by the word 'weather,' thus identifying queries like 'What's the weather like today?' or 'How's the weather in New York?'\\n\\nBy mastering the syntax and operators of Regular Expressions, you will gain a solid foundation for numerous NLP applications. These applications include text searching, text substitution, data validation, and various text normalization techniques like Tokenization and Sentence Segmentation. As we progress through this series of lessons, we will delve into practical examples and hands-on exercises that leverage Regular Expressions in modern NLP scenarios.\\n\\nI hope you're as excited as I am to dive into this topic. So without further ado, let's get started by exploring the syntax and operators of Regular Expressions!\", 'content': 'Title: Introduction to Regular Expressions in NLP\\n\\n- Regular Expressions (Regex) are a powerful tool used for pattern matching in Natural Language Processing (NLP).\\n- The basic syntax and operators of Regex form the foundation of pattern matching.\\n- Common operators include literals, wildcards, character classes, quantifiers, anchors, and grouping constructs.\\n- Regex can be used for various NLP tasks such as text searching, data validation, and text normalization.\\n- Understanding the syntax and operators of Regex is essential for developing algorithms in NLP.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'This slide will serve as an entry point to understanding Regular Expressions (Regex) and how they are used in the context of Natural Language Processing (NLP). It aims to establish foundational knowledge for students with no prior experience, as specified in the Notebank. The focus will be on familiarizing the student with the syntax and basic operators of regex, setting the foundation for understanding its applications in NLP tasks.', 'concepts': ['Basic Syntax and Operators of Regular Expressions', 'Regular Expressions']}\n",
      "\n",
      "{'title': 'Fundamentals of Text Normalization in NLP', 'presentation': \"Hello! In this slide, we will be discussing the fundamentals of text normalization in the context of Natural Language Processing (NLP). So let's dive in!\\n\\nText normalization is a crucial process in NLP that involves converting text into a more uniform format. It helps to ensure consistency in language data, making it easier for computers to process and analyze. Think of it as preparing raw text for further NLP tasks like language modeling or sentiment analysis.\\n\\nThe first concept we'll cover is tokenization of text. This is all about breaking down text into smaller elements like words or phrases. It's an essential step before further processing, such as parsing or Part-of-Speech tagging. Tokenization helps in reducing inflectional forms and sometimes derivationally related forms of a word to a common base form. For example, let's take the sentence 'I love cats and dogs.' Tokenization would split it into individual tokens: ['I', 'love', 'cats', 'and', 'dogs'].\\n\\nMoving on, lemmanization is another important aspect of text normalization. It involves reducing a word to its base or root form, called a lemma. Unlike stemming, which simply chops off word endings, lemmatization takes into account the vocabulary and morphological analysis of words. This improves accuracy and is critical for understanding contextual meaning in NLP tasks such as text-to-speech, machine translation, and sentiment analysis.\\n\\nAnother concept we'll explore is sentence segmentation techniques. These methods and algorithms are used to divide a text into its constituent sentences. The goal is to accurately identify sentence boundaries, even in the presence of punctuation or formatting challenges. Common techniques involve using punctuation cues, capitalization, machine learning models, and rules-based systems that consider the linguistic and contextual structure of the text.\\n\\nTo wrap things up, it's important to understand that text normalization plays a significant role in improving the efficiency and accuracy of NLP tasks. It reduces lexical variety and complexity in text, making it more accessible for further analysis. By standardizing and preparing data using text normalization techniques, we can unlock the full potential of NLP in various applications, like text-to-speech synthesis, machine translation, and sentiment analysis.\\n\\nI hope you now have a better understanding of the fundamentals of text normalization in NLP. If you have any questions or would like to explore some practical examples, feel free to ask!\", 'content': 'Slide Content:\\n- What is Text Normalization in NLP?\\n    - The process of converting text into a more uniform format to enhance computational handling.\\n- Tokenization of Text\\n    - Breaking down text into smaller elements like words or phrases.\\n- Lemmatization\\n    - Reducing words to their base or root forms to improve contextual understanding.\\n- Sentence Segmentation Techniques\\n    - Methods for accurately identifying sentence boundaries despite punctuation and formatting challenges.\\n- Significance of Text Normalization in NLP\\n    - Facilitating efficient and accurate NLP tasks like text-to-speech synthesis, machine translation, and sentiment analysis.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'This slide aims to introduce the concept of Text Normalization within the realm of Natural Language Processing (NLP). It will explain what Text Normalization is, why it is critical for processing natural languages, and how it aids in preparing data for further NLP tasks such as Language Modeling or Sentiment Analysis.', 'concepts': ['Text Normalization', 'Tokenization of Text', 'Sentence Segmentation Techniques', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Getting Started with Edit Distance in Natural Language Processing', 'presentation': \"Welcome to an exciting start in the world of Natural Language Processing, or NLP for short. Today, we're going to unveil a fundamental concept that acts as a linchpin for various tasks within this field – the Edit Distance. Imagine you're texting a friend and make a typo. Interestingly, the mechanisms that allow your phone to correct your spelling are rooted in the concept we're exploring today. Edit Distance isn't just about fixing errors. It's about understanding and enabling a myriad of operations that make our interactions with technology smarter. So let's dive in and discover how a simple measure of difference between text strings can have far-reaching implications in technology and beyond.\", 'content': 'In this slide, we will introduce the concept of Edit Distance in Natural Language Processing (NLP). Edit Distance is a measure of the minimum number of operations required to transform one string into another. It is used in various NLP tasks such as spell checking, plagiarism detection, and DNA sequence analysis in bioinformatics. Edit Distance plays a crucial role in comparing and correcting strings within NLP systems, making it an essential concept to understand in the field.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'To provide an introduction to the concept of Edit Distance in Natural Language Processing (NLP), emphasizing its definition and utility within the field.', 'concepts': ['Applications of Edit Distance Algorithm in NLP']}\n",
      "\n",
      "{'title': 'Harnessing the Power of Regular Expressions in NLP', 'presentation': \"Good day! Today, we will explore the fascinating world of regular expressions and their application in Natural Language Processing (NLP). Regular expressions are powerful tools that allow us to effectively search for and match patterns within text. They play a key role in various NLP tasks, such as data extraction, cleaning, and text analysis. So, let's dive in and see how regular expressions can be harnessed in NLP.\\n\\nTo start, let's focus on the basics of regular expressions and their significance in text analysis and cleaning. Regular expressions are like specialized codes that help us extract specific information from large volumes of text. They enable us to create patterns to match words, phrases, or any other desired text segments. For example, if you want to extract phone numbers from a dataset, we can use regular expressions to define a pattern that matches the desired format of a phone number.\\n\\nNow, let's move on to some practical examples. Imagine you have a massive collection of text data and you want to extract specific information, such as all the dates mentioned within the text. By using regular expressions, you can define a pattern that matches the format of a date, allowing you to efficiently extract all the dates in one go. This is just one example of the power regular expressions can bring to NLP tasks!\\n\\nAnother interesting application of regular expressions in NLP is sentiment analysis. Sentiment analysis involves determining the emotional tone of a piece of text, whether it's positive, negative, or neutral. By using regular expressions, we can filter and process user reviews or social media posts to identify the sentiment expressed. For example, we can define patterns that match specific positive or negative words, allowing us to quickly classify the sentiment of a given text sample.\\n\\nRegular expressions are also crucial in text normalization, which involves standardizing and transforming text data for various NLP tasks. One aspect of text normalization is sentence segmentation, where we split a paragraph into individual sentences. Regular expressions can help us detect sentence boundaries based on specific punctuation marks or capitalization patterns.\\n\\nNow, let's explore some practical exercises together. I will provide you with different scenarios related to chatbots, where regular expressions can be utilized to extract specific information from user queries. You will have the opportunity to write your own regular expressions based on the given task. This hands-on practice will enhance your understanding and reinforce the knowledge you've learned so far.\\n\\nTo summarize, regular expressions are indispensable in NLP, enabling us to efficiently search, match, and extract information from text data. They have wide-ranging applications, including text analysis, sentiment analysis, and text normalization. By the end of this lesson, you will gain a thorough understanding of regular expressions and how they can be practically applied in NLP tasks. So, let's get started and put our regular expression skills to the test!\", 'content': 'Slide 1: Harnessing the Power of Regular Expressions in NLP\\n\\n- Introduction to Regular Expressions and their application in Natural Language Processing (NLP)\\n- Basics of regular expressions for text analysis, cleaning, and normalization\\n- Practical examples showcasing the use of regular expressions in tasks such as data extraction, sentiment analysis, and text segmentation\\n- Hands-on exercises for students to write their own regular expressions\\n- Discussion on the practical relevance of regular expressions in modern NLP applications, including chatbots and speech recognition', 'latex_codes': '', 'purpose': 4, 'purpose_statement': \"This slide aims to provide practical examples and exercises demonstrating the application of Regular Expressions within various NLP tasks, reinforcing theoretical knowledge through hands-on learning experiences to solidify the student's understanding.\", 'concepts': ['Uses of Regular Expressions in NLP']}\n",
      "\n",
      "{'title': 'Understanding Edit Distance and Its Relevance in NLP Applications', 'presentation': \"Hey there! Today, we're going to delve into the concept of Edit Distance and its relevance in the field of Natural Language Processing (NLP). Edit Distance is a way to measure how different two strings, like words or sentences, are from each other based on the minimum number of operations required to transform one string into the other. These operations can include inserting, deleting, or substituting a single character.\\n\\nTo help us understand the concept better, let's look at an example. Consider the words 'kitten' and 'sitting'. These two words have an edit distance of 3. Here's why: we need to substitute 'k' with 's', 'e' with 'i', and insert 'g' to transform 'kitten' into 'sitting'.\\n\\nNow, let's talk about the practical use cases of Edit Distance in NLP. One important application is spell checking. Edit Distance helps in comparing words against a dictionary of correct spellings. By calculating the edit distance between a misspelled word and correctly spelled words, we can suggest possible correct spellings and improve the accuracy of spell-checking systems.\\n\\nAnother area where Edit Distance is utilized is plagiarism detection. Here, it is used to compare documents and measure their similarity. By calculating the edit distance between two texts, we can identify instances of text copying without proper attribution.\\n\\nIn addition to that, Edit Distance also has applications in genome sequence analysis. It helps in examining the sequence of DNA in a genome to understand its structure, function, and evolution. Scientists can use Edit Distance to identify different genomes, determine their functions, and detect mutations that may lead to diseases.\\n\\nOverall, understanding Edit Distance is crucial in various NLP tasks such as spell checking, plagiarism detection, and genome sequence analysis. It allows us to quantify the similarity between strings and provides a foundation for building smarter systems in the field of NLP, like chatbots and text-to-speech. So, let's dive deeper into the world of Edit Distance and explore its practical implementation in modern NLP applications. Does that make sense? Let me know if you have any questions!\", 'content': 'Title: Understanding Edit Distance and Its Relevance in NLP Applications\\n\\n- Edit Distance is a way of quantifying the dissimilarity between two strings by counting the minimal number of operations required to transform one string into another.\\n\\n- It is a vital concept in Natural Language Processing (NLP) used in applications such as spell checking, plagiarism detection, and genome sequence analysis.\\n\\n- Edit Distance is calculated by considering the operations of insertion, deletion, and substitution of characters.\\n\\n- In spell checking, Edit Distance is used to compare words against a dictionary for accurate text representation.\\n\\n- Edit Distance is also crucial in genome sequence analysis where it helps to measure the mutation variations in DNA and identify potential diseases.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'To introduce the concept of Edit Distance in the context of Natural Language Processing (NLP), laying the groundwork for understanding its relevance to real-world NLP applications such as spell checking, plagiarism detection, and genome sequence analysis.', 'concepts': ['Edit Distance', 'Applications of Edit Distance Algorithm in NLP', 'spell checking', 'plagiarism detection', 'genome sequence analysis']}\n",
      "\n",
      "{'title': 'Decoding Edit Distance Calculations in NLP Contexts', 'presentation': \"Good day! Today, we'll be diving into the fascinating world of Edit Distance calculations in the context of Natural Language Processing (NLP). This concept holds vital significance in tasks such as spell checking, plagiarism detection, and even genome sequence analysis. So, let's explore the intricacies of Edit Distance and how it plays a crucial role in the NLP domain.\\n\\nTo begin, let's define Edit Distance. In simple terms, it measures the dissimilarity between two strings, such as words, by counting the minimum number of operations required to transform one string into the other. These operations include insertions, deletions, or substitutions of a single character. Understanding Edit Distance is essential for various NLP tasks, as it enables us to quantify the similarity or difference between pieces of text.\\n\\nNow, let's delve into the Levenshtein algorithm, which is the algorithm used to compute Edit Distance. This algorithm calculates the minimum number of single-character edits required to change one word into another. It follows a step-by-step process to evaluate the difference between two sequences, whether they are words or genetic sequences. By counting the number of insertions, deletions, and substitutions, we can determine the Edit Distance.\\n\\nMoving on, we'll explore the practical applications of the Edit Distance algorithm in the context of Comparative Genomics. Comparative genomics is a field where we compare the genomic features of different organisms. It helps us understand the structure, function, and evolutionary processes that affect genomes. By aligning genome sequences and using the Edit Distance algorithm, we can identify similarities and differences, ultimately inferring the evolutionary relationships between species.\\n\\nTo make this concept more relatable, imagine you're working on a research project involving genome sequence analysis. By using the Edit Distance algorithm, you'll be able to compare DNA sequences and identify the genetic variations that distinguish one species from another. This knowledge is invaluable in fields such as Bioinformatics, genetic mapping, and DNA sequencing.\\n\\nThroughout this presentation, we'll illustrate Edit Distance with graphical representations and practical examples relevant to your studies in Natural Language Processing. By the end of this lesson, you will have gained a solid theoretical foundation in Edit Distance calculations and understand its practical applications in modern NLP.\\n\\nNow that we have introduced the topic, let's continue our journey into the world of Edit Distance calculations in NLP! If you have any questions or need further clarification along the way, feel free to ask. Remember, the goal is for you to gain a profound understanding of this concept to excel in your NLP course.\", 'content': 'Slide Content\\n\\n- Slide Title: Decoding Edit Distance Calculations in NLP Contexts\\n\\n- Edit Distance: In the context of Natural Language Processing (NLP), Edit Distance is a way of quantifying how dissimilar two strings (e.g., words) are. It counts the minimum number of operations required to transform one string into the other, such as insertions, deletions, or substitutions of a single character. Edit Distance is essential for tasks like spell checking, plagiarism detection, and genome sequence analysis.\\n\\n- Levenshtein algorithm: The Levenshtein algorithm, also known as the Levenshtein distance, is a measure of the difference between two sequences. It calculates the minimum number of single-character edits (insertions, deletions, or substitutions) needed to change one word into the other. The Levenshtein algorithm is a key component of Edit Distance calculations in applications like spell checking, plagiarism detection, and genome sequence analysis.\\n\\n- Comparative Genomics: Comparative genomics is the field of biological research that compares the genomic features of different organisms. It plays a crucial role in genome sequence analysis, helping to understand the structure, function, and evolutionary processes that impact genomes. Comparative genomics involves aligning genome sequences and identifying similarities and differences to infer the evolutionary relationships between species. It intersects with disciplines like Bioinformatics, genetic mapping, and DNA sequencing.', 'latex_codes': '', 'purpose': 3, 'purpose_statement': \"To provide an in-depth understanding of the Edit Distance concept and its calculation techniques within the Natural Language Processing (NLP) context, focusing on its applications in text analysis problems encountered in the student's academic curriculum.\", 'concepts': ['Edit Distance', 'Levenshtein algorithm', 'Comparative Genomics']}\n",
      "\n",
      "{'title': 'Expanding the Boundaries with Machine Learning in NLP', 'presentation': \"Welcome to the slide on 'Expanding the Boundaries with Machine Learning in NLP'. This slide aims to explore the intersection of Machine Learning (ML) and Natural Language Processing (NLP) and introduce the role of ML in enhancing NLP tasks.\\n\\nMachine Learning is a branch of Artificial Intelligence that enables systems to learn and improve from experience without explicit programming. It involves developing algorithms that can make predictions or decisions based on data. In the context of NLP, ML plays a crucial role in various aspects.\\n\\nOne of the key areas where ML enhances NLP is language modeling. Language modeling is the task of predicting the likelihood of word sequences in natural language. For example, ML algorithms can power auto-complete features or assist in predictive texting. By understanding the context and patterns within language, these models can generate more accurate and contextual predictions.\\n\\nAnother NLP task where ML shines is sentiment analysis. Sentiment analysis involves analyzing textual data to extract subjective information, such as opinions, attitudes, and emotions. ML algorithms can classify texts into categories like positive, negative, or neutral sentiment, providing insights into consumer attitudes in fields like business analytics and customer feedback.\\n\\nLastly, ML algorithms play a crucial role in the development of chatbots. Chatbots are automated dialogue systems that use NLP techniques to simulate conversational experiences. ML algorithms enable chatbots to understand user input, process it through techniques like Regular Expressions, Text Normalization, and Edit Distance, and generate appropriate responses. They find applications in customer service, personal assistants, and language modeling scenarios.\\n\\nIn summary, the slide demonstrates the powerful synergy between Machine Learning and Natural Language Processing. By leveraging ML algorithms, NLP tasks like language modeling, sentiment analysis, and chatbots can be significantly enhanced. Through this exploration, we hope to expand your knowledge boundaries and deepen your understanding of the role of ML in modern NLP applications.\", 'content': '1. Title: Expanding the Boundaries with Machine Learning in NLP\\n\\n2. Definition of Machine Learning as a subset of AI that enables systems to learn and improve from experience without explicit programming\\n\\n3. Explanation of language modeling as the task of predicting the likelihood of word sequences in natural language and its applications in speech recognition, machine translation, and text generation\\n\\n4. Overview of sentiment analysis, which involves the use of ML algorithms to extract subjective information from textual data, including examples of positive, negative, and neutral sentiment\\n\\n5. Exploration of chatbot technology and its utilization of ML techniques like Regular Expressions, Text Normalization, and Edit Distance to simulate conversational experiences', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide aims to explore the intersection of Machine Learning (ML) and Natural Language Processing (NLP), establishing a foundational understanding of how ML algorithms can enhance NLP tasks such as language modeling, sentiment analysis, and chatbots. The slide also seeks to arouse curiosity and expand the student's knowledge boundaries by introducing the crucial role of ML in modern NLP applications.\", 'concepts': ['Machine Learning', 'language modeling', 'Sentiment Analysis', 'chatbots']}\n",
      "\n",
      "{'title': 'Unveiling the Practicality of Text Normalization in NLP', 'presentation': \"Hey there! Today, we're going to delve into the practicality of Text Normalization in Natural Language Processing (NLP). So, what exactly is Text Normalization? It's a process that involves converting text into a more uniform format, which is crucial for various NLP tasks. This slide will provide practical examples to demonstrate the importance and application of Text Normalization in NLP.\\n\\nLet's start with the concept of Sentiment Analysis. Imagine we have a bunch of texts that we want to classify as positive, negative, or neutral sentiments. Text Normalization plays a crucial role in accurately classifying these texts. We'll explore how techniques like Tokenization, Lemmatization, and Sentence Segmentation, which are part of Text Normalization, can affect sentiment analysis.\\n\\nNext, we'll move on to another exciting application: Speech Recognition. As you know, speech recognition involves converting spoken language into text. Here, too, Text Normalization comes into play. We'll analyze examples of raw spoken language and compare them to their normalized versions. This will help us understand how Text Normalization, along with language modeling and Machine Translation, contributes to efficient speech-to-text technologies.\\n\\nBy connecting previously discussed concepts like Tokenization of Text and Lemmatization to the new concepts of Sentiment Analysis and Speech Recognition, we'll see how these building blocks come together to empower these applications. For example, we'll observe how recognizing different word forms using Lemmatization can impact sentiment extraction.\\n\\nThroughout the presentation, we'll explore not only the theory behind Text Normalization but also the practical considerations and challenges that arise when performing Text Normalization. This will provide a real-world perspective on the application of these techniques.\\n\\nBy the end of this presentation, you'll have a solid understanding of the theory behind Text Normalization and how it's implemented in NLP applications like Sentiment Analysis and Speech Recognition. Get ready to unravel the practicality of Text Normalization and uncover its significance in the world of NLP!\", 'content': '1. Introduction to Text Normalization:\\n- Process of converting text into a uniform format.\\n- Includes Tokenization, Lemmatization, and Sentence Segmentation.\\n2. Practical Example 1: Sentiment Analysis:\\n- Show how Text Normalization enhances sentiment classification.\\n- Demonstrate the impact of Tokenization, Lemmatization, and Sentence Segmentation.\\n3. Practical Example 2: Speech Recognition:\\n- Illustrate the role of Text Normalization in speech-to-text technologies.\\n- Compare raw spoken language with normalized versions.\\n- Address challenges in normalizing slang and colloquial expressions.', 'latex_codes': '', 'purpose': 4, 'purpose_statement': 'To provide practical examples that demonstrate the application of previous concepts such as Text Normalization and Tokenization and to introduce new related concepts, namely Sentiment Analysis and Speech Recognition, for a deeper theoretical and practical understanding in the context of academic applications.', 'concepts': ['Text Normalization', 'Tokenization of Text', 'Lemmatization', 'Sentiment Analysis', 'Speech Recognition']}\n",
      "\n",
      "{'title': 'Embarking on the Journey of Machine Translation in NLP', 'presentation': \"Hello, and welcome to our introductory slide on Machine Translation in the field of Natural Language Processing, or NLP. In this presentation, we will take the first step of our educational journey, exploring how machines can translate text or speech from one language to another.\\n\\nImagine you are standing at the beginning of a vast landscape, with different paths leading to various languages. This visual representation symbolizes the concept of translation and the interconnected nature of global communication.\\n\\nAs we delve into the topic of Machine Translation, it's important to understand that it is a subfield of NLP. Machine Translation focuses on the problem of automatically translating text or speech using software. We employ complex algorithms and language modeling techniques to achieve accurate and efficient translation.\\n\\nYou might wonder, what is language modeling? Well, language modeling involves predicting the likelihood of a sequence of words in a natural language. It is a fundamental concept in NLP, allowing us to build probabilistic models that can generate or determine the probability distribution of linguistic units, such as words or sentences. Language modeling is the backbone of various NLP applications, including machine translation.\\n\\nThink of language modeling as a tool that empowers machines to 'understand' human languages and generate coherent translations. This technology has revolutionized our ability to bridge language barriers and enable smooth communication across different cultures and regions.\\n\\nMoreover, Machine Translation is just one aspect of NLP. This field encompasses a wide range of techniques and tools, such as text analysis, sentiment analysis, and speech recognition. It serves as the foundation for applications like speech-to-text systems, chatbots, and text-to-speech technology.\\n\\nTo give you a glimpse of the practical side of NLP, imagine a real-time translation app on your phone. You speak into your device in one language, and the app automatically translates your speech into another language. Amazing, right?\\n\\nAs we continue our journey through the world of NLP, we'll explore the fascinating concepts of Regular Expressions, Text Normalization, and Edit Distance, which all contribute to Machine Translation and play crucial roles in understanding and processing human language.\\n\\nBy the end of this presentation, I hope you have gained a clear understanding of Machine Translation as a subfield of NLP and its vital role in facilitating cross-linguistic communication. We are excited to continue this educational adventure with you as we explore the depths of NLP and its vast applications. Let's get started!\", 'content': 'This slide serves as a premier introduction to the concept of Machine Translation within the field of Natural Language Processing. Machine Translation (MT) is a subfield of Natural Language Processing (NLP) that focuses on the problem of automatically translating text or speech from one language to another. It involves the use of software and complex algorithms to perform the translation task. MT can be approached in several ways, including rule-based, statistical, and neural methods. Language modeling is a fundamental concept in NLP, which involves developing probabilistic models that can generate or determine the probability distribution of linguistic units. TTS (Text-to-Speech) is often used in NLP for applications such as assistive technology, speech recognition, and chatbots. The ultimate objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both meaningful and useful.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'This slide will serve as a premier introduction to the concept of Machine Translation within the field of Natural Language Processing, setting the stage for students to understand how machines can translate text or speech from one language to another.', 'concepts': ['Machine Translation', 'Algorithms', 'Natural Language Processing (NLP)', 'language modeling', 'text-to-speech']}\n",
      "\n",
      "{'title': 'Mastering the Fundamentals of DNA Sequencing in NLP', 'presentation': \"Hey there! Welcome to the slide on mastering the fundamentals of DNA sequencing in NLP. In this slide, we will introduce you to the fascinating world of DNA sequencing and its relation to natural language processing. Are you ready to dive in? Great! Let's get started.\\n\\nDNA sequencing is the process of determining the precise order of nucleotides within a DNA molecule. It's like reading the genetic code of life! By knowing the order of the four bases—adenine, guanine, cytosine, and thymine—we can unlock valuable information about genes, genetic traits, and even diseases. This process has revolutionized the field of genomics and has numerous applications in genetic testing, biomedical research, and forensic biology.\\n\\nNow, let's talk about bioinformatics. Bioinformatics is an interdisciplinary field that combines computer science, biology, chemistry, statistics, and mathematics. It helps us interpret and analyze biological data, including the data obtained through DNA sequencing. By using computational methods and algorithms, bioinformatics allows us to make sense of complex genetic information and understand how genes and genomes work.\\n\\nOne important application of DNA sequencing is genetic mapping. Genetic mapping involves determining the location and chemical sequence of specific genes on a DNA strand. With the help of genome sequence analysis, scientists can associate particular segments of DNA with specific characteristics or diseases. This helps in tasks like genome editing, plagiarism detection in academic research, and understanding the genetic basis of diseases.\\n\\nAnother fascinating area is comparative genomics. Comparative genomics is all about comparing the genomic features of different organisms. By aligning genome sequences and identifying similarities and differences, scientists can gain insights into the structure, function, and evolutionary relationships between species. Comparative genomics intersects with bioinformatics, genetic mapping, and DNA sequencing, creating a synergy of knowledge and discoveries.\\n\\nNow, how does all of this relate to natural language processing? Well, believe it or not, there are parallels between DNA sequencing and text processing. Just like in comparative genomics, text processing in NLP involves using algorithms and methods like regular expressions for pattern matching and edit distance for measuring similarity between text sequences. Genetic mapping's resemblance to plagiarism detection tools in NLP shows how both fields use algorithms to identify patterns and copy-paste within datasets.\\n\\nSo, the fundamentals of DNA sequencing in NLP lay a solid foundation for understanding how computational methods can help us explore and exploit the vast amounts of genomic and textual data. With this knowledge, you'll be prepared to dive deeper into the fascinating world of genomics and natural language processing. I hope you're as excited as I am to explore this fascinating intersection of fields!\", 'content': 'Title: Mastering the Fundamentals of DNA Sequencing in NLP\\n\\n1. Introduction to DNA Sequencing: DNA sequencing is the process of determining the order of nucleotides in a DNA molecule. It has revolutionized fields like genomics, genetic testing, biomedical research, and forensic biology.\\n\\n2. Bioinformatics: Bioinformatics is an interdisciplinary field that combines computer science, biology, chemistry, statistics, and mathematics to analyze and interpret biological data. It is essential for genome sequence analysis and DNA sequencing.\\n\\n3. Genetic Mapping: Genetic mapping involves determining the location and chemical sequence of specific genes on a DNA strand. It aids in tasks like genome editing, plagiarism detection, and understanding the genetic basis of diseases.\\n\\n4. Comparative Genomics: Comparative genomics compares the genomic features of different organisms to understand their structure, function, and evolutionary processes. It intersects with bioinformatics, genetic mapping, and DNA sequencing.\\n\\n5. DNA Sequencing in NLP: DNA sequencing and NLP share similarities, such as the use of regular expressions for pattern matching and edit distance for measuring similarity between sequences of text. These connections bring together computational methods with biological and textual datasets.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'To introduce the interdisciplinary concepts of DNA sequencing and Bioinformatics and their relation to Natural Language Processing, setting a foundation for understanding the parallels between biological sequence analysis and text processing.', 'concepts': ['DNA sequencing', 'Bioinformatics', 'genetic mapping', 'Comparative Genomics']}\n",
      "\n",
      "{'title': 'Implementing Tokenization and Lemmatization in NLP Workflows', 'presentation': \"Good day! Today, we will delve into the intriguing world of Natural Language Processing, focusing on the implementation of two essential concepts: Tokenization of Text and Lemmatization. These concepts are fundamental for various NLP tasks, making them crucial to understand for our academic objectives. So, let's get started!\\n\\nTo begin, let's clarify the purpose of tokenization. It is the process of breaking down a sequence of characters into individual tokens, which can be words, phrases, symbols, or other meaningful elements. Tokenization is an initial step that allows us to analyze text further, like parsing or part-of-speech tagging. It plays a critical role in text normalization by reducing inflectional and derivational forms of words to a common base form.\\n\\nMoving on to lemmatization, it involves reducing a word to its base or root form, known as a lemma. Unlike stemming, which simply chops off word endings, lemmatization takes into account the vocabulary and morphological analysis of words, making it more sophisticated and accurate. Lemmatization is essential for various NLP tasks such as text-to-speech, machine translation, and sentiment analysis.\\n\\nNow, let's explore how tokenization and lemmatization are implemented in NLP workflows. Imagine we're working on a chatbot that needs to understand and respond to user inputs. The tokenization process would break down the user's message into individual words or tokens, allowing the chatbot to analyze and interpret each word effectively. Following that, lemmatization would then convert these tokens to their base form, ensuring a standardized representation of the text for further processing and analysis.\\n\\nTo demonstrate the practical application of these concepts, let's take a practical example. Suppose we have the following sentence: 'The cats are playing in the garden.' First, tokenization would split this sentence into individual tokens, resulting in ['The', 'cats', 'are', 'playing', 'in', 'the', 'garden']. Then, lemmatization would convert these tokens to their base form, giving us ['the', 'cat', 'be', 'play', 'in', 'the', 'garden']. By normalizing the text through tokenization and lemmatization, we derive a structured representation of the sentence that can be used for various NLP tasks.\\n\\nIn summary, tokenization and lemmatization are key concepts in NLP. Tokenization helps us break down text into meaningful units, while lemmatization ensures each token is reduced to its base form. By understanding and implementing these concepts, we can enhance our NLP workflows and effectively process and analyze text data for different applications. I hope this practical understanding will enable you to excel in your NLP studies and perform well in your academic pursuits. Thank you!\", 'content': '1. Tokenization of Text is the process of converting a sequence of characters into tokens, such as words, phrases, or symbols. It is an essential step in NLP for further processing and text normalization.\\n2. Lemmatization involves reducing a word to its base or root form, called a lemma, considering the vocabulary and morphology. It plays a crucial role in various NLP tasks like text-to-speech, machine translation, and sentiment analysis.\\n3. Tokenization and Lemmatization are vital in NLP workflows, ensuring text analysis and normalization, reducing complexity and variability of natural language.\\n4. We can use a popular NLP library like NLTK or spaCy to implement tokenization and lemmatization effectively.\\n5. A practical example will be demonstrated, showcasing the implementation of tokenization and lemmatization using code, highlighting the impact on text normalization and analysis.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': 'This slide aims to provide students with a practical understanding of how Text Tokenization and Lemmatization are implemented in NLP. The focus is on enhancing the student’s practical skills by allowing them to encode these concepts into a workflow, which is crucial for their academic applications and will assist with the NLP class objectives.', 'concepts': ['Tokenization of Text', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Implementing Tokenization and Lemmatization in NLP Workflows', 'presentation': \"Good afternoon! On this slide, we're going to explore the fascinating world of Natural Language Processing (NLP) and dive into two important concepts: Tokenization of Text and Lemmatization. These processes play a crucial role in NLP workflows and have practical applications in various fields.\\n\\nTokenization is the first step in NLP, where we break down a sequence of characters into smaller units called tokens. These tokens can be words, phrases, symbols, or other meaningful elements. Ultimately, tokenization allows us to convert text into a format suitable for further analysis, such as parsing and part-of-speech tagging.\\n\\nLemmatization, on the other hand, goes beyond simple stemming by reducing words to their base or root form, called a lemma. This process takes into account the word's morphological analysis and its part of speech in a sentence. It's a more sophisticated approach that ensures higher accuracy when normalizing words.\\n\\nNow, let's move beyond theory and look at the practical side of things. We'll explore how tokenization and lemmatization are implemented in NLP workflows. For example, we'll show you code snippets and workflow diagrams to demonstrate how these processes are used in tasks like text-to-speech, machine translation, and sentiment analysis.\\n\\nBy the end of this slide, you'll have a solid understanding of how tokenization and lemmatization fit into the bigger picture of NLP. You'll be able to connect the theoretical concepts with real-world applications, and see how they contribute to successful outcomes in various NLP tasks. So, let's dive in and explore the fascinating world of tokenization and lemmatization in NLP!\", 'content': '1. Tokenization of Text: The process of converting text into a sequence of tokens, which are smaller units such as words or phrases. Tokenization is a fundamental step in NLP and helps with tasks like parsing and Part-of-Speech tagging.\\n\\n2. Lemmatization: A process in NLP that reduces words to their base or dictionary form, known as a lemma. Unlike stemming, lemmatization considers the vocabulary and morphological analysis, resulting in higher accuracy.\\n\\n3. Practical Application: Tokenization and lemmatization are implemented in NLP workflows. They play a crucial role in text normalization and are used in tasks like text-to-speech, machine translation, and sentiment analysis.\\n\\n4. Code Examples: Code snippets and workflow diagrams will demonstrate how to implement tokenization and lemmatization in an NLP workflow.\\n\\n5. Academic Relevance: Understanding and applying tokenization and lemmatization are important for succeeding in NLP coursework and future NLP projects.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': 'This slide aims to provide students with a practical understanding of how Text Tokenization and Lemmatization are implemented in NLP. The focus is on enhancing the student’s practical skills by allowing them to encode these concepts into a workflow, which is crucial for their academic applications and will assist with the NLP class objectives.', 'concepts': ['Tokenization of Text', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Advancing Natural Language Understanding: Text Analysis Essentials', 'presentation': \"Welcome to the slide titled 'Advancing Natural Language Understanding: Text Analysis Essentials'. In this slide, we will delve into the intriguing world of Text Analysis as part of Natural Language Processing (NLP). Our aim is to enhance your understanding and practical knowledge of this field.\\n\\nText Analysis involves extracting meaningful information from natural language text. It comprises various methodologies and technologies used in NLP, such as Regular Expressions, Text Normalization, and understanding the Edit Distance between strings. By using these techniques, we can structure unstructured text data and perform tasks like sentiment analysis, topic modeling, and named entity recognition.\\n\\nOne of the fundamental concepts in NLP is language modeling. Language modeling focuses on predicting the likelihood of word sequences in a natural language. This concept serves as the basis for various NLP applications, including speech recognition, machine translation, and text generation.\\n\\nIn this slide, we will explore the key concepts of Regular Expressions, Text Normalization, and Edit Distance in the context of NLP. Regular Expressions are a powerful tool for text processing, allowing us to define search patterns with various basic syntax and operators. These patterns are used in tasks such as text searching, data validation, and tokenization of text.\\n\\nText Normalization, on the other hand, involves converting text into a more uniform format. It includes tasks like tokenization of text, lemmatization, and sentence segmentation techniques. Text Normalization helps in reducing lexical variety and complexity, making NLP tasks more efficient and accurate.\\n\\nEdit Distance is a measure of how dissimilar two strings (e.g., words) are by counting the minimum number of operations required to transform one string into the other. It is essential for tasks such as spell checking, plagiarism detection, and genome sequence analysis. We will explore the calculation of Edit Distance and its applications in NLP.\\n\\nThroughout the slide, we will provide practical examples and exercises to help solidify your understanding of these concepts. We will also discuss their applications in modern NLP, such as chatbots.\\n\\nBy the end of this slide, you will have a solid foundation in the key concepts of text analysis in NLP, and you will be equipped with the knowledge necessary to excel in your NLP class. Let's dive in and discover the fascinating world of Text Analysis!\", 'content': '1. Introduction to Text Analysis: Extracting meaningful information from natural language text using various methodologies and technologies in NLP.\\n\\n2. Regular Expressions: Powerful tools for text processing, allowing the specification of complex search patterns for tasks like text searching and substitution.\\n\\n3. Text Normalization: Process of converting text into a more uniform format, including tasks such as tokenization, lemmatization, and sentence segmentation.\\n\\n4. Edit Distance: Measure of dissimilarity between strings, used in spell checking, plagiarism detection, and genome sequence analysis.\\n\\n5. Practical Examples and Applications: Demonstration of how these concepts are applied in modern NLP applications like chatbots, with hands-on exercises and exercises to reinforce the understanding of the material.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide aims to bridge foundational NLP concepts with underexplored yet fundamental areas of Text Analysis. It will introduce Text Analysis and elucidate on its various applications in Natural Language Processing, enriching the student's academic curriculum and facilitating better performance in the NLP class through practical, example-driven learning.\", 'concepts': ['Text Analysis', 'language modeling', 'genome', 'Natural Language Processing (NLP)', 'Regular Expressions', 'Text Normalization', 'Edit Distance']}\n",
      "\n",
      "{'title': 'Unlocking Advanced Text Normalization Techniques in NLP', 'presentation': \"Welcome to the slide 'Unlocking Advanced Text Normalization Techniques in NLP'. In this slide, we will explore advanced techniques used in Natural Language Processing or NLP. These techniques are essential for text processing in NLP applications and involve regular expressions, text normalization, and edit distance.\\n\\nLet's begin by talking about regular expressions. Regular expressions are powerful tools used to match patterns in text. They play a crucial role in text cleaning and preprocessing for NLP. You can think of them as a kind of language for specifying patterns in text. Regular expressions consist of a combination of symbols and metacharacters that allow you to perform various operations on a text.\\n\\nThe use of regular expressions in NLP extends beyond just basic text matching. They enable us to perform tasks like tokenization, sentence segmentation, and even more complex operations like named entity recognition. By understanding and utilizing regular expressions, you will gain the ability to process and manipulate text in powerful and efficient ways, which is fundamental in NLP.\\n\\nMoving on to the concept of text normalization. Text normalization is the process of converting text into its canonical or standard form. It involves tasks like tokenization and lemmatization. Tokenization is the process of breaking text into individual tokens or words. This is essential for further analysis as it allows us to work with smaller units of text. Lemmatization, on the other hand, aims to reduce words to their base or root form. This helps to reduce information redundancy and ensures consistent interpretation of text by NLP models.\\n\\nLastly, we will discuss the concept of edit distance. Edit distance is a measure of the similarity between two strings. It quantifies the minimum number of operations required to transform one string into another. This concept is particularly useful in tasks like spell-checking systems, plagiarism detection, and even DNA sequence alignment. By understanding edit distance, you will gain insight into how similarity among texts can be measured and how it can be applied in various NLP applications.\\n\\nTo reinforce your understanding of these techniques, we will provide practical examples and real-world applications using regular expressions, text normalization, and edit distance. We will explore how these concepts can be applied in tasks like chatbot development, where understanding and normalizing user input are crucial for effective communication.\\n\\nBy the end of this slide, you will have a solid understanding of advanced text normalization techniques in NLP and their practical applications. You will be well-equipped to work with text data and apply these techniques in your own NLP projects. So let's dive in and unlock the world of advanced text normalization in NLP!\", 'content': '1. Introduction to Regular Expressions in NLP, covering basic syntax and operators. \\n\\n2. Practical examples showcasing Regular Expressions in text normalization, including sentence segmentation and tokenization. \\n\\n3. Importance of lemmatization and its role in reducing information redundancy and ensuring consistent text interpretation. \\n\\n4. Exploration of Edit Distance concept, its formal equation, and real-world applications such as spell-check systems and plagiarism detection. \\n\\n5. A case study demonstrating the practical applications of the discussed concepts in developing a chatbot, highlighting text normalization, regular expressions, and Edit Distance for handling user input variations.', 'latex_codes': '', 'purpose': 4, 'purpose_statement': \"To provide practical application and reinforcement of learned concepts in advanced text normalization techniques, enhancing the student's proficiency in text processing for NLP applications.\", 'concepts': ['Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Unlocking Advanced Text Normalization Techniques in NLP', 'presentation': \"Good day! Today, we'll be exploring the fascinating world of advanced text normalization techniques in Natural Language Processing (NLP). In this session, we'll delve into the concepts of regular expressions, text normalization, and edit distance, understanding how they play a crucial role in NLP applications.\\n\\nTo begin, let's take a quick review of the foundational concepts of regular expressions, text normalization, and edit distance within NLP. Regular expressions are powerful patterns used to search, match, and manipulate text data. Text normalization involves transforming raw text into a standardized format, making it easier to process. Edit distance, on the other hand, measures the similarity between two strings, benefiting tasks like spell checking and duplicate detection.\", 'content': '1. Foundational concepts: Regular Expressions, Text Normalization, and Edit Distance in NLP.\\n2. Application of concepts in modern NLP tasks and their impact on language model performance.\\n3. Practical examples of advanced text normalization techniques, including handling typos, slang, and other irregularities in text data.\\n4. Tokenization strategies and the importance of lemmatization in text preprocessing.\\n5. Edit Distance algorithm, its role in measuring similarity between strings, and its application in spell-checking and duplicate detection.\\n6. Interactive exercises to apply the concepts, particularly in chatbot scenarios.', 'latex_codes': '', 'purpose': 4, 'purpose_statement': \"To provide practical application and reinforcement of learned concepts in advanced text normalization techniques, enhancing the student's proficiency in text processing for NLP applications.\", 'concepts': ['Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Delving Deeper: Advanced Applications of Edit Distance in NLP', 'presentation': \"Good day! In our journey through Natural Language Processing, we've encountered a concept called Edit Distance, which measures the dissimilarity between two text strings. Today, we will delve deeper into Edit Distance and explore its advanced applications in the fields of NLP.\\n\\nOne of the fascinating applications of Edit Distance is in plagiarism detection. By employing Edit Distance algorithms alongside Text Normalization techniques, we can compare textual content and enhance the accuracy of plagiarism detection software. This is particularly important for academic integrity, ensuring that proper credit is given and intellectual property is respected.\\n\\nMoving from the textual realm to the biological realm, Edit Distance also plays a crucial role in genome sequence analysis. In the subfield of Comparative Genomics, Edit Distance helps us understand the evolutionary relationships between different species by aligning genome sequences and identifying similarities and differences. With this powerful computational tool, scientists can gain insights into the structure, function, and evolutionary processes of genomes.\\n\\nTo further solidify your understanding, let's explore some practical examples. Imagine a chatbot that uses Edit Distance to analyze human conversation and respond intelligently. This application showcases the syntactic and semantic understanding that Edit Distance can bring to chatbots, making them more conversational and engaging.\\n\\nSimilarly, Edit Distance is essential for detecting plagiarism in scholarly works, ensuring that originality is preserved and academic honesty is maintained. By applying Edit Distance algorithms, we can identify similar strings of text and determine the level of similarity between documents. This is crucial for verifying the authenticity of written work.\\n\\nAdditionally, Edit Distance is widely used for genome comparison tasks, where it helps researchers analyze genetic sequences and detect mutations that may lead to diseases. By aligning and comparing genome sequences, scientists can gain insights into the structure and function of different genes, ultimately advancing our understanding of human health and biology.\\n\\nOverall, Edit Distance holds great significance in the field of NLP, stretching its arms out to diverse applications in plagiarism detection, genome sequence analysis, and beyond. As you explore these advanced applications, remember that Edit Distance is not just an abstract concept but a powerful tool that spans various disciplines, connecting academia and real-world implications. Let's embrace the depth and breadth of Edit Distance in our exciting journey through Natural Language Processing!\", 'content': '1. Edit Distance:  Edit Distance is a way of quantifying the dissimilarity between two strings by counting the minimum number of operations (insertions, deletions, substitutions) required to transform one string into the other.\\n\\n2. Advanced Applications in NLP:  Edit Distance finds advanced applications in NLP, such as plagiarism detection and genome sequence analysis.\\n\\n3. Plagiarism Detection: Edit Distance algorithms combined with Text Normalization techniques result in more accurate plagiarism detection, aiding in identifying instances where text has been copied without proper authorization and attribution.\\n\\n4. Genome Sequence Analysis: Edit Distance plays a crucial role in Comparative Genomics, offering insights into the genetic relationships between species by identifying similarities and differences in their DNA sequences.\\n\\n5. Real-world Applications: Edit Distance is used in various real-world applications, such as creating chatbots with better syntactic and semantic understanding, enhancing plagiarism detection software, and assisting comparative genomics researchers in understanding evolutionary pathways.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide aims to build upon the foundational understanding of the Edit Distance concept by exploring its advanced applications in the fields of NLP, specifically focusing on plagiarism detection, genome sequence analysis, and comparative genomics, which have not been fully examined in previous sessions. The goal is to leverage this deeper dive to enhance the student's comprehension for their NLP class and to nourish their interest in wide-ranging practical applications.\", 'concepts': ['Edit Distance', 'plagiarism detection', 'Comparative Genomics', 'genome sequence analysis']}\n",
      "\n",
      "{'title': 'Exploring the Nuances of Sentence Segmentation in Text Analysis', 'presentation': \"Alright, let's dive into the fascinating world of sentence segmentation in text analysis. Today, we'll explore the nuances of breaking down text into meaningful sentences, an essential step in Natural Language Processing (NLP). We'll build on what you've already learned about text normalization and tokenization to enhance your understanding of how texts are prepared for NLP tasks. So, let's get started!\\n\\nSentence segmentation is all about dividing a block of text into its constituent sentences. This may sound simple, but it can actually be quite challenging due to various factors like punctuation, capitalization, and linguistic intricacies. To truly grasp the complexities, we'll cover different techniques and strategies used to accurately identify sentence boundaries.\\n\\nFirst, let's understand why sentence segmentation is crucial in text analysis. It lays the foundation for many NLP applications, such as sentiment analysis, topic modeling, and named entity recognition. By breaking text into sentences, we can extract meaningful information from unstructured data.\\n\\nOne technique we'll explore is the use of regular expressions. These powerful search patterns help identify potential sentence boundaries by detecting punctuation patterns. For example, we can look for periods followed by a space and an uppercase letter. We'll dive into the basic syntax and operators of regular expressions and provide practical examples to solidify your understanding.\\n\\nAdditionally, we'll discuss how capitalization cues and linguistic patterns play a role in sentence segmentation. Sometimes, punctuation alone is not enough, and we need to consider contextual elements. We'll examine how linguistic and grammatical patterns can guide the segmentation process.\\n\\nIt's important to note that inaccuracies in sentence segmentation can have a significant impact on downstream NLP tasks. We'll address common challenges and discuss strategies to improve accuracy, combining multiple techniques to achieve better results.\\n\\nTo make the concepts more tangible, we'll explore how machine learning models can be leveraged for sentence segmentation. These models learn from linguistic context beyond predetermined rules and patterns, enhancing the segmentation process. We'll provide visual representations and examples, allowing you to see the segmentation in action.\\n\\nThroughout the presentation, I'll make connections to real-world applications, such as chatbots and text-to-speech systems. This will highlight the practical relevance of precise sentence segmentation and prepare you for scenarios you may encounter in your NLP journey.\\n\\nBy the end of this presentation, you'll have a deeper understanding of sentence segmentation, its challenges, and its importance in NLP. You'll be well-prepared to tackle academic evaluations and apply your knowledge to real-world NLP tasks. So, let's embark on this exploration together and unlock the secrets of sentence segmentation!\", 'content': '1. Introduction to Sentence Segmentation Techniques:\\n- Sentence Segmentation Techniques involve methods and algorithms used in Natural Language Processing (NLP) to divide a text into its constituent sentences.\\n\\n2. Importance of Sentence Segmentation:\\n- Discuss the significance of accurate sentence segmentation as a crucial preprocessing step in NLP tasks.\\n\\n3. Challenges in Sentence Segmentation:\\n- Highlight the complexities posed by punctuation, capitalization, and linguistic structures, and the impact of inaccuracies in subsequent NLP tasks.\\n\\n4. Common Sentence Segmentation Techniques:\\n- Explore techniques such as punctuation cues, capitalization, rule-based systems, and machine learning models.\\n\\n5. Practical Applications of Sentence Segmentation:\\n- Demonstrate the importance of accurate segmentation in modern NLP applications like chatbots and text-to-speech systems.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide will delve into the intricacies of segmenting text into sentences, an important pre-processing step in NLP, and how it paves the way for more advanced text analysis. As we have already touched upon Text Normalization and Tokenization, understanding Sentence Segmentation will enrich the student's comprehension of how texts are prepared for NLP tasks.\", 'concepts': ['Sentence Segmentation Techniques', 'Text Analysis', 'Natural Language Processing (NLP)', 'Regular Expressions']}\n",
      "\n",
      "{'title': 'Exploring the Nuances of Sentence Segmentation in Text Analysis', 'presentation': \"Good day! Today, we are going to explore the nuances of sentence segmentation in text analysis. Sentence segmentation involves dividing a text into its constituent sentences, and it is an important pre-processing step in Natural Language Processing (NLP). By accurately identifying sentence boundaries, we can unlock more advanced text analysis techniques.\\n\\nImagine you have a text that you want to analyze. You need to be able to separate it into individual sentences so you can perform tasks like sentiment analysis, topic modeling, and named entity recognition. For example, let's say you want to analyze customer reviews of a product to determine overall sentiment. Being able to isolate each sentence allows you to assess sentiment on a sentence-by-sentence basis, giving you more granular insights.\\n\\nBut why is sentence segmentation challenging? Punctuation marks can complicate the process. For instance, periods can indicate the end of a sentence, but they can also appear in abbreviations and decimal numbers. Capitalization can also be helpful, but it may not always be a reliable indicator, especially in different languages and text corpora.\\n\\nTo overcome these challenges, various techniques have been developed. One approach is to rely on punctuation cues to identify sentence boundaries. Another is to analyze capitalization patterns in the text. Additionally, there are machine learning models that can be trained to detect sentence boundaries, as well as rule-based systems that consider the linguistic and contextual structure of the text.\\n\\nThink about your objectives as a computer science student and how sentence segmentation can enhance your NLP skills. Mastering this technique will enable you to better understand and analyze text data, empowering you to develop more sophisticated NLP applications in the future.\\n\\nBy the end of this lesson, you will have a solid understanding of sentence segmentation techniques and how they fit into the broader field of text analysis. Are you ready to dive in?\", 'content': '1. Introduction to Sentence Segmentation Techniques\\n2. Challenges in identifying sentence boundaries\\n3. Common methods for sentence segmentation\\n4. Connection to Text Normalization and Tokenization\\n5. Importance of sentence segmentation in advanced text analysis', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide will delve into the intricacies of segmenting text into sentences, an important pre-processing step in NLP, and how it paves the way for more advanced text analysis. As we have already touched upon Text Normalization and Tokenization, understanding Sentence Segmentation will enrich the student's comprehension of how texts are prepared for NLP tasks.\", 'concepts': ['Sentence Segmentation Techniques', 'Text Analysis', 'Natural Language Processing (NLP)', 'Regular Expressions']}\n"
     ]
    }
   ],
   "source": [
    "### Slide generation from AITutor\n",
    "notebank = NoteBank()\n",
    "[notebank.add_note(n) for n in tutor_plan.split(\"\\n\")]\n",
    "slide_planner = SlidePlanner(notebank, concept_db)\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"./temp_slideplan.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(\"./temp_slideplan.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        slide_plans = pkl.load(f)\n",
    "        print(\"\\n\\n\".join([str(slide.format_json()) for slide in slide_plans]))\n",
    "        slide_planner.SlidePlans = slide_plans\n",
    "else:\n",
    "    slide_planner.generate_slide_plan()\n",
    "    with open(\"./temp_slideplan.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(slide_planner.SlidePlans, f)\n",
    "\n",
    "\n",
    "if os.path.exists(\"./temp_slides.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(\"./temp_slides.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        slides = pkl.load(f)\n",
    "        print(\"\\n\\n\".join([str(slide.format_json()) for slide in slides]))\n",
    "        slide_planner.Slides = slides\n",
    "else:\n",
    "    slide_planner.generate_slide_deque()\n",
    "    with open(\"./temp_slides.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(slide_planner.Slides, f)\n",
    "\n",
    "print(\"\\n\\n\".join([str(slide.format_json()) for slide in slide_planner.Slides]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "- Preprocessing\n",
    "- Generation of questions from (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERRORS\n",
    "\n",
    "\n",
    "the thing we are checking for errors is number of api calls per errors. api calls during translation / errors during translation\n",
    "gpt-4 and gpt-3.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCEPTS RATIO OF NUMBER OF RELEVANT CONCEPTS OVER NUMBER OF CONCEPS\n",
    "GPT-3.5 vs GPT-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
