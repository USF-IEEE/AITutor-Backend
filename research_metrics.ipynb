{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricizing LLMaAiTB-E\n",
    "\n",
    "- Our Focus: Generation Quality\n",
    "- Measurement Techniques: \n",
    "    - Vector Comparison\n",
    "    - Human Preference Sample (A/B/C Testing)\n",
    "    - ???\n",
    "- Iterative documents to measure:\n",
    "    - Concepts (Generation phase)\n",
    "    - Slides (Teaching phase)\n",
    "    - Questions (Testing phase)\n",
    "- Resources for testing:\n",
    "    - Expert (From classes)\n",
    "    - GPT4 (Generation)\n",
    "    - LLMaAiTB-E (Teachabull)\n",
    "- Main concepts to cover:\n",
    "    - Object Oriented Programming\n",
    "    - Programming Language Semantics\n",
    "    - Math\n",
    "    - History\n",
    "    - \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Helper Functions\n",
    "We will demonstrate our metrics using OpenAI's Vector Embeddings on our generated documents. We decided to use OpenAI's embeddings due to their large document size capacity. We agreed that this method would prove to be the best while comparing large documents.\n",
    "\n",
    "## LLM Prompt/Text Completion\n",
    "\n",
    "\n",
    "## Vector Comparison\n",
    "Embeddings: OpenAI’s text embeddings measure the relatedness of text strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pickle as pkl\n",
    "from AITutor_Backend.src.TutorUtils.concepts import *\n",
    "from AITutor_Backend.src.TutorUtils.notebank import NoteBank\n",
    "from AITutor_Backend.src.TutorUtils.slides import SlidePlan, Slide, SlidePlanner, Purpose, Concept, ConceptDatabase\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPENAI HELPER FUNCTIONS \n",
    "def request_output_from_llm(prompt, model: str):\n",
    "    \"\"\"Requests the Concept information from an LLM.\n",
    "\n",
    "    Args:\n",
    "        prompt: (str) - string to get passed to the model\n",
    "        model: (str) - \n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI() \n",
    "    \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "    {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": prompt,\n",
    "    },\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=8000,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts\n",
    "- Preprocessing\n",
    "- Generation of Graph for (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTEBANK from AI Tutor\n",
    "tutor_plan = '''Main Concept: Regular Expressions, Text Normalization, Edit Distance in Natural Language Processing (NLP)\n",
    "Student is a computer science student with no prior knowledge of the topic, requiring an introductory lesson.\n",
    "Student is taking an NLP class, suggesting the lessons are for academic purposes and should cover necessary conceptual detail.\n",
    "Student provided a chapter summary that includes key subtopics; this will be a guide in structuring the lesson plan.\n",
    "Tutor shall educate on the following concepts:\n",
    "Subconcept: Introduction to Regular Expressions\n",
    "Subconcept: Uses of Regular Expressions in NLP\n",
    "Subconcept: Basic Syntax and Operators of Regular Expressions\n",
    "Subconcept: Practical Examples and Exercises Using Regular Expressions\n",
    "Subconcept: Introduction to Text Normalization\n",
    "Subconcept: Tokenization of Text\n",
    "Subconcept: Lemmatization and its Importance\n",
    "Subconcept: Sentence Segmentation Techniques\n",
    "Subconcept: Introduction to Edit Distance\n",
    "Subconcept: Applications of Edit Distance Algorithm in NLP\n",
    "Subconcept: Calculation of Edit Distance and String Alignment\n",
    "Tutor will apply practical examples relevant to modern NLP applications, such as chatbots, using the chapter summary as a conversational context.\n",
    "Tutor will provide hands-on practice problems and ensure the student understands the implementation of the concepts.\n",
    "Student's objective: To gain a foundational understanding of the chapter's main points, to apply this understanding in an academic setting, and to perform well in the NLP class.\n",
    "Since the student might need to have a deep understanding of the class material, the lesson should provide a solid theoretical basis, followed by practical application.\n",
    "Concept: Regular Expressions, Text Normalization, Edit Distance in Natural Language Processing (NLP)\n",
    "Concept: Introduction to Regular Expressions\n",
    "Concept: Uses of Regular Expressions in NLP\n",
    "Concept: Basic Syntax and Operators of Regular Expressions\n",
    "Concept: Practical Examples and Exercises Using Regular Expressions\n",
    "Concept: Introduction to Text Normalization\n",
    "Concept: Tokenization of Text\n",
    "Concept: Lemmatization and its Importance\n",
    "Concept: Sentence Segmentation Techniques\n",
    "Concept: Introduction to Edit Distance\n",
    "Concept: Applications of Edit Distance Algorithm in NLP\n",
    "Concept: Calculation of Edit Distance and String Alignment\n",
    "Concept: Practical Examples and Exercises in Modern NLP Applications (e.g., Chatbots)\n",
    "Concept: Hands-on Practice Problems\n",
    "Concept: Foundational Understanding of Main Points\n",
    "Concept: Academic Application of Concepts\n",
    "Concept: Theoretical Basis Followed by Practical Application\n",
    "Student's Interest Statement: I find natural language processing interesting and important since I am taking it as a course in college where I will be tested\n",
    "Student's Slides Preference Statement: I want to be taught by information and examples\n",
    "Student's Questions Preference Statement: 2 of multiple choice, 2 of free response and 2 coding questions'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"Natural Language Processing: Regular Expressions, Text Normalization, Edit Distance\",\n",
      "\"definition\": \"Natural Language Processing (NLP) encompasses a suite of techniques for enabling computers to understand and process human languages. Within this field, Regular Expressions are a powerful tool for text pattern recognition, allowing the automation of searching, editing, and manipulation of text. Text Normalization is a preprocessing step which involves transforming text into a consistent format, often through Tokenization of Text , Lemmatization , and Sentence Segmentation Techniques . Edit Distance is a way of quantifying how dissimilar two strings (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other, which has various Applications of Edit Distance Algorithm in NLP .\",\n",
      "\"latex\": \"Natural Language Processing: Regular Expressions, Text Normalization, Edit Distance\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Regular Expressions\",\n",
      "\"definition\": \"Regular Expressions, often abbreviated as regex or regexp, are a sequence of characters that define a search pattern, primarily for use in pattern matching with strings, or string matching, in Natural Language Processing (NLP) . They serve as a powerful tool for text processing, allowing the specification of complex search patterns with various Basic Syntax and Operators of Regular Expressions . Uses of Regular Expressions in NLP include text searching, text substitution, data validation, and is foundational for various Text Normalization techniques such as Tokenization of Text and Sentence Segmentation Techniques .\",\n",
      "\"latex\": \"Regular Expressions\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Natural Language Processing (NLP)\",\n",
      "\"definition\": \"Natural Language Processing (NLP) is a field of computer science and artificial intelligence concerned with the interactions between computers and human (natural language</Concept>). The ultimate objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both meaningful and useful. NLP encompasses a range of techniques and tools that allow for text analysis , language modeling , machine translation , sentiment analysis , and more, making it essential in the development of applications such as speech recognition systems, chatbots , and text-to-speech systems.\",\n",
      "\"latex\": \"Natural Language Processing (NLP)\",\n",
      "}\n",
      "\n",
      "{\"name\": \"(natural\",\n",
      "\"definition\": \"The concept of (natural might refer to an incorrectly formatted or incomplete token, typically encountered in programming or data parsing where the string literal '(natural' may appear as part of a larger expression or construct. This concept may relate to syntax errors, string literals, and the importance of proper tokenization in programming and Natural Language Processing (NLP).\",\n",
      "\"latex\": \"(natural\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Text Analysis\",\n",
      "\"definition\": \"Text Analysis is a broad term for various processes that involve extracting meaningful information from natural language text. It encompasses a range of methodologies and technologies that Natural Language Processing (NLP) employs, such as Regular Expressions , Text Normalization , and understanding the Edit Distance between strings. These techniques allow for the structuring of unstructured text data, enabling tasks like sentiment analysis, topic modeling, and named entity recognition.\",\n",
      "\"latex\": \"Text Analysis\",\n",
      "}\n",
      "\n",
      "{\"name\": \"language modeling\",\n",
      "\"definition\": \"Language modeling is the task of predicting the likelihood of a sequence of words in a natural language . It is a fundamental concept in Natural Language Processing (NLP) , which involves developing probabilistic models that can generate or determine the probability distribution of linguistic units, usually in the form of words or sentences. These models are the basis for various NLP applications such as speech recognition , machine translation , and text generation .\",\n",
      "\"latex\": \"language modeling\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Machine Translation\",\n",
      "\"definition\": \"Machine Translation (MT) is a subfield of Natural Language Processing (NLP) that focuses on the problem of automatically translating text or speech from one language to another. It involves the use of software to translate text or speech, often employing complex algorithms and language modeling . MT can be approached in several ways, including rule-based, statistical, and neural methods, each with its own strengths and applications.\",\n",
      "\"latex\": \"Machine Translation\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Sentiment Analysis\",\n",
      "\"definition\": \"Sentiment Analysis, often referred to as opinion mining, is a subfield of Natural Language Processing (NLP) that involves the use of algorithms and techniques to extract subjective information—such as opinions, attitudes, and emotions—from textual data. This process often involves the classification of texts into categories like positive, negative, or neutral sentiment. Sentiment Analysis is widely used in fields such as business analytics, social media monitoring, and customer feedback to understand consumer attitudes and to inform decision-making.\",\n",
      "\"latex\": \"Sentiment Analysis\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Speech Recognition\",\n",
      "\"definition\": \"Speech recognition is a technology that allows computers to recognize and interpret human speech as a means of input. It is a subfield of Natural Language Processing (NLP) and involves the conversion of spoken language into text. This technology utilizes language modeling , Machine Translation , and Sentiment Analysis to understand and process the speech data accurately.\",\n",
      "\"latex\": \"Speech Recognition\",\n",
      "}\n",
      "\n",
      "{\"name\": \"chatbots\",\n",
      "\"definition\": \"Chatbots are automated, typically text-based dialogue systems that interact with humans using Natural Language Processing (NLP) techniques. They are designed to simulate conversational experiences and can be found in customer service, personal assistants, and language modeling applications. Key elements in the functioning of chatbots include Regular Expressions for pattern matching, Text Normalization for preparing user input for processing, and Edit Distance algorithms to understand and correct misspellings or errors in user input.\",\n",
      "\"latex\": \"chatbots\",\n",
      "}\n",
      "\n",
      "{\"name\": \"text-to-speech\",\n",
      "\"definition\": \"Text-to-speech (TTS) is a type of speech synthesis application that converts written text into spoken words using voice generation . TTS is often used in Natural Language Processing (NLP) for applications such as assistive technology , Speech Recognition , and chatbots . It involves processes such as Text Analysis , language modeling , and the synthesis of speech waveforms .\",\n",
      "\"latex\": \"text-to-speech\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Basic Syntax and Operators of Regular Expressions\",\n",
      "\"definition\": \"The basic syntax and operators of Regular Expressions constitute the foundational grammar used to describe patterns in text for tasks in Natural Language Processing (NLP) . This syntax includes a set of special characters that enable the matching of a wide variety of string patterns. Common operators include literals, wildcards, character classes, quantifiers, anchors, and grouping constructs, each serving a specific purpose in pattern matching.\",\n",
      "\"latex\": \"Basic Syntax and Operators of Regular Expressions\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Uses of Regular Expressions in NLP\",\n",
      "\"definition\": \"The uses of Regular Expressions in Natural Language Processing (NLP) are diverse and crucial. Regular expressions are utilized for tasks such as searching and matching patterns within text, data extraction and cleaning, Text Analysis , Sentiment Analysis , and parsing text data efficiently. They are also fundamental in Text Normalization processes like Tokenization of Text , Sentence Segmentation Techniques , and preparing text for more complex tasks such as language modeling , Machine Translation , Speech Recognition , and interacting with chatbots .\",\n",
      "\"latex\": \"Uses of Regular Expressions in NLP\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Text Normalization\",\n",
      "\"definition\": \"Text Normalization is a process in Natural Language Processing (NLP) that involves converting text into a more uniform format. It includes tasks such as Tokenization of Text , Lemmatization and its Importance , and Sentence Segmentation Techniques . Text Normalization is essential for preparing raw text for further processing like text-to-speech synthesis, Machine Translation , and Sentiment Analysis . It helps in reducing lexical variety and complexity, thus making NLP tasks more efficient and accurate.\",\n",
      "\"latex\": \"Text Normalization\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Tokenization of Text\",\n",
      "\"definition\": \"Tokenization of Text is the process in Natural Language Processing (NLP) of converting a sequence of characters into a sequence of tokens. A token is typically a word, but can also be a phrase, symbol, or other meaningful element called a lexeme within the text. Tokenization is a fundamental step before further processing such as parsing , Part-of-Speech tagging , and Text Analysis . It is crucial for Text Normalization as it helps in reducing inflectional forms and sometimes derivationally related forms of a word to a common base form.\",\n",
      "\"latex\": \"Tokenization of Text\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Sentence Segmentation Techniques\",\n",
      "\"definition\": \"Sentence Segmentation Techniques involve methods and algorithms used in Natural Language Processing (NLP) to divide a text into its constituent sentences. The goal is to accurately identify sentence boundaries, which can be complicated by the use of punctuation for other purposes, such as decimal points or abbreviations. Common techniques include the use of punctuation cues, capitalization, machine learning models, and rules-based systems that consider the linguistic and contextual structure of the text.\",\n",
      "\"latex\": \"Sentence Segmentation Techniques\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Lemmatization\",\n",
      "\"definition\": \"Lemmatization is a process in Natural Language Processing (NLP) that involves reducing a word to its base or root form, called a lemma. Unlike stemming, which crudely chops off word endings, Lemmatization considers the vocabulary and morphological analysis of words, which makes it more sophisticated and accurate. It is an essential step in Text Normalization and plays a crucial role in various NLP tasks like text-to-speech , Machine Translation , and Sentiment Analysis .\",\n",
      "\"latex\": \"Lemmatization\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Edit Distance\",\n",
      "\"definition\": \"In the context of Natural Language Processing (NLP) , Edit Distance is a way of quantifying how dissimilar two strings (e.g., words) are by counting the minimum number of operations required to transform one string into the other. The operations typically include insertions, deletions, or substitutions of a single character. Understanding Edit Distance is important for various NLP tasks such as spell checking , plagiarism detection , and genome sequence analysis . The algorithm used for computing edit distance is known as the Levenshtein algorithm .\",\n",
      "\"latex\": \"Edit Distance\",\n",
      "}\n",
      "\n",
      "{\"name\": \"spell checking\",\n",
      "\"definition\": \"Spell checking is the process of detecting and correcting spelling errors in text. This process often involves the use of algorithms such as the Edit Distance algorithm to compare words against a dictionary of correct spellings. Spell checking is an essential component in various Natural Language Processing (NLP) applications like text-to-speech , chatbots , and Sentiment Analysis , where accurate text representation is crucial.\",\n",
      "\"latex\": \"spell checking\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Algorithms\",\n",
      "\"definition\": \"In computer science, an Algorithms is a finite sequence of well-defined, computer-implementable instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are essential for processing data, automating tasks, and providing decision-making logic for software and systems.\",\n",
      "\"latex\": \"Algorithms\",\n",
      "}\n",
      "\n",
      "{\"name\": \"plagiarism detection\",\n",
      "\"definition\": \"Plagiarism detection is the process of identifying instances where text or work has been copied without authorization and proper attribution. This concept is closely related to Natural Language Processing (NLP) , as it involves the analysis and comparison of texts using computational techniques. Tools for plagiarism detection often employ Regular Expressions to identify similar strings of text, utilize Edit Distance algorithms to measure the similarity between documents, and may apply Machine Learning models to discern patterns that indicate plagiarism. The effectiveness of plagiarism detection software can be augmented by understanding the principles of Text Normalization and Tokenization of Text to standardize and break down the text for more accurate comparisons.\",\n",
      "\"latex\": \"plagiarism detection\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Machine Learning\",\n",
      "\"definition\": \"Machine Learning is a subset of Artificial Intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It involves the development of algorithms that can learn from and make predictions or decisions based on data. This field intersects with various other concepts such as statistics , data mining , and predictive modeling .\",\n",
      "\"latex\": \"Machine Learning\",\n",
      "}\n",
      "\n",
      "{\"name\": \"genome sequence analysis\",\n",
      "\"definition\": \"Genome sequence analysis is the process of examining the sequence of DNA in a genome to understand its structure, function, and evolution. This involves various techniques and tools such as DNA sequencing , bioinformatics , genetic mapping , and comparative genomics . Through genome sequence analysis, scientists can identify genome , determine their functions, and detect mutations that may lead to diseases.\",\n",
      "\"latex\": \"genome sequence analysis\",\n",
      "}\n",
      "\n",
      "{\"name\": \"genome\",\n",
      "\"definition\": \"In biology, a genome refers to the complete set of genetic material present in an organism or a cell. This includes all of its genome , which are the basic units of heredity, as well as the non-coding sequences of DNA that have various regulatory functions. The study of genomes is a central part of genomics , which often involves the use of Natural Language Processing (NLP) techniques like Regular Expressions , Text Normalization , and Edit Distance in genome sequence analysis .\",\n",
      "\"latex\": \"genome\",\n",
      "}\n",
      "\n",
      "{\"name\": \"DNA sequencing\",\n",
      "\"definition\": \"DNA sequencing is the process of determining the precise order of nucleotides within a DNA molecule. It includes any method or technology that is used to determine the order of the four bases—adenine, guanine, cytosine, and thymine—in a strand of DNA. This process has revolutionized the field of genomics and is essential for a wide range of applications, including genome sequence analysis , genetic testing , biomedical research , and forensic biology . With advancements in Machine Learning and Algorithms , DNA sequencing technology has become faster, cheaper, and more accurate, leading to significant breakthroughs in biology and medicine.\",\n",
      "\"latex\": \"DNA sequencing\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Bioinformatics\",\n",
      "\"definition\": \"Bioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data. It combines computer science , biology , chemistry , statistics , and mathematics to analyze and interpret biological data. Applications include genome sequence analysis and DNA sequencing .\",\n",
      "\"latex\": \"Bioinformatics\",\n",
      "}\n",
      "\n",
      "{\"name\": \"genetic mapping\",\n",
      "\"definition\": \"Genetic mapping is the process of determining the location and chemical sequence of specific genes on a DNA strand. This process involves techniques such as genome sequence analysis , which allows scientists to associate particular segments of DNA with specific characteristics or diseases. Genetic mapping is a key tool in Bioinformatics , aiding in tasks like genome editing, plagiarism detection in academic research, and understanding the genetic basis of diseases for better medical interventions.\",\n",
      "\"latex\": \"genetic mapping\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Comparative Genomics\",\n",
      "\"definition\": \"Comparative genomics is a field of biological research in which the genomic features of different organisms are compared. It is often used in genome sequence analysis to understand the structure, function, and evolutionary processes that affect genomes. Comparative genomics involves the alignment of genome sequences and the identification of similarities and differences to infer the evolutionary relationships between species. This field intersects with other areas such as Bioinformatics , genetic mapping , and DNA sequencing .\",\n",
      "\"latex\": \"Comparative Genomics\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Levenshtein algorithm\",\n",
      "\"definition\": \"The Levenshtein algorithm, also known as the Levenshtein distance, is a measure of the difference between two sequences. It is the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one word into the other. This algorithm is a key part of Edit Distance calculations in various applications such as spell checking , plagiarism detection , and genome sequence analysis in Bioinformatics .\",\n",
      "\"latex\": \"Levenshtein algorithm\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Applications of Edit Distance Algorithm in NLP\",\n",
      "\"definition\": \"The Edit Distance algorithm is utilized in Natural Language Processing (NLP) for various tasks, such as spell checking , plagiarism detection , and DNA sequence analysis in Bioinformatics . It measures the minimum number of operations required to transform one string into another, which is particularly useful in tasks involving string comparison and correction within NLP.\",\n",
      "\"latex\": \"Applications of Edit Distance Algorithm in NLP\",\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Concept generation from AITutor:\n",
    "import pickle as pkl\n",
    "notebank = NoteBank()\n",
    "[notebank.add_note(n) for n in tutor_plan.split(\"\\n\")]\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"./temp_concepts.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(\"./temp_concepts.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        concept = pkl.load(f)\n",
    "        concept_db = ConceptDatabase(\"Natural Language Processing: Regular Expressions, Text Normalization, Edit Distance\",notebank.env_string(), False)\n",
    "        concept_db.Concepts = concept\n",
    "else:\n",
    "    concept_db = ConceptDatabase(\"Natural Language Processing: Regular Expressions, Text Normalization, Edit Distance\",notebank.env_string())\n",
    "    with open(\"./temp_concepts.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(concept_db.Concepts, f)\n",
    "\n",
    "print(\"\\n\\n\".join([slide.format_json() for slide in concept_db.Concepts]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slides: \n",
    "- Preprocessing\n",
    "- Generation of Document for (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SLIDE OBJ PROMPTs\n",
    "prompt = ''' #Your task is to create a JSON object from a slide string. View the example Input and output, and then repeat the same for the provided input. \n",
    "Perform the conversion for each slide s in the input string such that s->json_object(s). You should be able to figure out which is the title and which is the description.\n",
    "IMPORTANT: Escape Characters in JSON Data can cause errors if the JSON Object or JSON data contains backslashes, which means they need to be properly escaped\n",
    "Avoid these errors: Invalid \\escape: line 24 column 72 (char 2199)\n",
    "By properly escaping your backslashes ('\\\\')\n",
    "IMPORTANT: If there is two words together, such as \"functionwhere\", without being separated with a white space, that most probably means that there is a new line ('\\n') or space (' ') in between them, e.g. \"function where\".\n",
    "\n",
    "// Input:\n",
    "Page 1 Content:\n",
    "Natural Language ProcessingProfessor John LicatoUniversity of South FloridaChapter 2:RegEx, Edit Distance\n",
    "\n",
    "----------------------------------------\n",
    "Page 2 Content:\n",
    "\"Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When you’re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.\"Regular Expressions\n",
    "----------------------------------------\n",
    "Page 3 Content:\n",
    "The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))Regular Expressions\n",
    "----------------------------------------\n",
    "Page 4 Content:\n",
    "The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')Regular Expressions\n",
    "----------------------------------------\n",
    "Page 5 Content:\n",
    "Creating regex objectsr’ = raw string\\d – placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)\n",
    "----------------------------------------\n",
    "Page 6 Content:\n",
    "Matching regex objects\n",
    "mo = match object – contains the result of our search>>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)>>> mo = phoneNumRegex.search(‘My number is 415-555-4242.’)>>> print(‘Phone number found: ’ + mo.group())Phone number found: 415-555-4242\n",
    "----------------------------------------\n",
    "Page 7 Content:\n",
    "Text Normalization•We will work a lot with large datasets / corpora•We often need to pre-process text•Tokenizing (segmenting) words•Normalizing word formats•Segmenting sentences (e.g. by using punctuation)\n",
    "----------------------------------------\n",
    "Page 8 Content:\n",
    "Tokenization – segmenting running text into words (or word-like units)>>> text = 'That U.S.A. poster-print costs $12.40...'>>> pattern = r\\'\\'\\', (?x)  # set flag to allow verbose regexps...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A....     | \\w+(-\\w+)*      # words with optional internal hyphens...     | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%...     | \\.\\.\\.          # ellipsis...     | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [... \\'\\'\\'>>> nltk.regexp_tokenize(text, pattern)['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n",
    "----------------------------------------\n",
    "Page 9 Content:\n",
    "Subword tokenization•How do we capture relations between words like:–new, newer–blow, blowing–precipitation, precipitate•Often useful to break tokens into *sub*words•Usually split into token learners, and token segmenters\n",
    "----------------------------------------\n",
    "Page 10 Content:\n",
    "Byte-pair encoding (BPE)•A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab VV <- all unique characters in C                  # initial set of tokens is charactersfor i = 1 to k do                                # merge tokens til k times    t_L, t_R <- Most frequent pair of adjacent tokens in C    t_new <- t_L + t_R                           # make new token by concatenating    V <- V + t_new                               # update the vocabulary    Replace each occurrence of t_L, t_R in C with t_new # and update the corpusreturn Vcorpus5 low_2 lowest_6 newer_3 wider_2 new_vocabulary_, d, e, i, l, n, o, r, s, t, wcorpus5 low _2 lowest _6 newer _3 wider _2 new _vocabulary_, d, e, i, l, n, o, r, s, t, w, er\n",
    "----------------------------------------\n",
    "...\n",
    "        \n",
    "// Output:\n",
    "        { \n",
    "                \\\"slides\\\":[\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Natural Language Processing\\\", \n",
    "                                \\\"Description\\\": \\\"Professor John Licato University of South Florida Chapter 2:RegEx, Edit Distance\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When you’re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Creating regex objects\\\", \n",
    "                                \\\"Description\\\": \\\"r’ = raw string\\d – placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Matching regex objects\\\", \n",
    "                                \\\"Description\\\": \\\">>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)>>> mo = phoneNumRegex.search(‘My number is 415-555-4242.’)>>> print(‘Phone number found: ’ + mo.group())Phone number found: 415-555-4242 mo = match object – contains the result of our search\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Text Normalization\\\", \n",
    "                                \\\"Description\\\": \\\"•We will work a lot with large datasets / corpora\\n•We often need to pre-process text\\n•Tokenizing (segmenting) words\\n•Normalizing word formats\\n•Segmenting sentences (e.g. by using punctuation) )\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Tokenization – segmenting running text into words (or word-like units)\\\", \n",
    "                                \\\"Description\\\": \\\">>> text = 'That U.S.A. poster-print costs $12.40...'\\n>>> pattern = r\\'\\'\\', (?x)  # set flag to allow verbose regexps\\n...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A\\n....     | \\w+(-\\w+)*      # words with optional internal hyphens\\n...     | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\\n...     | \\.\\.\\.          # ellipsis...     | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [\\n... \\'\\'\\'\\n>>> nltk.regexp_tokenize(text, pattern)\\n['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Subword tokenization\\\", \n",
    "                                \\\"Description\\\": \\\"•How do we capture relations between words like:\\n–new, newer\\n–blow, blowing\\n–precipitation, precipitate\\n•Often useful to break tokens into *sub*words•Usually split into token learners, and token segmenters\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Byte-pair encoding (BPE)\\\", \n",
    "                                \\\"Description\\\": \\\"•A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab V\\nV <- all unique characters in C                  # initial set of tokens is characters\\nfor i = 1 to k do                                # merge tokens til k times    \\nt_L, t_R <- Most frequent pair of adjacent tokens in C    \\nt_new <- t_L + t_R                           # make new token by concatenating    \\nV <- V + t_new                               # update the vocabulary    \\nReplace each occurrence of t_L, t_R in C with t_new # and update the corpus\\nreturn V\\ncorpus\\n5 low_\\n2 lowest_\\n6 newer_\\n3 wider_\\n2 new_\\nvocabulary\\n_, d, e, i, l, n, o, r, s, t, w\\ncorpus\\n5 low _\\n2 lowest _\\n6 newer _\\n3 wider _\\n2 new _\\nvocabulary\\n_, d, e, i, l, n, o, r, s, t, w, er\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        ...\n",
    "                ]\n",
    "        }\n",
    "Remember! Escape Characters in JSON Data: If the JSON Object or JSON data contains backslashes, they need to be properly escaped.\n",
    "Avoid these errors: Invalid \\escape: line 24 column 72 (char 2199)\n",
    "\n",
    "// Input:\n",
    "        $SLIDE$\n",
    "\n",
    "// Output:\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Slide helper functions\n",
    "import PyPDF2\n",
    "from pptx import Presentation\n",
    "import json\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"Reads a PDF file and prints the content of each page\"\"\"\n",
    "    slide_str = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "\n",
    "        for i in range(num_pages):\n",
    "            page = reader.pages[i]\n",
    "            text = page.extract_text()\n",
    "            slide_str += f\"Page {i+1} Content:\\n{text}\"\n",
    "            slide_str += \"\\n\" + (\"-\" * 40) + \"\\n\"\n",
    "    return slide_str\n",
    "\n",
    "def extract_text_from_slide(slide):\n",
    "    \"\"\"Extracts title and content from a slide\"\"\"\n",
    "    title = slide.shapes.title.text if slide.shapes.title else \"No Title\"\n",
    "    content = []\n",
    "\n",
    "    for shape in slide.shapes:\n",
    "        if hasattr(shape, \"text\"):\n",
    "            content.append(shape.text)\n",
    "\n",
    "    return title, content\n",
    "\n",
    "def read_pptx(file_path):\n",
    "    \"\"\"Reads a pptx file and prints the title and content of each slide\"\"\"\n",
    "    prs = Presentation(file_path)\n",
    "\n",
    "    for slide in prs.slides:\n",
    "        title, content = extract_text_from_slide(slide)\n",
    "        print(f\"Title: {title}\")\n",
    "        print(\"Content:\", \"\\n\".join(content))\n",
    "        print(\"-\" * 40)\n",
    "def get_slide_prompt(slide_template, data):\n",
    "    return slide_template.replace(\"$SLIDE$\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'slides': [{'Title': '', 'Description': 'Page 1 Content:', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 2 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 3 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 4 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 5 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 6 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 7 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 8 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 9 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 10 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 11 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 12 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 13 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 14 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 15 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 16 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 17 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 18 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 19 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 20 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 21 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 22 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 23 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 24 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 25 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 26 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 27 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 28 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 29 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 30 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 31 Content:\\n\\n----------------------------------------', 'Latex': []}, {'Title': '', 'Description': '----------------------------------------\\n\\nPage 32 Content:\\n\\n----------------------------------------', 'Latex': []}]}\n"
     ]
    }
   ],
   "source": [
    "### TEST SLIDE OBJ GEN FROM GPT FOR EXPERT\n",
    "slide_str = read_pdf('Research/generation_data/slides/Expert/codingSlides_expert_RAW.pdf')\n",
    "\n",
    "\n",
    "curr_prompt = get_slide_prompt(prompt, slide_str)\n",
    "try:\n",
    "    json_data = request_output_from_llm(prompt=curr_prompt, model=\"gpt-3.5-turbo-16k\")\n",
    "    slide_obj = json.loads(json_data)\n",
    "    print(slide_obj)\n",
    "\n",
    "    # Convert the dictionary to a JSON-formatted string\n",
    "    json_str = json.dumps(slide_obj, indent=4)  # indent for pretty-printing\n",
    "\n",
    "    # Write the JSON string to a file\n",
    "    with open(\"Research/generation_data/slides/Expert/codingSlides_expert.json\", \"w\") as f:\n",
    "        f.write(json_str)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Introduction to Regular Expressions in NLP', 'purpose': 0, 'purpose_statement': 'This slide will serve as an entry point to understanding Regular Expressions (Regex) and how they are used in the context of Natural Language Processing (NLP). It aims to establish foundational knowledge for students with no prior experience, as specified in the Notebank. The focus will be on familiarizing the student with the syntax and basic operators of regex, setting the foundation for understanding its applications in NLP tasks.', 'concepts': ['Basic Syntax and Operators of Regular Expressions', 'Regular Expressions']}\n",
      "\n",
      "{'title': 'Fundamentals of Text Normalization in NLP', 'purpose': 0, 'purpose_statement': 'This slide aims to introduce the concept of Text Normalization within the realm of Natural Language Processing (NLP). It will explain what Text Normalization is, why it is critical for processing natural languages, and how it aids in preparing data for further NLP tasks such as Language Modeling or Sentiment Analysis.', 'concepts': ['Text Normalization', 'Tokenization of Text', 'Sentence Segmentation Techniques', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Getting Started with Edit Distance in Natural Language Processing', 'purpose': 0, 'purpose_statement': 'To provide an introduction to the concept of Edit Distance in Natural Language Processing (NLP), emphasizing its definition and utility within the field.', 'concepts': ['Applications of Edit Distance Algorithm in NLP']}\n",
      "\n",
      "{'title': 'Harnessing the Power of Regular Expressions in NLP', 'purpose': 4, 'purpose_statement': \"This slide aims to provide practical examples and exercises demonstrating the application of Regular Expressions within various NLP tasks, reinforcing theoretical knowledge through hands-on learning experiences to solidify the student's understanding.\", 'concepts': ['Uses of Regular Expressions in NLP']}\n",
      "\n",
      "{'title': 'Understanding Edit Distance and Its Relevance in NLP Applications', 'purpose': 0, 'purpose_statement': 'To introduce the concept of Edit Distance in the context of Natural Language Processing (NLP), laying the groundwork for understanding its relevance to real-world NLP applications such as spell checking, plagiarism detection, and genome sequence analysis.', 'concepts': ['Edit Distance', 'Applications of Edit Distance Algorithm in NLP', 'spell checking', 'plagiarism detection', 'genome sequence analysis']}\n",
      "\n",
      "{'title': 'Decoding Edit Distance Calculations in NLP Contexts', 'purpose': 3, 'purpose_statement': \"To provide an in-depth understanding of the Edit Distance concept and its calculation techniques within the Natural Language Processing (NLP) context, focusing on its applications in text analysis problems encountered in the student's academic curriculum.\", 'concepts': ['Edit Distance', 'Levenshtein algorithm', 'Comparative Genomics']}\n",
      "\n",
      "{'title': 'Expanding the Boundaries with Machine Learning in NLP', 'purpose': 2, 'purpose_statement': \"This slide aims to explore the intersection of Machine Learning (ML) and Natural Language Processing (NLP), establishing a foundational understanding of how ML algorithms can enhance NLP tasks such as language modeling, sentiment analysis, and chatbots. The slide also seeks to arouse curiosity and expand the student's knowledge boundaries by introducing the crucial role of ML in modern NLP applications.\", 'concepts': ['Machine Learning', 'language modeling', 'Sentiment Analysis', 'chatbots']}\n",
      "\n",
      "{'title': 'Unveiling the Practicality of Text Normalization in NLP', 'purpose': 4, 'purpose_statement': 'To provide practical examples that demonstrate the application of previous concepts such as Text Normalization and Tokenization and to introduce new related concepts, namely Sentiment Analysis and Speech Recognition, for a deeper theoretical and practical understanding in the context of academic applications.', 'concepts': ['Text Normalization', 'Tokenization of Text', 'Lemmatization', 'Sentiment Analysis', 'Speech Recognition']}\n",
      "\n",
      "{'title': 'Embarking on the Journey of Machine Translation in NLP', 'purpose': 0, 'purpose_statement': 'This slide will serve as a premier introduction to the concept of Machine Translation within the field of Natural Language Processing, setting the stage for students to understand how machines can translate text or speech from one language to another.', 'concepts': ['Machine Translation', 'Algorithms', 'Natural Language Processing (NLP)', 'language modeling', 'text-to-speech']}\n",
      "\n",
      "{'title': 'Mastering the Fundamentals of DNA Sequencing in NLP', 'purpose': 0, 'purpose_statement': 'To introduce the interdisciplinary concepts of DNA sequencing and Bioinformatics and their relation to Natural Language Processing, setting a foundation for understanding the parallels between biological sequence analysis and text processing.', 'concepts': ['DNA sequencing', 'Bioinformatics', 'genetic mapping', 'Comparative Genomics']}\n",
      "\n",
      "{'title': 'Implementing Tokenization and Lemmatization in NLP Workflows', 'purpose': 2, 'purpose_statement': 'This slide aims to provide students with a practical understanding of how Text Tokenization and Lemmatization are implemented in NLP. The focus is on enhancing the student’s practical skills by allowing them to encode these concepts into a workflow, which is crucial for their academic applications and will assist with the NLP class objectives.', 'concepts': ['Tokenization of Text', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Implementing Tokenization and Lemmatization in NLP Workflows', 'purpose': 2, 'purpose_statement': 'This slide aims to provide students with a practical understanding of how Text Tokenization and Lemmatization are implemented in NLP. The focus is on enhancing the student’s practical skills by allowing them to encode these concepts into a workflow, which is crucial for their academic applications and will assist with the NLP class objectives.', 'concepts': ['Tokenization of Text', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Advancing Natural Language Understanding: Text Analysis Essentials', 'purpose': 2, 'purpose_statement': \"This slide aims to bridge foundational NLP concepts with underexplored yet fundamental areas of Text Analysis. It will introduce Text Analysis and elucidate on its various applications in Natural Language Processing, enriching the student's academic curriculum and facilitating better performance in the NLP class through practical, example-driven learning.\", 'concepts': ['Text Analysis', 'language modeling', 'genome', 'Natural Language Processing (NLP)', 'Regular Expressions', 'Text Normalization', 'Edit Distance']}\n",
      "\n",
      "{'title': 'Unlocking Advanced Text Normalization Techniques in NLP', 'purpose': 4, 'purpose_statement': \"To provide practical application and reinforcement of learned concepts in advanced text normalization techniques, enhancing the student's proficiency in text processing for NLP applications.\", 'concepts': ['Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Unlocking Advanced Text Normalization Techniques in NLP', 'purpose': 4, 'purpose_statement': \"To provide practical application and reinforcement of learned concepts in advanced text normalization techniques, enhancing the student's proficiency in text processing for NLP applications.\", 'concepts': ['Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Delving Deeper: Advanced Applications of Edit Distance in NLP', 'purpose': 2, 'purpose_statement': \"This slide aims to build upon the foundational understanding of the Edit Distance concept by exploring its advanced applications in the fields of NLP, specifically focusing on plagiarism detection, genome sequence analysis, and comparative genomics, which have not been fully examined in previous sessions. The goal is to leverage this deeper dive to enhance the student's comprehension for their NLP class and to nourish their interest in wide-ranging practical applications.\", 'concepts': ['Edit Distance', 'plagiarism detection', 'Comparative Genomics', 'genome sequence analysis']}\n",
      "\n",
      "{'title': 'Exploring the Nuances of Sentence Segmentation in Text Analysis', 'purpose': 2, 'purpose_statement': \"This slide will delve into the intricacies of segmenting text into sentences, an important pre-processing step in NLP, and how it paves the way for more advanced text analysis. As we have already touched upon Text Normalization and Tokenization, understanding Sentence Segmentation will enrich the student's comprehension of how texts are prepared for NLP tasks.\", 'concepts': ['Sentence Segmentation Techniques', 'Text Analysis', 'Natural Language Processing (NLP)', 'Regular Expressions']}\n",
      "\n",
      "{'title': 'Exploring the Nuances of Sentence Segmentation in Text Analysis', 'purpose': 2, 'purpose_statement': \"This slide will delve into the intricacies of segmenting text into sentences, an important pre-processing step in NLP, and how it paves the way for more advanced text analysis. As we have already touched upon Text Normalization and Tokenization, understanding Sentence Segmentation will enrich the student's comprehension of how texts are prepared for NLP tasks.\", 'concepts': ['Sentence Segmentation Techniques', 'Text Analysis', 'Natural Language Processing (NLP)', 'Regular Expressions']}\n",
      "{'title': 'Introduction to Regular Expressions in NLP', 'presentation': \"Good day! Welcome to the introduction of Regular Expressions in Natural Language Processing. In this slide, we're going to explore the fundamentals of Regular Expressions, also known as regex, and their significance in NLP tasks.\\n\\nSo, what exactly are Regular Expressions? They are sequences of characters that define search patterns, primarily used for string matching. They play a crucial role in text processing, enabling us to specify complex search patterns with various syntax and operators.\\n\\nTo begin, let's delve into the basic syntax and operators of Regular Expressions. We'll discuss literals, wildcards, character classes, quantifiers, anchors, and grouping constructs, each serving a specific purpose in pattern matching.\\n\\nUnderstanding the syntax and operators of regex is crucial because it forms the foundation for various Text Normalization techniques used in NLP, such as Tokenization of Text and Sentence Segmentation Techniques. These techniques help structure raw text for further processing. I will show you examples and practical implementations to demystify regex implementation for you.\\n\\nLastly, we'll connect the concepts we've covered to real-world NLP applications like chatbots, emphasizing the practical relevance of Regular Expressions. We'll also hint at upcoming topics like advanced regex applications, keeping you engaged and curious about the possibilities.\\n\\nBy the end of this slide, you'll have a solid understanding of regex and how it relates to NLP. Let's get started!\", 'content': 'Slide Content:\\n\\n- Welcome to the slide on Introduction to Regular Expressions in NLP!\\n- Regular Expressions (regex) are powerful tools used in Natural Language Processing (NLP) to define search patterns.\\n- They are sequences of characters that allow for the matching of strings, making it easier to perform tasks like text searching, text substitution, and data validation.\\n- In this slide, we will introduce the basic syntax and operators of regex, including literals, wildcards, character classes, quantifiers, anchors, and grouping constructs.\\n- Understanding regex is essential for various NLP techniques, such as Tokenization of Text and Sentence Segmentation.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'This slide will serve as an entry point to understanding Regular Expressions (Regex) and how they are used in the context of Natural Language Processing (NLP). It aims to establish foundational knowledge for students with no prior experience, as specified in the Notebank. The focus will be on familiarizing the student with the syntax and basic operators of regex, setting the foundation for understanding its applications in NLP tasks.', 'concepts': ['Basic Syntax and Operators of Regular Expressions', 'Regular Expressions']}\n",
      "\n",
      "{'title': 'Fundamentals of Text Normalization in NLP', 'presentation': \"Welcome to the fascinating world of Natural Language Processing, or NLP for short. Today, we step into the foundational concept of Text Normalization. Now, imagine trying to make sense of a book where there are no spaces between words, or where the rules of punctuation are... well, apparently optional. That's pretty much the challenge in processing natural language data! But fear not, because Text Normalization is our savior in bringing order to this chaos. It prepares the battleground for the machines to dive into the vast ocean of human language. So let's begin our journey into Tokenization, saunter through Lemmatization, and puzzle over Sentence Segmentation. By the end of this slide, you'll see why these processes are not just steps but leaps towards understanding and conquering the complexities of language in the digital age.\", 'content': 'Introduction to Text Normalization in NLP:\\n- Text Normalization is the process of converting text into a more uniform format in NLP.\\n- It includes tasks like Tokenization of Text, Lemmatization, and Sentence Segmentation Techniques.\\n- Tokenization breaks down a sequence of characters into tokens, such as words or phrases.\\n- Lemmatization reduces words to their base or root form, considering their context and part-of-speech.\\n- Sentence Segmentation Techniques divide a text into its constituent sentences, accurately identifying sentence boundaries.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'This slide aims to introduce the concept of Text Normalization within the realm of Natural Language Processing (NLP). It will explain what Text Normalization is, why it is critical for processing natural languages, and how it aids in preparing data for further NLP tasks such as Language Modeling or Sentiment Analysis.', 'concepts': ['Text Normalization', 'Tokenization of Text', 'Sentence Segmentation Techniques', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Getting Started with Edit Distance in Natural Language Processing', 'presentation': 'Welcome to the world of Natural Language Processing. In this lesson, we will explore the concept of Edit Distance and its importance in NLP. Edit Distance is an algorithm that measures the minimum number of operations needed to transform one string into another. It is used in various NLP tasks, such as spell checking and plagiarism detection. By calculating the Edit Distance, we can determine the similarity between two strings. For example, it can help us identify if a word is misspelled or if a text has been copied from another source. Edit Distance is not only limited to NLP, but also finds applications in Bioinformatics, where it is used to analyze DNA sequences. This algorithm is a versatile tool that plays a crucial role in understanding and processing human language. As we progress through this course, you will see how Edit Distance contributes to many advanced NLP processes. Keep this concept in mind as we dive deeper into the exciting field of Natural Language Processing.', 'content': '1. Edit Distance: The Edit Distance algorithm measures the minimum number of operations required to transform one string into another. It is a crucial tool in Natural Language Processing (NLP) for tasks such as spell checking, plagiarism detection, and DNA sequence analysis in Bioinformatics.\\n\\n2. Importance in NLP: Edit Distance is vital in NLP as it allows us to compare and correct strings, enabling accurate text analysis and processing. It helps in tasks like identifying spelling errors, identifying similarities between documents, and aligning DNA sequences.\\n\\n3. Applications in NLP: Edit Distance is widely used in NLP applications, including spell checking to suggest alternative words, plagiarism detection to compare and identify similarities between texts, and DNA sequence alignment for analyzing genetic data.\\n\\n4. Versatility in Bioinformatics: Edit Distance also plays a significant role in Bioinformatics, where it aids in comparing DNA sequences and analyzing genetic information to understand evolutionary relationships and genetic variations.\\n\\nRemember, Edit Distance is a powerful concept that underlies many essential NLP algorithms and techniques, and it has diverse applications in various domains.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'To provide an introduction to the concept of Edit Distance in Natural Language Processing (NLP), emphasizing its definition and utility within the field.', 'concepts': ['Applications of Edit Distance Algorithm in NLP']}\n",
      "\n",
      "{'title': 'Harnessing the Power of Regular Expressions in NLP', 'presentation': \"Hello! Today, we're diving into the fascinating world of regular expressions in natural language processing (NLP). Regular expressions, also known as regex, are an essential tool for manipulating and processing text data with precision. They are widely used in various NLP tasks, such as text analysis, sentiment analysis, and text normalization.\\n\\nRegular expressions enable us to search for and match specific patterns within text, extract relevant information, and perform data cleaning. In NLP, they play a crucial role in tasks like tokenization, sentence segmentation, language modeling, and machine translation.\\n\\nTo help solidify your understanding of regex, we'll explore practical examples and exercises. We'll start by examining how regex can be used to identify and extract email addresses from a body of text. We'll then move on to text normalization, where regex aids in preparing text for further analysis or machine learning tasks.\\n\\nNext, we'll delve into tokenization, a crucial step in NLP that involves breaking text into individual words or tokens. We'll guide you through constructing regex patterns to split text effectively, enabling tasks like word frequency analysis or language modeling.\\n\\nWe'll also explore sentence segmentation techniques and demonstrate how regex can be used to split text into sentences, which is essential for applications like machine translation and sentiment analysis.\\n\\nFor those interested in chatbots, we'll engage in an interactive exercise that simulates how regex can extract user commands or queries from text, mirroring how chatbots interpret input.\\n\\nLastly, we'll present practical examples of regex application in modern NLP tasks, such as cleaning datasets, preparing text for speech recognition algorithms, and creating language models.\\n\\nBy the end of this slide, you'll have a solid understanding of regex and its immense potential in NLP. You'll also have gained hands-on experience through practical exercises. So, let's dive in and harness the power of regular expressions in NLP!\", 'content': '1. Introduction to Regular Expressions and their importance in NLP\\n2. Practical examples of regex for pattern matching tasks\\n3. Using regex for text normalization and cleaning\\n4. Tokenization of text using regex\\n5. Sentence segmentation techniques using regex', 'latex_codes': '', 'purpose': 4, 'purpose_statement': \"This slide aims to provide practical examples and exercises demonstrating the application of Regular Expressions within various NLP tasks, reinforcing theoretical knowledge through hands-on learning experiences to solidify the student's understanding.\", 'concepts': ['Uses of Regular Expressions in NLP']}\n",
      "\n",
      "{'title': 'Understanding Edit Distance and Its Relevance in NLP Applications', 'presentation': \"Welcome to the world of Natural Language Processing! Today, we'll be exploring the concept of Edit Distance and its relevance in various NLP applications. Edit Distance is a measure of similarity between two strings, quantifying the minimum number of operations required to transform one string into the other. Through the Levenshtein algorithm, we can computationally determine the Edit Distance between strings.\\n\\nNow, let's delve into the exciting applications of Edit Distance in NLP. First, we have spell checking. In this context, Edit Distance helps identify and correct spelling errors by comparing words against a dictionary of correct spellings. Next, we have plagiarism detection. Edit Distance enables us to measure the similarity between documents, detecting cases where text has been copied without proper attribution.\\n\\nLastly, Edit Distance plays a crucial role in genome sequence analysis. By analyzing the DNA sequence, scientists can understand the structure, function, and evolution of genomes. Edit Distance computations help identify mutations that may lead to diseases. It's truly fascinating how Edit Distance can be applied across different domains!\\n\\nTo better grasp the concept, let's consider the practical example of spell checking in text editors. As you're typing, the spell checker analyzes your text and suggests corrections based on the Edit Distance between the misspelled word and a dictionary of correct spellings.\\n\\nIn summary, understanding Edit Distance is vital in NLP. It serves as a foundation for various applications, such as spell checking, plagiarism detection, and genome sequence analysis. Now that we've introduced the concept, you're one step closer to becoming an NLP expert. Let's continue our exciting journey together!\", 'content': '1. Edit Distance: Edit Distance is a measure of how dissimilar two strings are by counting the minimum number of operations required for their transformation, such as insertions, deletions, or substitutions.\\n\\n2. Computational understanding through the Levenshtein algorithm: The Levenshtein algorithm is used to efficiently calculate the Edit Distance between two strings.\\n\\n3. Practical applications in various NLP domains: Edit Distance is relevant in NLP applications such as spell checking, plagiarism detection, and genome sequence analysis.\\n\\n4. Overview of how Edit Distance is computed: Edit Distance is computed by considering the minimum number of operations required to transform one string into another.\\n\\n5. Real-world example: Spell checking in text editors: The Edit Distance algorithm is used to determine possible corrections for spelling errors in text editors.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'To introduce the concept of Edit Distance in the context of Natural Language Processing (NLP), laying the groundwork for understanding its relevance to real-world NLP applications such as spell checking, plagiarism detection, and genome sequence analysis.', 'concepts': ['Edit Distance', 'Applications of Edit Distance Algorithm in NLP', 'spell checking', 'plagiarism detection', 'genome sequence analysis']}\n",
      "\n",
      "{'title': 'Decoding Edit Distance Calculations in NLP Contexts', 'presentation': \"Good day! Today, we will be diving into the fascinating world of Natural Language Processing (NLP) to understand the concept of Edit Distance and its calculation techniques in NLP contexts. Edit Distance is a way of measuring the dissimilarity between two strings by counting the minimum number of operations needed to transform one string into the other.\\n\\nLet's start by exploring the importance of Edit Distance in various NLP tasks such as spell checking, plagiarism detection, and even genome sequence analysis in bioinformatics.\\n\\nNow, to calculate Edit Distance efficiently, we will be utilizing the Levenshtein algorithm. This algorithm determines the minimum number of single-character edits required to change one word into another. We will walk through the steps of the Levenshtein algorithm, which involves insertions, deletions, and substitutions. Remember, each of these edit operations contributes to the total Edit Distance calculation.\\n\\nBeyond NLP, Edit Distance plays a significant role in comparative genomics, allowing us to compare genomic features across different organisms. We will delve deeper into this intersection of bioinformatics and NLP.\\n\\nTo solidify your understanding, we will engage in a practical exercise where you will compute Edit Distance for genetic data. This hands-on approach will help reinforce the concepts we've covered.\\n\\nBy the end of this session, you will not only have a comprehensive understanding of Edit Distance in NLP, but you will also be able to apply this knowledge to practical scenarios. So let's get started on this exciting journey!\", 'content': 'Content:\\n- Edit Distance: a measure of dissimilarity between two strings\\n- The Levenshtein Algorithm: a computational method for calculating Edit Distance\\n- Calculation of Edit Distance: involves insertions, deletions, and substitutions\\n- Applications of Edit Distance: spell checking, plagiarism detection, text similarity\\n- Comparative Genomics: the use of Edit Distance in analyzing genome sequences', 'latex_codes': '', 'purpose': 3, 'purpose_statement': \"To provide an in-depth understanding of the Edit Distance concept and its calculation techniques within the Natural Language Processing (NLP) context, focusing on its applications in text analysis problems encountered in the student's academic curriculum.\", 'concepts': ['Edit Distance', 'Levenshtein algorithm', 'Comparative Genomics']}\n",
      "\n",
      "{'title': 'Expanding the Boundaries with Machine Learning in NLP', 'presentation': \"Welcome to the slide titled 'Expanding the Boundaries with Machine Learning in NLP'. In this slide, we will explore the intersection of Machine Learning (ML) and Natural Language Processing (NLP) and how ML algorithms can enhance NLP tasks. Let's begin by understanding what Machine Learning is. Machine Learning is a subset of Artificial Intelligence that allows systems to learn and improve from experience without being explicitly programmed. It involves the development of algorithms that can make predictions or decisions based on data. Now, let's connect ML to NLP. NLP deals with interactions between computers and human languages. By applying ML techniques to NLP, we can significantly improve the performance of NLP tasks. One important NLP task is language modeling, which involves predicting the likelihood of a sequence of words in a natural language. ML algorithms can greatly enhance language modeling by developing probabilistic models that generate or determine the probability distribution of words or sentences. This is crucial for various NLP applications such as machine translation and text generation. Another interesting aspect we'll explore is sentiment analysis. Sentiment analysis is a subfield of NLP that involves extracting subjective information, such as opinions and emotions, from textual data. ML algorithms can be used to classify texts into categories like positive, negative, or neutral sentiment. This is useful in applications like business analytics and social media monitoring. Finally, we'll discuss chatbots. Chatbots are automated dialogue systems that interact with humans using NLP techniques. ML plays a crucial role in improving the functionality of chatbots. We'll examine how ML enhances chatbot capabilities, such as pattern matching, text normalization, and understanding user input. By the end of this slide, you will have a solid understanding of how ML enhances NLP tasks and how it can be applied to modern applications like language modeling, sentiment analysis, and chatbots. Let's dive in and explore this exciting intersection of ML and NLP!\", 'content': 'Title: Expanding the Boundaries with Machine Learning in NLP\\n\\n- Introduction to Machine Learning in Natural Language Processing (NLP)\\n- Language Modeling: Predicting word sequence likelihoods with Machine Learning\\n- Sentiment Analysis: Using Machine Learning algorithms to categorize text into sentiments\\n- Chatbots: Enhancing dialogue systems with natural language processing and Machine Learning techniques\\n- Intersection of Machine Learning and NLP in modern applications: Language modeling, sentiment analysis, and chatbots', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide aims to explore the intersection of Machine Learning (ML) and Natural Language Processing (NLP), establishing a foundational understanding of how ML algorithms can enhance NLP tasks such as language modeling, sentiment analysis, and chatbots. The slide also seeks to arouse curiosity and expand the student's knowledge boundaries by introducing the crucial role of ML in modern NLP applications.\", 'concepts': ['Machine Learning', 'language modeling', 'Sentiment Analysis', 'chatbots']}\n",
      "\n",
      "{'title': 'Unveiling the Practicality of Text Normalization in NLP', 'presentation': \"Welcome, everyone. Today, we are going to explore the practical applications of Text Normalization in Natural Language Processing (NLP). Text Normalization is the process of converting text into a more uniform format, and it plays a crucial role in various NLP tasks. Let's dive into the details.\\n\\nFirst, we will discuss the initial step of Tokenization. Tokenization involves breaking down a sequence of characters into individual tokens, such as words or phrases. This process is essential for further analysis and understanding of text.\\n\\nNext, we will explore Lemmatization, which reduces words to their base or root form, called a lemma. Unlike stemming, which chops off word endings crudely, Lemmatization considers the vocabulary and morphological analysis of words, making it more accurate and sophisticated. We will see how it helps in improving the efficiency and accuracy of NLP tasks.\\n\\nMoving forward, we will discuss the concept of Sentiment Analysis. This subfield of NLP involves extracting subjective information, such as opinions and emotions, from text. We will explore how Text Normalization supports Sentiment Analysis to classify texts into positive, negative, or neutral categories. This technique is widely used in various domains like business analytics, social media monitoring, and customer feedback analysis.\\n\\nFinally, we will dive into Speech Recognition, which allows computers to interpret and understand human speech as input. By applying Text Normalization techniques, we can convert spoken language to text more accurately. This technology is used in applications like virtual assistants and voice-controlled systems.\\n\\nThroughout this presentation, we will provide practical examples and discuss real-world contexts to help you understand the importance and application of Text Normalization. So let's get started and explore the practicality of Text Normalization in NLP.\", 'content': '1. Text Normalization: A process in NLP that converts text into a more uniform format, including tokenization, lemmatization, and sentence segmentation techniques.\\n2. Tokenization of Text: Converting a sequence of characters into tokens, such as words or phrases, for further processing in NLP tasks like parsing and part-of-speech tagging.\\n3. Lemmatization: Reducing words to their base or root form, considering vocabulary and morphological analysis for accurate processing in NLP tasks.\\n4. Sentiment Analysis: Extracting subjective information like opinions and attitudes from textual data, often used in business analytics and social media monitoring.\\n5. Speech Recognition: Technology that converts spoken language into text, utilizing language modeling and sentiment analysis for accurate processing.', 'latex_codes': '', 'purpose': 4, 'purpose_statement': 'To provide practical examples that demonstrate the application of previous concepts such as Text Normalization and Tokenization and to introduce new related concepts, namely Sentiment Analysis and Speech Recognition, for a deeper theoretical and practical understanding in the context of academic applications.', 'concepts': ['Text Normalization', 'Tokenization of Text', 'Lemmatization', 'Sentiment Analysis', 'Speech Recognition']}\n",
      "\n",
      "{'title': 'Embarking on the Journey of Machine Translation in NLP', 'presentation': \"Good day everyone! Today, we are embarking on an exciting journey into the world of Machine Translation within Natural Language Processing (NLP). Machine Translation is the amazing ability of computers to translate text or speech from one language to another. What exactly is NLP? It is a field of computer science and artificial intelligence that focuses on the interaction between computers and human language. NLP allows computers to understand, interpret, and generate human languages in a meaningful and useful way. We'll start our exploration by understanding the basics of Machine Translation and its relationship with NLP. We will also touch upon the evolution of Machine Translation and the different approaches used, such as rule-based, statistical, and neural network methods. Additionally, we will briefly discuss the importance of algorithms and language modeling in Machine Translation. Algorithms provide step-by-step instructions for solving specific problems, while language modeling helps predict the likelihood of word sequences in a given language. Throughout this presentation, I will provide you with real-world examples to help you better grasp these concepts, such as translating real-time subtitles or using chatbots in multiple languages. By the end of this presentation, you will have a solid understanding of the foundations of Machine Translation and its significance in our increasingly interconnected world. So, let's dive in and begin our fascinating journey into Machine Translation within NLP!\", 'content': '1. Introduction to Machine Translation: Machine Translation (MT) is the process of automatically translating text or speech from one language to another. It leverages complex algorithms and language modeling techniques to achieve accurate and fluent translations.\\n\\n2. Overview of Natural Language Processing (NLP): Natural Language Processing is a field of computer science and artificial intelligence that focuses on the interaction between computers and human language. NLP encompasses various techniques and tools that enable computers to understand, interpret, and generate human languages.\\n\\n3. The Evolution of Machine Translation: Machine Translation has evolved from rule-based approaches to statistical and neural network methods. Each approach has its own strengths and applications, and has contributed to advancements in translation accuracy and fluency.\\n\\n4. Fundamental Approaches to MT: Rule-based, statistical, and neural network approaches are the three main ways to approach Machine Translation. Each approach has different advantages and applications, and has shaped the development of the field.\\n\\n5. Basic Concepts: Algorithms and Language Modeling are fundamental concepts in Machine Translation. Algorithms provide a step-by-step process for solving problems, while language modeling predicts the likelihood of word sequences in a given language. Both concepts are essential for building effective Machine Translation systems.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'This slide will serve as a premier introduction to the concept of Machine Translation within the field of Natural Language Processing, setting the stage for students to understand how machines can translate text or speech from one language to another.', 'concepts': ['Machine Translation', 'Algorithms', 'Natural Language Processing (NLP)', 'language modeling', 'text-to-speech']}\n",
      "\n",
      "{'title': 'Mastering the Fundamentals of DNA Sequencing in NLP', 'presentation': \"Welcome to today's lesson on mastering the fundamentals of DNA sequencing in NLP. In this lesson, we will explore the intriguing parallels between DNA sequencing and natural language processing. We will start by understanding the process of DNA sequencing, which involves determining the precise order of nucleotides in a DNA molecule. Think of it as decoding the language of life! Next, we will dive into the field of bioinformatics, an interdisciplinary field that combines computer science, biology, and more to analyze biological data. We will see how bioinformatics uses algorithms similar to those in NLP for genome sequence analysis. Genetic mapping will also be discussed, which involves determining the location of specific genes on a DNA strand. This process is similar to finding keywords in a text corpus in NLP. Finally, we will explore the concept of comparative genomics, which compares genomic features of different organisms. This is akin to comparing texts or language data in NLP. As we go through each concept, you will see visual and conceptual analogies between DNA sequencing and NLP, helping you understand the similarities between the two fields. Get ready to unravel the mysteries of DNA sequencing and its connection to natural language processing!\", 'content': '1. DNA sequencing is the process of determining the precise order of nucleotides within a DNA molecule. It has significant applications in genomics, genetic testing, biomedical research, and forensic biology.\\n2. Bioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data. It combines computer science, biology, chemistry, statistics, and mathematics to analyze and interpret biological data.\\n3. Genetic mapping involves determining the location and sequence of specific genes on a DNA strand. It aids in tasks like genome editing, plagiarism detection, and understanding the genetic basis of diseases.\\n4. Comparative genomics compares genomic features of different organisms, helping us understand evolutionary processes and the structure and function of genomes.\\n5. DNA sequencing, bioinformatics, genetic mapping, and comparative genomics are interconnected fields that provide insights into the connections between biological sequence analysis and text processing in natural language.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'To introduce the interdisciplinary concepts of DNA sequencing and Bioinformatics and their relation to Natural Language Processing, setting a foundation for understanding the parallels between biological sequence analysis and text processing.', 'concepts': ['DNA sequencing', 'Bioinformatics', 'genetic mapping', 'Comparative Genomics']}\n",
      "\n",
      "{'title': 'Implementing Tokenization and Lemmatization in NLP Workflows', 'presentation': \"Good day! Today, we will be diving into the implementation of Tokenization and Lemmatization in NLP workflows. Starting with Tokenization, we'll explore how this process breaks down a sequence of characters into smaller units, or tokens, like words or phrases. Tokenization is a crucial step before further processing, such as parsing and text analysis. It helps reduce different forms of words to a common base form, enhancing text normalization.\\n\\nMoving on to Lemmatization, we'll understand how it goes beyond basic stemming. Lemmatization considers vocabulary and morphology to reduce words to their base or root form. This process ensures accuracy by analyzing the context in which words appear.\\n\\nTo visualize these concepts, we'll include flowcharts and examples of tokenized and lemmatized text. These visuals will help you grasp the transformations that occur during preprocessing.\\n\\nLastly, we'll apply what we've learned by showcasing practical implementations of tokenization and lemmatization in NLP workflows. We'll reference popular applications such as text-to-speech, sentiment analysis, and machine translation. Additionally, we'll provide Python code snippets using libraries like NLTK or spaCy, enabling you to integrate these techniques into your own NLP pipeline.\\n\\nBy the end of this presentation, you'll have a solid understanding of tokenization and lemmatization in NLP workflows. You'll recognize their importance and be able to apply them in academic and real-world scenarios such as your NLP class. Now, let's dive in and explore the exciting world of tokenization and lemmatization in NLP!\", 'content': '1. Text Tokenization: The process of converting a sequence of characters into a sequence of tokens. Tokens can be words, phrases, symbols, or lexemes within the text. Tokenization is a fundamental step in NLP preprocessing, enabling further analysis.\\n2. Lemmatization: The process of reducing a word to its base or root form, called a lemma. Unlike stemming, lemmatization considers vocabulary and morphology, resulting in more accurate word reduction. It is crucial for text normalization and various NLP tasks.\\n3. Tokenization and Lemmatization in NLP Workflow: Practical implementation of these concepts in an NLP pipeline. Examples of text-to-speech, sentiment analysis, and machine translation showcase the application of tokenization and lemmatization techniques in real-world scenarios.\\n4. Flowchart: A graphical representation of the NLP preprocessing workflow, highlighting the steps of tokenization and lemmatization.\\n5. Python Code Snippets: Examples of Python code using NLTK or spaCy libraries to demonstrate the integration of tokenization and lemmatization into an NLP pipeline.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': 'This slide aims to provide students with a practical understanding of how Text Tokenization and Lemmatization are implemented in NLP. The focus is on enhancing the student’s practical skills by allowing them to encode these concepts into a workflow, which is crucial for their academic applications and will assist with the NLP class objectives.', 'concepts': ['Tokenization of Text', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Implementing Tokenization and Lemmatization in NLP Workflows', 'presentation': \"Good morning everyone! Today, we will be diving into the exciting world of Natural Language Processing and exploring two crucial tasks - Tokenization of Text and Lemmatization. The goal of this presentation is to provide you with a practical understanding of how these tasks are implemented in NLP workflows.\\n\\nLet's start with Tokenization. It is the process of breaking down a sequence of characters into smaller units called tokens. Tokens can be words, phrases, symbols, or meaningful elements within the text. Tokenization is a fundamental step in NLP as it allows for further analysis like parsing and sentiment analysis. It also helps in reducing inflectional and derivationally related forms of a word to a common base form.\\n\\nNow, let's move on to Lemmatization. Unlike stemming, which crudely chops off word endings, Lemmatization considers the vocabulary and morphological analysis of words. It reduces words to their base or root form, called a lemma. Lemmatization plays a crucial role in various NLP tasks like text-to-speech, machine translation, and sentiment analysis.\\n\\nTo provide you with a hands-on experience, we will showcase practical examples using popular NLP libraries such as NLTK or SpaCy. You will see how Tokenization and Lemmatization can be implemented in real NLP tasks.\\n\\nBy the end of this presentation, you will have a solid understanding of Tokenization and Lemmatization and be able to incorporate these concepts into your NLP workflows. So let's get started!\", 'content': \"1. Tokenization of Text: \\n\\n- Tokenization is the process of converting a sequence of characters into a sequence of tokens, such as words, phrases or symbols. \\n\\n- It is a fundamental step in NLP and helps in reducing words to their base form for further processing.\\n\\n2. Lemmatization: \\n\\n- Lemmatization involves reducing a word to its base or root form, called a lemma.\\n\\n- Unlike stemming, it considers the vocabulary and morphological analysis of words, resulting in more accurate outcomes.\\n\\n3. Practical Examples: \\n\\n- We will demonstrate code examples showcasing how Tokenization and Lemmatization are implemented in NLP workflows using popular libraries like NLTK or SpaCy.\\n\\n4. Tokenization and Lemmatization in NLP Workflow: \\n\\n- Both Tokenization and Lemmatization play crucial roles in NLP workflows, serving as foundational steps for various tasks like parsing, sentiment analysis, and text normalization.\\n\\n5. Importance of Tokenization and Lemmatization: \\n\\n- Tokenization and Lemmatization enhance practical skills and enable efficient NLP application in academic settings, aligning with the student's objectives and course requirements.\", 'latex_codes': '', 'purpose': 2, 'purpose_statement': 'This slide aims to provide students with a practical understanding of how Text Tokenization and Lemmatization are implemented in NLP. The focus is on enhancing the student’s practical skills by allowing them to encode these concepts into a workflow, which is crucial for their academic applications and will assist with the NLP class objectives.', 'concepts': ['Tokenization of Text', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Advancing Natural Language Understanding: Text Analysis Essentials', 'presentation': \"Good afternoon, students! Today, we embark on an exploratory journey into the intriguing domain of Text Analysis within Natural Language Processing (NLP). Our objective is to delve into the fundamental concepts that underpin NLP and their interplay with Text Analysis. So, let's get started!\\n\\nOn this slide, we will introduce the concept of Text Analysis, which involves extracting meaningful information from unstructured text data. Imagine having the power to structure and understand vast volumes of textual content! This is precisely what Text Analysis enables us to do by utilizing techniques such as Regular Expressions, Text Normalization, and Edit Distance.\\n\\nOur first stop on this journey is Regular Expressions. These powerful tools help us identify and extract specific patterns from text using syntax and operators. For example, we can use Regular Expressions to search for specific words or phrases, validate data, and perform other text processing tasks.\\n\\nMoving along, we encounter Text Normalization. This process involves transforming raw text into a more uniform format, making it easier to analyze. Techniques like tokenization, lemmatization, and sentence segmentation play a crucial role in standardizing text, reducing complexity, and preparing it for tasks like sentiment analysis or machine translation.\\n\\nNext, let's explore Edit Distance. It quantifies the differences between two text strings. By counting the minimum number of operations required to transform one string into another, we can measure how similar or dissimilar they are. Edit Distance has useful applications in spell checking and even genomic sequence analysis.\\n\\nAs we progress through this slide, we will present practical examples and hands-on exercises to help you grasp these concepts better. You'll gain a solid theoretical foundation while also understanding how they apply to real-world NLP applications, such as chatbots.\\n\\nUltimately, our aim is to equip you with a profound understanding of Text Analysis and its significance in the field of NLP. By the end of this exciting journey, you'll have the knowledge and tools to succeed academically and apply your skills practically. Enjoy the rest of the lesson, and keep exploring the fascinating world of Natural Language Processing!\", 'content': 'In this slide, we will explore the essential components of Text Analysis within Natural Language Processing (NLP). We will start with an introduction to Text Analysis and its applications in NLP. Then, we will delve into Regular Expressions, covering the basic syntax and operators that allow for efficient pattern matching in text data. Next, we will explore Text Normalization techniques, including tokenization, lemmatization, and sentence segmentation. Finally, we will discuss the concept of Edit Distance and its applications in spell checking and genome sequence analysis.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide aims to bridge foundational NLP concepts with underexplored yet fundamental areas of Text Analysis. It will introduce Text Analysis and elucidate on its various applications in Natural Language Processing, enriching the student's academic curriculum and facilitating better performance in the NLP class through practical, example-driven learning.\", 'concepts': ['Text Analysis', 'language modeling', 'genome', 'Natural Language Processing (NLP)', 'Regular Expressions', 'Text Normalization', 'Edit Distance']}\n",
      "\n",
      "{'title': 'Unlocking Advanced Text Normalization Techniques in NLP', 'presentation': \"Welcome to the slide on 'Unlocking Advanced Text Normalization Techniques in NLP.' Today, we will explore practical applications of advanced text normalization techniques and their importance in natural language processing. Let's dive right in.\\n\\nFirst, let's understand the significance of advanced text normalization techniques in NLP. These techniques play a crucial role in refining NLP tasks, such as language modeling, sentiment analysis, and machine translation.\\n\\nTo demonstrate the practical side of these techniques, we'll start with an exercise on advanced regular expressions. We'll explore how regular expressions can be used to parse and normalize complex textual data. This exercise will solidify your understanding of regular expression syntax and operators by working with real-world examples.\\n\\nNext, we'll delve into tokenization, a technique that segments text into semantic units. This process enhances language modeling accuracy and helps in tasks like part-of-speech tagging. You'll have the opportunity to practice tokenization by segmenting a block of text.\\n\\nMoving on, we'll discuss lemmatization, a process used to reduce words to their base or canonical form. We'll explore its importance in text normalization and provide coding examples to illustrate its practical implementation.\\n\\nWe'll then explore sentence segmentation techniques, which involve dividing text into individual sentences. This technique is essential for various NLP tasks, such as text summarization and sentiment analysis. You'll gain hands-on experience by performing sentence segmentation exercises.\\n\\nLastly, we'll introduce the Edit Distance algorithm, which measures the similarity between two string sequences. We'll use coding exercises to calculate edit distances between different strings, enabling you to grasp the concept and its applications in NLP.\\n\\nTo conclude, we'll bring everything together by presenting a practical example that integrates the advanced text normalization techniques we've discussed. This example will showcase their relevance in modern NLP applications, specifically in the development of chatbots. The aim is to emphasize how these techniques can transform conversational AI systems in real-world scenarios.\\n\\nThroughout this presentation, we'll provide exercises and examples to reinforce your understanding and ensure your mastery of advanced text normalization techniques. By the end, you'll have the knowledge and skills needed to excel in your NLP course and apply these techniques effectively.\\n\\nLet's embark on this exciting journey into advanced text normalization techniques!\", 'content': '1. Introduction to advanced text normalization techniques in NLP\\n2. Practical applications and real-world examples of text normalization\\n3. Hands-on exercise: Advanced regular expressions for text parsing and normalization\\n4. Exploring tokenization strategies for semantic unit segmentation\\n5. Lemmatization and its importance in reducing words to their canonical forms\\n6. Sentence segmentation techniques for text partitioning\\n7. Understanding the Edit Distance algorithm for measuring string similarity\\n8. Calculation of edit distances between different strings\\n9. Application of advanced text normalization techniques in a chatbot scenario', 'latex_codes': '', 'purpose': 4, 'purpose_statement': \"To provide practical application and reinforcement of learned concepts in advanced text normalization techniques, enhancing the student's proficiency in text processing for NLP applications.\", 'concepts': ['Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Unlocking Advanced Text Normalization Techniques in NLP', 'presentation': \"Good day! Today, we will explore the practical applications of advanced text normalization techniques in Natural Language Processing (NLP). Text normalization is an important aspect of NLP that allows us to clean and prepare text for analysis. Let's dive in!\\n\\nFirst, let's recap the importance of Regular Expressions, Text Normalization, and Edit Distance in NLP. We will then move on to discussing how Regular Expressions are used to identify patterns in text. I will provide coding examples to demonstrate the syntax and operators used for these tasks.\\n\\nNext, we will explore Text Normalization, which involves processes such as tokenization, lemmatization, and sentence segmentation. These techniques help us optimize the processing of language data. We will look at real-life examples to understand how these processes work.\\n\\nAfter that, we will delve into the concept of Edit Distance. We will discuss its algorithmic foundations and how it is used in applications like spell-checkers and plagiarism detection. To reinforce your understanding, I will guide you through a hands-on coding exercise to compute Edit Distance.\\n\\nLastly, I will present questions to encourage critical thinking and reinforce your learning. Feel free to ask any questions and make connections to your personal experiences with technology. By the end of this presentation, you will have a solid understanding of these text normalization techniques and be prepared for your NLP class assessments. Let's get started!\", 'content': '1. Introduction to Regular Expressions: Learn the importance of using Regular Expressions in NLP and how they can be applied to identify patterns in text.\\n2. Text Normalization Techniques: Explore tokenization, lemmatization, and sentence segmentation as fundamental components of text normalization in NLP.\\n3. Edit Distance Algorithm: Understand the algorithmic foundation of Edit Distance and its application in tasks such as spell-checking and plagiarism detection.\\n4. Practical Examples and Exercises: Engage in interactive coding exercises and examples to strengthen your understanding of advanced text normalization techniques.\\n5. Bridging Theory to Practice: Reflect on how these techniques are used in real-world NLP applications and how they can enhance your NLP class performance.', 'latex_codes': '', 'purpose': 4, 'purpose_statement': \"To provide practical application and reinforcement of learned concepts in advanced text normalization techniques, enhancing the student's proficiency in text processing for NLP applications.\", 'concepts': ['Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Delving Deeper: Advanced Applications of Edit Distance in NLP', 'presentation': \"Welcome to our slide on the advanced applications of Edit Distance in NLP. Edit Distance is a powerful concept in Natural Language Processing (NLP) that measures the similarity between strings. Today, we will explore its role in two fascinating domains: plagiarism detection and genome sequence analysis.\\n\\nLet's start with plagiarism detection. By using Edit Distance along with techniques like text normalization and tokenization, we can identify instances of copied text without proper attribution. Understanding the mechanics behind this process is crucial in building effective plagiarism detection software.\\n\\nNext, we will delve into genome sequence analysis. Edit Distance is instrumental in comparing DNA sequences across different species, which helps us understand evolutionary relationships and detect genetic mutations. Through this application, we can gain valuable insights into the structure, function, and evolution of genomes.\\n\\nLastly, we will touch on comparative genomics, where Edit Distance plays a pivotal role. By aligning genome sequences and identifying similarities and differences, we can infer evolutionary relationships and study the genetic variations that exist between species.\\n\\nThroughout this presentation, we will use engaging examples to illustrate the real-world applications of Edit Distance. By the end, you'll have a deeper understanding of how Edit Distance can be applied in academic settings and practical scenarios. So get ready for an exciting journey into the advanced applications of Edit Distance in NLP!\", 'content': \"1. Edit Distance: Measures the dissimilarity between two strings by quantifying the minimum number of operations required to transform one string into the other. \\n\\n2. Plagiarism Detection: Utilizes Edit Distance to compare documents and identify instances of copied content without proper authorization and attribution. \\n\\n3. Genome Sequence Analysis: Applies Edit Distance to align and compare DNA sequences of different organisms, providing insights into evolutionary relationships and genetic structures. \\n\\n4. Comparative Genomics: Explores how Edit Distance is employed in comparing genomic features between species, facilitating the understanding of evolutionary processes and genetic diversity. \\n\\n5. Applications of Edit Distance: Demonstrates Edit Distance's relevance in real-world scenarios such as chatbot development and AI diagnostics.\", 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide aims to build upon the foundational understanding of the Edit Distance concept by exploring its advanced applications in the fields of NLP, specifically focusing on plagiarism detection, genome sequence analysis, and comparative genomics, which have not been fully examined in previous sessions. The goal is to leverage this deeper dive to enhance the student's comprehension for their NLP class and to nourish their interest in wide-ranging practical applications.\", 'concepts': ['Edit Distance', 'plagiarism detection', 'Comparative Genomics', 'genome sequence analysis']}\n",
      "\n",
      "{'title': 'Exploring the Nuances of Sentence Segmentation in Text Analysis', 'presentation': 'Welcome to the slide on Exploring the Nuances of Sentence Segmentation in Text Analysis. In this slide, we will delve into the intricacies of segmenting text into sentences, an important pre-processing step in Natural Language Processing (NLP), and how it paves the way for more advanced text analysis.\\n\\nSentence Segmentation Techniques involve methods and algorithms used in NLP to divide a text into its constituent sentences. The goal is to accurately identify sentence boundaries, which can be complicated by the use of punctuation for other purposes, such as decimal points or abbreviations.\\n\\nCommon techniques for sentence segmentation include the use of punctuation cues, capitalization, machine learning models, and rules-based systems that consider the linguistic and contextual structure of the text.\\n\\nAccurate sentence segmentation is crucial in various text analysis tasks, such as sentiment analysis, topic modeling, and named entity recognition. It enables the effective structuring of unstructured text data, allowing for more in-depth analysis and insights.\\n\\nIn this slide, we will explore different sentence segmentation methods and their relevance in text analysis. We will also provide practical examples and exercises using Regular Expressions, a powerful tool for text processing, to demonstrate how sentence boundaries can be identified. Through interactive components, you will have the opportunity to apply these techniques and evaluate their effectiveness in segmenting a given text.\\n\\nBy the end of this slide, you will have a solid grasp of sentence segmentation techniques in NLP and their practical applications. This knowledge will not only enrich your understanding of text analysis but also equip you with the skills needed to excel in your NLP class and beyond.', 'content': '1. Introduction to Sentence Segmentation Techniques: In NLP, Sentence Segmentation Techniques involve dividing a text into sentences, identifying sentence boundaries, and overcoming challenges like punctuation dual use and abbreviations.\\n\\n2. Overview of Sentence Segmentation Methods: Methods include punctuation and capitalization cues, machine learning models, and rules-based systems that consider the linguistic and contextual structure of the text.\\n\\n3. Relevance and Importance in Text Analysis: Accurate sentence segmentation is crucial for effective sentiment analysis, topic modeling, and named entity recognition, enhancing the overall performance of NLP tasks.\\n\\n4. Application using Regular Expressions: Regular Expressions can be employed to detect sentence boundaries, and understanding the basic syntax and operators is essential in this context.\\n\\n5. Practical Exercise: Engage with a hands-on exercise to practice different sentence segmentation techniques and evaluate their effectiveness.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide will delve into the intricacies of segmenting text into sentences, an important pre-processing step in NLP, and how it paves the way for more advanced text analysis. As we have already touched upon Text Normalization and Tokenization, understanding Sentence Segmentation will enrich the student's comprehension of how texts are prepared for NLP tasks.\", 'concepts': ['Sentence Segmentation Techniques', 'Text Analysis', 'Natural Language Processing (NLP)', 'Regular Expressions']}\n",
      "\n",
      "{'title': 'Exploring the Nuances of Sentence Segmentation in Text Analysis', 'presentation': \"Good day, students! Today, we will explore the nuances of sentence segmentation in text analysis. Sentence segmentation is a crucial pre-processing step in natural language processing (NLP), as it paves the way for more advanced text analysis. By accurately identifying sentence boundaries, we can decipher the meaning and structure of texts in NLP applications.\\n\\nLet's first understand the importance of sentence segmentation in NLP. Accurate segmentation allows us to perform tasks like sentiment analysis, machine translation, and named entity recognition. It forms the backbone of these analyses, providing a solid foundation for further processing.\\n\\nHowever, sentence segmentation comes with its challenges. Punctuation can often complicate the process, as it is used for purposes other than sentence boundaries, such as decimals or abbreviations. We'll look at examples where standard boundary markers, like periods, can lead to incorrect segmentations.\\n\\nTo overcome these challenges, various techniques are employed. Rule-based systems use linguistic rules and contextual understanding to determine sentences. Machine learning models leverage large datasets to learn sentence boundary prediction. We also rely on punctuation cues and capitalization to guide the segmentation process.\\n\\nOne technique that plays a significant role in sentence segmentation is regular expressions (regex). Regular expressions serve as a powerful tool for pattern recognition and can help identify potential sentence boundaries. We'll explore the application of regex in practical examples and discuss its adaptability and limitations in various text scenarios.\\n\\nIn summary, we have discussed the importance of sentence segmentation in NLP, the challenges it poses, and the techniques used to overcome those challenges. We have also seen the connection between sentence segmentation and broader text analysis in NLP. By the end of this lesson, you will have a solid understanding of how sentences are segmented and why it is essential in NLP applications.\\n\\nNow, let's move on to some interactive examples and exercises to reinforce the material and ensure a thorough understanding. Feel free to ask any questions you may have along the way!\", 'content': '1. Importance of Sentence Segmentation in NLP: Sentence segmentation is a critical pre-processing step in NLP, enabling advanced text analysis tasks by accurately identifying sentence boundaries.\\n2. Sentence Segmentation Challenges: Various challenges, such as handling punctuation and abbreviations, can complicate sentence segmentation. Incorrect segmentations due to punctuation cues will be explored.\\n3. Techniques Overview: This section presents an overview of sentence segmentation techniques, including rule-based systems, machine learning models, and the use of punctuation cues.\\n4. Connection to Text Analysis and NLP: The importance of correctly segmented sentences will be discussed, highlighting how it supports further text analysis tasks in the broader field of NLP.\\n5. Exploring Regular Expressions Role: Regular Expressions play a crucial role in pattern recognition for identifying sentence boundaries. Practical illustrations will demonstrate their application.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide will delve into the intricacies of segmenting text into sentences, an important pre-processing step in NLP, and how it paves the way for more advanced text analysis. As we have already touched upon Text Normalization and Tokenization, understanding Sentence Segmentation will enrich the student's comprehension of how texts are prepared for NLP tasks.\", 'concepts': ['Sentence Segmentation Techniques', 'Text Analysis', 'Natural Language Processing (NLP)', 'Regular Expressions']}\n"
     ]
    }
   ],
   "source": [
    "### Slide generation from AITutor\n",
    "notebank = NoteBank()\n",
    "[notebank.add_note(n) for n in tutor_plan.split(\"\\n\")]\n",
    "slide_planner = SlidePlanner(notebank, concept_db)\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"./temp_slideplan.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(\"./temp_slideplan.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        slide_plans = pkl.load(f)\n",
    "        print(\"\\n\\n\".join([str(slide.format_json()) for slide in slide_plans]))\n",
    "        slide_planner.SlidePlans = slide_plans\n",
    "else:\n",
    "    slide_planner.generate_slide_plan()\n",
    "    with open(\"./temp_slideplan.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(slide_planner.SlidePlans, f)\n",
    "\n",
    "\n",
    "if os.path.exists(\"./temp_slides.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(\"./temp_slides.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        slides = pkl.load(f)\n",
    "        print(\"\\n\\n\".join([str(slide.format_json()) for slide in slides]))\n",
    "        slide_planner.Slides = slides\n",
    "else:\n",
    "    slide_planner.generate_slide_deque()\n",
    "    with open(\"./temp_slides.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(slide_planner.Slides, f)\n",
    "\n",
    "print(\"\\n\\n\".join([str(slide.format_json()) for slide in slide_planner.Slides]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "- Preprocessing\n",
    "- Generation of questions from (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERRORS\n",
    "\n",
    "\n",
    "the thing we are checking for errors is number of api calls per errors. api calls during translation / errors during translation\n",
    "gpt-4 and gpt-3.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCEPTS RATIO OF NUMBER OF RELEVANT CONCEPTS OVER NUMBER OF CONCEPS\n",
    "GPT-3.5 vs GPT-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
