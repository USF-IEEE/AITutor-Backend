{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricizing LLMaAiTB-E\n",
    "\n",
    "- Our Focus: Generation Quality\n",
    "- Measurement Techniques: \n",
    "    - Vector Comparison\n",
    "    - Human Preference Sample (A/B/C Testing)\n",
    "    - ???\n",
    "- Iterative documents to measure:\n",
    "    - Concepts (Generation phase)\n",
    "    - Slides (Teaching phase)\n",
    "    - Questions (Testing phase)\n",
    "- Resources for testing:\n",
    "    - Expert (From classes)\n",
    "    - GPT4 (Generation)\n",
    "    - LLMaAiTB-E (Teachabull)\n",
    "- Main concepts to cover:\n",
    "    - Object Oriented Programming\n",
    "    - Programming Language Semantics\n",
    "    - Math\n",
    "    - History\n",
    "    - \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Helper Functions\n",
    "We will demonstrate our metrics using OpenAI's Vector Embeddings on our generated documents. We decided to use OpenAI's embeddings due to their large document size capacity. We agreed that this method would prove to be the best while comparing large documents.\n",
    "\n",
    "## LLM Prompt/Text Completion\n",
    "\n",
    "\n",
    "## Vector Comparison\n",
    "Embeddings: OpenAIâ€™s text embeddings measure the relatedness of text strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import myenv\n",
    "import os\n",
    "import pickle as pkl\n",
    "from AITutor_Backend.src.TutorUtils.concepts import *\n",
    "from AITutor_Backend.src.TutorUtils.notebank import NoteBank\n",
    "from AITutor_Backend.src.TutorUtils.slides import SlidePlan, Slide, SlidePlanner, Purpose\n",
    "from AITutor_Backend.src.TutorUtils.questions import Question, QuestionSuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPENAI HELPER FUNCTIONS \n",
    "def request_output_from_llm(prompt, model: str):\n",
    "    \"\"\"Requests the Concept information from an LLM.\n",
    "\n",
    "    Args:\n",
    "        prompt: (str) - string to get passed to the model\n",
    "        model: (str) - \n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI() \n",
    "    \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "    {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": prompt,\n",
    "    },\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=8000,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Embeddings (1, 1536): [[-0.00125975  0.00951579  0.0048352  ... -0.01831474 -0.00588236\n",
      "  -0.04087433]]\n",
      "Cosine Similarity: 1.0\n"
     ]
    }
   ],
   "source": [
    "### Vector Functions\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "def tokenizer(text):\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    return tokens\n",
    "\n",
    "def process_in_batches(tokens, batch_size=8000):\n",
    "    for i in range(0, len(tokens), batch_size):\n",
    "        yield tokens[i:i + batch_size]\n",
    "\n",
    "def create_embeddings(text):\n",
    "    tokens = tokenizer(text)\n",
    "    embeddings = []\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")  # Reuse the encoding for decoding\n",
    "\n",
    "    for token_batch in process_in_batches(tokens):\n",
    "        # Convert token batch back to string\n",
    "        batch_text = encoding.decode(token_batch)\n",
    "        batch_embedding = get_embedding(batch_text)\n",
    "        embeddings.append(batch_embedding)\n",
    "\n",
    "    return np.mean([embeddings], axis=1)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n",
    "# Test works\n",
    "embeddings = create_embeddings(\"Research/generation_data/slides/Expert/codingSlides_expert.json\")\n",
    "print(f\"Vector Embeddings {embeddings.shape}:\", embeddings, )\n",
    "\n",
    "# Test = 1\n",
    "vec1 = np.array([1, 2, 3])\n",
    "vec2 = np.array([2, 4, 6])\n",
    "similarity = cosine_similarity(vec1, vec2)\n",
    "print(f\"Cosine Similarity: {similarity}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts\n",
    "- Preprocessing\n",
    "- Generation of Graph for (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notebanks from AI Tutor\n",
    "current_topic = \"ds\"\n",
    "main_concept = \"Graph Data Structure\"\n",
    "tutor_plan_nlp = '''Main Concept: Regular Expressions, Text Normalization, Edit Distance in Natural Language Processing (NLP)\n",
    "Student is a computer science student with no prior knowledge of the topic, requiring an introductory lesson.\n",
    "Student is taking an NLP class, suggesting the lessons are for academic purposes and should cover necessary conceptual detail.\n",
    "Student provided a chapter summary that includes key subtopics; this will be a guide in structuring the lesson plan.\n",
    "Tutor shall educate on the following concepts:\n",
    "Subconcept: Introduction to Regular Expressions\n",
    "Subconcept: Uses of Regular Expressions in NLP\n",
    "Subconcept: Basic Syntax and Operators of Regular Expressions\n",
    "Subconcept: Practical Examples and Exercises Using Regular Expressions\n",
    "Subconcept: Introduction to Text Normalization\n",
    "Subconcept: Tokenization of Text\n",
    "Subconcept: Lemmatization and its Importance\n",
    "Subconcept: Sentence Segmentation Techniques\n",
    "Subconcept: Introduction to Edit Distance\n",
    "Subconcept: Applications of Edit Distance Algorithm in NLP\n",
    "Subconcept: Calculation of Edit Distance and String Alignment\n",
    "Tutor will apply practical examples relevant to modern NLP applications, such as chatbots, using the chapter summary as a conversational context.\n",
    "Tutor will provide hands-on practice problems and ensure the student understands the implementation of the concepts.\n",
    "Students objective: To gain a foundational understanding of the chapter's main points, to apply this understanding in an academic setting, and to perform well in the NLP class.\n",
    "Since the student might need to have a deep understanding of the class material, the lesson should provide a solid theoretical basis, followed by practical application.\n",
    "Concept: Regular Expressions, Text Normalization, Edit Distance in Natural Language Processing (NLP)\n",
    "Concept: Introduction to Regular Expressions\n",
    "Concept: Uses of Regular Expressions in NLP\n",
    "Concept: Basic Syntax and Operators of Regular Expressions\n",
    "Concept: Practical Examples and Exercises Using Regular Expressions\n",
    "Concept: Introduction to Text Normalization\n",
    "Concept: Tokenization of Text\n",
    "Concept: Lemmatization and its Importance\n",
    "Concept: Sentence Segmentation Techniques\n",
    "Concept: Introduction to Edit Distance\n",
    "Concept: Applications of Edit Distance Algorithm in NLP\n",
    "Concept: Calculation of Edit Distance and String Alignment\n",
    "Concept: Practical Examples and Exercises in Modern NLP Applications (e.g., Chatbots)\n",
    "Concept: Hands-on Practice Problems\n",
    "Concept: Foundational Understanding of Main Points\n",
    "Concept: Academic Application of Concepts\n",
    "Concept: Theoretical Basis Followed by Practical Application\n",
    "Student's Interest Statement: I find natural language processing interesting and important since I am taking it as a course in college where I will be tested\n",
    "Student's Slides Preference Statement: I want to be taught by information and examples\n",
    "Student's Questions Preference Statement: 2 of multiple choice, 2 of free response and 2 coding questions'''\n",
    "\n",
    "tutor_plan_economics = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "tutor_plan_calc = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "tutor_plan_data_structures = \"\"\"Comprehensive overview of graph data structures planned.\\n Student wants to learn about graph data structures, their formalization, complexities (time and space), representations, algorithms, and applications.\\n 'Subconcept: Definitions and Formalization of Graph Theory\\n Subconcept: Time Complexity of Graph Algorithms\\n 'Subconcept: Space Complexity of Graph Data Structures\\n 'Subconcept: Representations of Graphs (Adjacency Matrix and List)\\n 'Subconcept: Graph Traversal Algorithms (DFS and BFS)\\n \"Subconcept: Graph Pathfinding Algorithms (Dijkstra's, A*, Bellman-Ford)\", 'Subconcept: Network Flow (Ford-Fulkerson, Edmonds-Karp)\\n 'Subconcept: Graph Coloring and Scheduling (Chromatic Number, Greedy Algorithm)\\n \"Subconcept: Trees and Special Graphs (Spanning Trees, Minimum Spanning Trees: Prim's and Kruskal's Algorithms)\", 'Subconcept: Graph Invariants (Degree Sequence, Hamiltonian, Eulerian Paths and Circuits)\\n 'Subconcept: Practical Applications of Graph Theory in Various Fields (Computer Science, Biology, Social Sciences, etc.)\\n 'Tutor will explain and demystify complex topics with easily digestible examples, ensuring theoretical knowledge is bolstered by practical application.\\n 'Tutor will discuss the computational considerations involved in using graphs with a focus on optimizations and real-world constraints.\\n 'Tutor will present common problems and solutions in graph theory to illustrate course concepts.\\n 'Since the conversation is leading toward a structured and comprehensive overview, it will be necessary to propose structured lessons that build upon each other, to cement understanding and facilitate retention.\n",
    "Graph Data Structures\\n 'Definitions and Formalization of Graph Theory\\n 'Time Complexity of Graph Algorithms\\n 'Space Complexity of Graph Data Structures\\n 'Representations of Graphs (Adjacency Matrix and List)\\n 'Graph Traversal Algorithms (DFS and BFS)\\n \"Graph Pathfinding Algorithms (Dijkstra's, A*, Bellman-Ford)\", 'Network Flow (Ford-Fulkerson, Edmonds-Karp)\\n 'Graph Coloring and Scheduling (Chromatic Number, Greedy Algorithm)\\n \"Trees and Special Graphs (Spanning Trees, Minimum Spanning Trees: Prim's and Kruskal's Algorithms)\", 'Graph Invariants (Degree Sequence, Hamiltonian, Eulerian Paths and Circuits)\\n 'Practical Applications of Graph Theory in Various Fields (Computer Science, Biology, Social Sciences, etc.)\\n 'Graph Theory Learning Approach\\n 'Demystifying Complex Topics with Easily Digestible Examples\\n 'Theoretical Knowledge Bolstered by Practical Application\\n 'Computational Considerations in Graph Theory\\n 'Optimizations and Real-World Constraints\\n 'Common Problems and Solutions in Graph Theory\\n Structured and Comprehensive Lesson Plan\\n\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "tutor_plan_history = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "current_plan = {'NLP': tutor_plan_nlp, \"history\": tutor_plan_history, \"ds\":tutor_plan_data_structures, \"econ\": tutor_plan_economics, \"calc\": tutor_plan_calc}[current_topic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concept generation from AITutor:\n",
    "import pickle as pkl\n",
    "notebank = NoteBank()\n",
    "[notebank.add_note(n) for n in current_plan.split(\"\\n\")]\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(f\"Research/temp_data/temp_concepts_{current_topic}.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(f\"Research/temp_data/temp_concepts_{current_topic}.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        concept = pkl.load(f)\n",
    "        concept_db = ConceptDatabase(main_concept, notebank.env_string(), False)\n",
    "        concept_db.Concepts = concept\n",
    "else:\n",
    "    concept_db = ConceptDatabase(main_concept,notebank.env_string())\n",
    "    with open(f\"Research/temp_data/temp_concepts_{current_topic}.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(concept_db.Concepts, f)\n",
    "\n",
    "print(\"\\n\\n\".join([slide.format_json() for slide in concept_db.Concepts]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slides: \n",
    "- Preprocessing\n",
    "- Generation of Document for (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SLIDE OBJ PROMPTs\n",
    "prompt = ''' #Your task is to create a JSON object from a slide string. View the example Input and output, and then repeat the same for the provided input. \n",
    "Perform the conversion for each slide s in the input string such that s->json_object(s). You should be able to figure out which is the title and which is the description.\n",
    "IMPORTANT: Escape Characters in JSON Data can cause errors if the JSON Object or JSON data contains backslashes, which means they need to be properly escaped\n",
    "Avoid these errors: Invalid \\escape: line 24 column 72 (char 2199)\n",
    "By properly escaping your backslashes ('\\\\')\n",
    "IMPORTANT: If there is two words together, such as \"functionwhere\", without being separated with a white space, that most probably means that there is a new line ('\\n') or space (' ') in between them, e.g. \"function where\".\n",
    "\n",
    "// Input:\n",
    "Page 1 Content:\n",
    "Natural Language ProcessingProfessor John LicatoUniversity of South FloridaChapter 2:RegEx, Edit Distance\n",
    "\n",
    "----------------------------------------\n",
    "Page 2 Content:\n",
    "\"Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When youâ€™re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.\"Regular Expressions\n",
    "----------------------------------------\n",
    "Page 3 Content:\n",
    "The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))Regular Expressions\n",
    "----------------------------------------\n",
    "Page 4 Content:\n",
    "The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')Regular Expressions\n",
    "----------------------------------------\n",
    "Page 5 Content:\n",
    "Creating regex objectsrâ€™ = raw string\\d â€“ placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(râ€™\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\dâ€™)\n",
    "----------------------------------------\n",
    "Page 6 Content:\n",
    "Matching regex objects\n",
    "mo = match object â€“ contains the result of our search>>> import re>>> phoneNumRegex = re.compile(râ€™\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\dâ€™)>>> mo = phoneNumRegex.search(â€˜My number is 415-555-4242.â€™)>>> print(â€˜Phone number found: â€™ + mo.group())Phone number found: 415-555-4242\n",
    "----------------------------------------\n",
    "Page 7 Content:\n",
    "Text Normalizationâ€¢We will work a lot with large datasets / corporaâ€¢We often need to pre-process textâ€¢Tokenizing (segmenting) wordsâ€¢Normalizing word formatsâ€¢Segmenting sentences (e.g. by using punctuation)\n",
    "----------------------------------------\n",
    "Page 8 Content:\n",
    "Tokenization â€“ segmenting running text into words (or word-like units)>>> text = 'That U.S.A. poster-print costs $12.40...'>>> pattern = r\\'\\'\\', (?x)  # set flag to allow verbose regexps...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A....     | \\w+(-\\w+)*      # words with optional internal hyphens...     | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%...     | \\.\\.\\.          # ellipsis...     | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [... \\'\\'\\'>>> nltk.regexp_tokenize(text, pattern)['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n",
    "----------------------------------------\n",
    "Page 9 Content:\n",
    "Subword tokenizationâ€¢How do we capture relations between words like:â€“new, newerâ€“blow, blowingâ€“precipitation, precipitateâ€¢Often useful to break tokens into *sub*wordsâ€¢Usually split into token learners, and token segmenters\n",
    "----------------------------------------\n",
    "Page 10 Content:\n",
    "Byte-pair encoding (BPE)â€¢A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab VV <- all unique characters in C                  # initial set of tokens is charactersfor i = 1 to k do                                # merge tokens til k times    t_L, t_R <- Most frequent pair of adjacent tokens in C    t_new <- t_L + t_R                           # make new token by concatenating    V <- V + t_new                               # update the vocabulary    Replace each occurrence of t_L, t_R in C with t_new # and update the corpusreturn Vcorpus5 low_2 lowest_6 newer_3 wider_2 new_vocabulary_, d, e, i, l, n, o, r, s, t, wcorpus5 low _2 lowest _6 newer _3 wider _2 new _vocabulary_, d, e, i, l, n, o, r, s, t, w, er\n",
    "----------------------------------------\n",
    "...\n",
    "        \n",
    "// Output:\n",
    "        { \n",
    "                \\\"slides\\\":[\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Natural Language Processing\\\", \n",
    "                                \\\"Description\\\": \\\"Professor John Licato University of South Florida Chapter 2:RegEx, Edit Distance\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When youâ€™re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Creating regex objects\\\", \n",
    "                                \\\"Description\\\": \\\"râ€™ = raw string\\d â€“ placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(râ€™\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\dâ€™)\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Matching regex objects\\\", \n",
    "                                \\\"Description\\\": \\\">>> import re>>> phoneNumRegex = re.compile(râ€™\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\dâ€™)>>> mo = phoneNumRegex.search(â€˜My number is 415-555-4242.â€™)>>> print(â€˜Phone number found: â€™ + mo.group())Phone number found: 415-555-4242 mo = match object â€“ contains the result of our search\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Text Normalization\\\", \n",
    "                                \\\"Description\\\": \\\"â€¢We will work a lot with large datasets / corpora\\nâ€¢We often need to pre-process text\\nâ€¢Tokenizing (segmenting) words\\nâ€¢Normalizing word formats\\nâ€¢Segmenting sentences (e.g. by using punctuation) )\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Tokenization â€“ segmenting running text into words (or word-like units)\\\", \n",
    "                                \\\"Description\\\": \\\">>> text = 'That U.S.A. poster-print costs $12.40...'\\n>>> pattern = r\\'\\'\\', (?x)  # set flag to allow verbose regexps\\n...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A\\n....     | \\w+(-\\w+)*      # words with optional internal hyphens\\n...     | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\\n...     | \\.\\.\\.          # ellipsis...     | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [\\n... \\'\\'\\'\\n>>> nltk.regexp_tokenize(text, pattern)\\n['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Subword tokenization\\\", \n",
    "                                \\\"Description\\\": \\\"â€¢How do we capture relations between words like:\\nâ€“new, newer\\nâ€“blow, blowing\\nâ€“precipitation, precipitate\\nâ€¢Often useful to break tokens into *sub*wordsâ€¢Usually split into token learners, and token segmenters\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Byte-pair encoding (BPE)\\\", \n",
    "                                \\\"Description\\\": \\\"â€¢A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab V\\nV <- all unique characters in C                  # initial set of tokens is characters\\nfor i = 1 to k do                                # merge tokens til k times    \\nt_L, t_R <- Most frequent pair of adjacent tokens in C    \\nt_new <- t_L + t_R                           # make new token by concatenating    \\nV <- V + t_new                               # update the vocabulary    \\nReplace each occurrence of t_L, t_R in C with t_new # and update the corpus\\nreturn V\\ncorpus\\n5 low_\\n2 lowest_\\n6 newer_\\n3 wider_\\n2 new_\\nvocabulary\\n_, d, e, i, l, n, o, r, s, t, w\\ncorpus\\n5 low _\\n2 lowest _\\n6 newer _\\n3 wider _\\n2 new _\\nvocabulary\\n_, d, e, i, l, n, o, r, s, t, w, er\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        ...\n",
    "                ]\n",
    "        }\n",
    "Remember! Escape Characters in JSON Data: If the JSON Object or JSON data contains backslashes, they need to be properly escaped.\n",
    "Avoid these errors: Invalid \\escape: line 24 column 72 (char 2199)\n",
    "\n",
    "// Input:\n",
    "        $SLIDE$\n",
    "\n",
    "// Output:\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Slide helper functions\n",
    "import PyPDF2\n",
    "from pptx import Presentation\n",
    "import json\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"Reads a PDF file and prints the content of each page\"\"\"\n",
    "    slide_str = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "\n",
    "        for i in range(num_pages):\n",
    "            page = reader.pages[i]\n",
    "            text = page.extract_text()\n",
    "            slide_str += f\"Page {i+1} Content:\\n{text}\"\n",
    "            slide_str += \"\\n\" + (\"-\" * 40) + \"\\n\"\n",
    "    return slide_str\n",
    "\n",
    "def extract_text_from_slide(slide):\n",
    "    \"\"\"Extracts title and content from a slide\"\"\"\n",
    "    title = slide.shapes.title.text if slide.shapes.title else \"No Title\"\n",
    "    content = []\n",
    "\n",
    "    for shape in slide.shapes:\n",
    "        if hasattr(shape, \"text\"):\n",
    "            content.append(shape.text)\n",
    "\n",
    "    return title, content\n",
    "\n",
    "def read_pptx(file_path):\n",
    "    \"\"\"Reads a pptx file and prints the title and content of each slide\"\"\"\n",
    "    prs = Presentation(file_path)\n",
    "    s = \"\"\n",
    "    for slide in prs.slides:\n",
    "        title, content = extract_text_from_slide(slide)\n",
    "        s+=\"Title: {title}\"\n",
    "        s+=\"Content:\"+\"\\n\".join(content)\n",
    "        s+=\"-\" * 40 + \"\\n\"\n",
    "    return s\n",
    "def get_slide_prompt(slide_template, data):\n",
    "    return slide_template.replace(\"$SLIDE$\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 21 column 38 (char 1854)\n"
     ]
    }
   ],
   "source": [
    "### TEST SLIDE OBJ GEN FROM GPT FOR EXPERT\n",
    "slide_str = read_pptx('Research/generation_data/slides/Expert/L18_ Graphs.pptx')\n",
    "\n",
    "\n",
    "curr_prompt = get_slide_prompt(prompt, slide_str)\n",
    "try:\n",
    "    json_data = request_output_from_llm(prompt=curr_prompt, model=\"gpt-3.5-turbo-16k\")\n",
    "    slide_obj = json.loads(json_data)\n",
    "    print(slide_obj)\n",
    "\n",
    "    # Convert the dictionary to a JSON-formatted string\n",
    "    json_str = json.dumps(slide_obj, indent=4)  # indent for pretty-printing\n",
    "\n",
    "    # Write the JSON string to a file\n",
    "    with open(\"Research/generation_data/slides/Expert/dsSlides_expert.json\", \"w\") as f:\n",
    "        f.write(json_str)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Introduction to Natural Language Processing (NLP) and Regular Expressions', 'purpose': 0, 'purpose_statement': 'To give the student an initial overview of NLP with a focus on Regular Expressions and how they play a foundational role in text analysis.', 'concepts': ['Natural Language Processing (NLP)', 'Regular Expressions', 'Text Normalization', 'Tokenization of Text', 'Edit Distance']}\n",
      "\n",
      "{'title': 'Decoding Patterns with Regular Expressions in NLP', 'purpose': 0, 'purpose_statement': 'This slide will serve as an initial deep dive into the world of Regular Expressions, enabling students to understand their syntax, basic operators, and fundamental uses in NLP, setting the groundwork for more sophisticated text processing tasks.', 'concepts': ['Regular Expressions']}\n",
      "\n",
      "{'title': 'Discovering Lemmatization: Enhancing Text Analysis in NLP', 'purpose': 0, 'purpose_statement': \"This slide aims to introduce Lemmatization as an essential NLP text preprocessing technique, building on the student's understanding of Text Normalization and Tokenization, to further enhance their text analysis skills.\", 'concepts': ['Lemmatization', 'Text Normalization', 'Tokenization of Text']}\n",
      "\n",
      "{'title': 'Engaging with Part-of-Speech Tagging in NLP', 'purpose': 0, 'purpose_statement': 'To introduce the foundational concept of part-of-speech tagging within Natural Language Processing (NLP), elucidating its definition, importance, and the role it plays in understanding the grammatical structure of language for text analysis and other NLP applications.', 'concepts': ['part-of-speech tagging', 'Tokenization of Text', 'Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Unveiling the Layers of Language: Syntax Parsing and Sentence Segmentation', 'purpose': 0, 'purpose_statement': \"This slide is designed to introduce the foundational concepts of 'Syntax Parsing' and 'Sentence Segmentation Techniques' in NLP, underscoring their roles in text analysis, their importance in machine understanding of language, and how they pave the way for advanced NLP tasks such as dependency parsing and named entity recognition.\", 'concepts': ['Syntax Parsing', 'Sentence Segmentation Techniques']}\n",
      "\n",
      "{'title': 'Transforming Texts: Mastering Edit Distance & Regular Expressions in NLP', 'purpose': 3, 'purpose_statement': \"This slide is tailored to solidify the student's foundational knowledge of Regular Expressions, focusing on their practical applications in text normalization and edit distance calculations in NLP. The content will bridge theoretical concepts with real-world tools to enhance the student's performance in an academic setting, and facilitate the application of these concepts through interactive examples and aligned practice problems.\", 'concepts': ['Regular Expressions', 'Edit Distance', 'Text Normalization']}\n",
      "\n",
      "{'title': 'Diving Deeper: Practical Applications of Edit Distance in NLP', 'purpose': 2, 'purpose_statement': 'This slide uncovers the significant role of Edit Distance in the field of Natural Language Processing, with a focus on real-world applications like spelling correction in digital text analysis and comparative genomics through DNA sequencing. The aim is to provide the student with an understanding of Edit Distance that bridges the gap between academic learning and practical implementation in modern computational linguistics and bioinformatics.', 'concepts': ['Edit Distance']}\n",
      "\n",
      "{'title': 'Enhancing Natural Language Processing with Tokenization Techniques', 'purpose': 2, 'purpose_statement': \"This slide aims to delve into the specifics of 'Tokenization'â€”a fundamental concept in NLP not yet fully explored in our slide set. By exploring different tokenization techniques and their pertinence in various contexts of NLP, the student will gain a deeper understanding of how text is processed at the foundational level for more complex tasks.\", 'concepts': ['Tokenization', 'Regular Expressions', 'Text Normalization']}\n",
      "\n",
      "{'title': 'Enhancing Natural Language Processing with Tokenization Techniques', 'purpose': 2, 'purpose_statement': \"This slide aims to delve into the specifics of 'Tokenization'â€”a fundamental concept in NLP not yet fully explored in our slide set. By exploring different tokenization techniques and their pertinence in various contexts of NLP, the student will gain a deeper understanding of how text is processed at the foundational level for more complex tasks.\", 'concepts': ['Tokenization', 'Regular Expressions', 'Text Normalization']}\n",
      "\n",
      "{'title': 'Navigating the Grammar of Language: part-of-speech tagging in NLP', 'purpose': 2, 'purpose_statement': 'This slide aims to deepen the studentâ€™s understanding of the part-of-speech tagging process within the broader scope of NLP, demonstrating the connection between tagging, sentence structure, and linguistic meaning, as well as its application in real-world text analysis challenges.', 'concepts': ['part-of-speech tagging', 'Natural Language Processing (NLP)', 'Tokenization of Text', 'Syntax Parsing']}\n",
      "\n",
      "{'title': 'Practical Insights into Lemmatization and Sentence Segmentation in NLP', 'purpose': 4, 'purpose_statement': \"To consolidate the student's theoretical understanding of Lemmatization and Sentence Segmentation Techniques through practical examples and exercises, demonstrating their vital roles in NLP applications such as machine translation and information retrieval systems.\", 'concepts': ['Lemmatization', 'Sentence Segmentation Techniques']}\n",
      "\n",
      "{'title': 'Advanced Utilization of Regular Expressions in NLP', 'purpose': 3, 'purpose_statement': \"To expand the student's knowledge of Regular Expressions beyond basics, covering advanced pattern matching and efficiency in text parsing, and to provide practical applications in NLP.\", 'concepts': ['Regular Expressions']}\n",
      "\n",
      "{'title': 'Mastery Through Practice: Regular Expressions in Real-World NLP Applications', 'purpose': 4, 'purpose_statement': \"This slide aims to reinforce and enhance the student's theoretical understanding of Regular Expressions through curated examples and exercises, focusing on their implementation in real-world Natural Language Processing applications.\", 'concepts': ['Regular Expressions', 'Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Synergy of Patterns and Distance: Regular Expressions Meet Edit Distances in NLP', 'purpose': 3, 'purpose_statement': 'To explain the synergistic relationship between Regular Expressions and Edit Distance in the context of Natural Language Processing (NLP) and demonstrate their application in text analysis, error detection, and correction.', 'concepts': ['Regular Expressions', 'Edit Distance', 'Text Normalization']}\n",
      "{\n",
      "    \"slides\": [\n",
      "        {\n",
      "            \"title\": \"Introduction to Natural Language Processing (NLP) and Regular Expressions\",\n",
      "            \"presentation\": \"Welcome to our first session where we'll dive into the world of Natural Language Processing, or NLP as it's commonly known. You might remember how when we text or chat, computers need to understand and process our language - that's what NLP is all about. It stands at an exciting crossroads of computer science and linguistics. Now, to make this all possible, NLP utilizes some very cool tools and techniques, one of which is called Regular Expressions. Think of Regular Expressions as a detective's toolkit for finding patterns in text - super handy for tasks like searching through documents or sorting information. For instance, if you wanted to find every email address in a document, a Regular Expression would be your go-to tool. But, before we handle complex patterns, we start with the basics of text normalization. Imagine having a robot that needs to understand our sentences; it helps to break down our sentences into words, or tokens as we call them, like taking 'I love NLP' and splitting it into individual words 'I', 'love', and 'NLP'. Plus, if we're not picky about whether a word is 'running', 'ran', or 'runs', we can simplify it to its base form 'run', which is what lemmatization is all about. This makes it a lot easier for our robot friend to understand what we're saying. Up next is this nifty concept called Edit Distance, which is like a scoring system that tells us how similar or different two words are by counting the tweaks needed to turn one into the other. For example, how would you turn 'sing' into 'sign'? Just one swap, right? Precisely, that's Edit Distance for you. Now, you're a fan of practical learning, so let's talk about how you can use Regular Expressions in real life. Let's say you're helping a friend who's overwhelmed with a messy document filled with various phrases ending in 'ing'. You can use a regex pattern like /\\\\b\\\\w+ing\\\\b/ and voila \\u2013 it'll match words like 'learning', 'running', and so on. Plus, we will walk through some exercises to reinforce your understanding \\u2013 like extracting all email addresses from a chunk of text. I'll introduce you to some essential NLP tools as well, like NLTK or spaCy, which are like the Swiss Army knives for language processing. These libraries are packed with pre-built functions and methods, so you don't have to reinvent the wheel every time you want to work with natural language data. So, are you ready to start exploring these concepts with some hands-on practice? Let's get to it and make you a pro in identifying patterns and making sense of the text!\",\n",
      "            \"content\": \"Definition of NLP: An interdisciplinary field focusing on the interaction between computers and human languages.\\nRole of Regular Expressions: Used for pattern matching and text normalization in NLP tasks such as searching, sorting, and categorization.\\nBasics of Text Normalization: Includes tokenization (e.g., 'I love NLP' -> ['I', 'love', 'NLP']) and lemmatization (e.g., 'running', 'ran', 'runs' -> 'run').\\nEdit Distance Concept: A measure for quantifying text similarity, pivotal for spell checking and plagiarism detection (e.g., From 'sing' to 'sign' entails one transposition).\\nReal-world Applications: Practical examples demonstrating Regular Expressions (e.g., regex pattern /\\\\b\\\\w+ing\\\\b/ matching words ending in 'ing').\\nText Analysis Practice: Exercises and examples where students apply concepts such as regex patterns for extracting emails from a text document.\\nEssential NLP Tools: Introduction to tools and libraries like NLTK or spaCy for processing natural language data.\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 0,\n",
      "            \"purpose_statement\": \"To give the student an initial overview of NLP with a focus on Regular Expressions and how they play a foundational role in text analysis.\",\n",
      "            \"concepts\": [\n",
      "                \"Natural Language Processing (NLP)\",\n",
      "                \"Regular Expressions\",\n",
      "                \"Text Normalization\",\n",
      "                \"Tokenization of Text\",\n",
      "                \"Edit Distance\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Decoding Patterns with Regular Expressions in NLP\",\n",
      "            \"presentation\": \"Alright, let's demystify what might initially seem like a jumble of characters and symbols by diving into Regular Expressions, or regex for short, and their role in Natural Language Processing, known as NLP. Imagine you're a detective looking for specific clues within a mountain of books. Regex is your magnifying glass that allows you to find and highlight these clues quickly. In the context of text on a computer, regex lets us search, match, and manipulate strings of text with incredible precision. This slide gives us the overview needed to start wielding this tool. First up, we have components of regex: think of characters as the individual letters or digits you're looking for, like finding the letter 'a' in a word. Wildcards are like saying 'I'm looking for anything that fits here,' a real versatile part of regex. I like to think of quantifiers as telling us how picky we are with our pattern: Are we happy with just one match, or do we want as many as possible? Anchors keep our patterns in place; we can specify whether we're latching on to the beginning or the end of the text. Look how we can apply these in basic syntax examples: Want to find every 'NLP' in a text? Use the literal characters 'NLP'. Want anything that fits the pattern 'N.X.'? Throw in a wildcard represented by a dot. With quantifiers, we can stretch a pattern out to match 'a', 'aa', or 'aaaa' using 'a+'. Now think about why we care about all of this in NLP. Tasks like tokenizing 'I love NLP!', which is splitting it into separate meaningful chunks like words or punctuation, becomes a breeze with regex. Need to standardize data? Text normalization with regex effortlessly turns 'Caf\\u00e9' into 'Cafe'. And I know you'll love this: Imagine you're working on a chatbot for your NLP class project. Regex can help your bot understand user queries by finding patterns in the text, so preliminary work with regex will pay off big time. And here's a practical example\\u2014say you're tasked with extracting email addresses from a document. Regex can swoop in and collect them for you, with a pattern like '\\\\S+@\\\\S+\\\\.\\\\S+'. That looks complex now, but by the end of our sessions, you'll be crafting patterns like that with ease. This is just the start of our journey through regex and NLP, and with these foundations, you'll be well on your way to acing your text analysis assignments.\",\n",
      "            \"content\": \"Introduction to Regular Expressions (Regex):\\n- A powerful tool for searching and manipulating text.\\n- Fundamental in text processing tasks in Natural Language Processing (NLP).\\nComponents of Regex:\\n- Characters: the simplest form of patterns (e.g., 'a', 'b', '1').\\n- Wildcards: represent any character (e.g., '.' matches any single character).\\n- Quantifiers: define quantities (e.g., '*', '+', '?' to denote zero or more, one or more, zero or one occurrences respectively).\\n- Anchors: specify the position within the text (e.g., '^' for start, '$' for end).\\nBasic Syntax Examples:\\n- Matching a specific string: 'NLP' matches 'NLP' in 'NLP is exciting'.\\n- Using wildcards: 'N.L.' matches 'NLP', 'NLL', 'N3P', etc.\\n- Applying quantifiers: 'a+' matches 'a', 'aa', 'aaa', etc.\\nUses of Regex in NLP:\\n- Tokenization: splitting text into meaningful units.\\n  Example: 'I love NLP!' -> ['I', 'love', 'NLP', '!']\\n- Text Normalization: standardizing text data.\\n  Example: 'Caf\\u00e9' -> 'Cafe'\\nPractical Application:\\n- Extracting email addresses from text:\\n  Example: '\\\\\\\\S+@\\\\\\\\S+\\\\\\\\.\\\\\\\\S+' matches 'user@example.com'\\n- Regex in chatbots for pattern matching user queries.\\nCourse Relevance:\\n- Students will use regex for text analysis assignments.\\n- Understanding regex is critical for efficient text processing in NLP-related projects.\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 0,\n",
      "            \"purpose_statement\": \"This slide will serve as an initial deep dive into the world of Regular Expressions, enabling students to understand their syntax, basic operators, and fundamental uses in NLP, setting the groundwork for more sophisticated text processing tasks.\",\n",
      "            \"concepts\": [\n",
      "                \"Regular Expressions\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Discovering Lemmatization: Enhancing Text Analysis in NLP\",\n",
      "            \"presentation\": \"Okay, let's delve into the world of lemmatization. Imagine we've got a pile of words in front of us, all different shapes and forms but with a shared identity. Think of 'run', 'runs', 'ran', and 'running'. Essentially, they're all the same action, right? Well, in NLP, we use lemmatization to strip these words down to their core, the lemma, which in this case is 'run'. Now, this isn't just about cutting corners or simplifying text; it's a meticulous process that respects the word's grammatical stance. Unlike stemming, which might just chop off word endings and call it a day, lemmatization considers how the word functions in a sentence, its actual meaning, and its inherent grammar. This is of tremendous help in NLP tasks, especially when the nuance of meaning is a game-changer, like in distinguishing verbs from nouns or adjectives in their comparative forms. By standardizing text this way, we're giving AI algorithms a clearer, more consistent diet of data to work with, enhancing machine learning model performance across various applications, including text indexing, topic modeling, and poetic semantic analysis. Yes, even poetry can be grist for the NLP mill. The process starts with tokenization, breaking down sentences into individual pieces, kind of like dissecting sentences into lego blocks. Then comes the cool part - morphological analysis. Here we're detective Holmes, examining how each word is built, its grammatical relationships, and then finally, we piece together a lemmatized sentence. Take the sentence, 'The striped bats are hanging on their feet for best.' Post lemmatization, we get a transformed lineup like 'The stripe bat be hang on their foot for good.' Moving forward, we'll see how this plays out in real-world scenarios, like chatbots, where understanding human language is critical. Lemmatization ensures these applications don't just hear, but they comprehend. It's like we're teaching them the difference between just nodding along and actually getting the point of the conversation. Now, any questions before we jump into some practical exercises?\",\n",
      "            \"content\": \"Lemmatization is the process of reducing words to their base or dictionary form (lemma)\\nExamples: 'run', 'runs', 'ran', 'running' -> Lemma: 'run'\\nLemmatization vs. Stemming:\\n- Stemming: Cuts off the ends of words (often crudely)\\n- Lemmatization: Considers word's grammatical role, meaning, and morphologies\\nImportance in NLP:\\n- Enhances text consistency and standardization for AI algorithms\\n- Improves performance of machine learning models\\n- Crucial for text indexing, topic modeling, semantic analysis\\nWorks by:\\n- 1. Tokenizing text\\n- 2. Analyzing morphological structure\\n- 3. Producing lemmatized output\\nExample Process: 'The striped bats are hanging on their feet for best.'\\nStep 1: Tokenization -> ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\\nStep 2: Morphological Analysis -> Identifying parts of speech and relationships\\nStep 3: Lemmatization Output -> ['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'good']\\nNext steps: Integration of Lemmatization in practical NLP applications (e.g., chatbots)\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 0,\n",
      "            \"purpose_statement\": \"This slide aims to introduce Lemmatization as an essential NLP text preprocessing technique, building on the student's understanding of Text Normalization and Tokenization, to further enhance their text analysis skills.\",\n",
      "            \"concepts\": [\n",
      "                \"Lemmatization\",\n",
      "                \"Text Normalization\",\n",
      "                \"Tokenization of Text\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Engaging with Part-of-Speech Tagging in NLP\",\n",
      "            \"presentation\": \"Alright, let's dive into an essential concept in Natural Language Processing, known as Part-of-Speech Tagging, or POS Tagging for short. Imagine reading a sentence and being able to identify what role each word plays \\u2014 whether a word is a noun, a verb, an adjective, and so forth. That's exactly what POS tagging does; it's like assigning a role to each actor in a play, ensuring they all contribute to the story correctly. This step is fundamental in structuring and understanding language, which, as someone fascinated by NLP and looking to excel in its applications, you'll find quite essential. Now, before we can discuss POS tagging, we have already touched upon the process of tokenization. Remember how it's like chopping up a string of text into individual pieces, like words or phrases? Well, once we have those pieces, we can begin the task of POS tagging. By analyzing the tokens, algorithms can classify them based on their usage in a sentence. Take the example 'The quick brown fox'. After going through POS tagging, we would have 'The' as a determiner, 'quick' and 'brown' as adjectives, and 'fox' as a noun. This is crucial not just for sentence structure, but for meaning too. For instance, understanding the POS helps in text normalization processes like lemmatization \\u2014 where knowing a word's POS helps us deduce its dictionary form, thereby standardizing textual data for better analysis. As you can tell, this can be incredibly useful in things you're passionate about, like improving machine translations or making chatbot interactions more natural and helpful. Going forward, we're going to look at the machine learning models that drive POS tagging under the hood and explore more advanced NLP applications. But for now, just remember, POS tagging is a key player when it comes to the grammar of language processing. Any questions on this before we move on?\",\n",
      "            \"content\": \"Definition: Assigning words to their grammatical counterparts, such as nouns, verbs, etc.\\nImportance: Essential for parsing sentences and understanding their structure and meaning.\\nConnection to Tokenization: Precedes POS tagging by dividing text into tokens (words, phrases).\\nTechniques: Use of machine learning algorithms and Regular Expressions to identify POS.\\nSyntax Example: 'The quick brown fox' -> [('The', 'Determiner'), ('quick', 'Adjective'), ('brown', 'Adjective'), ('fox', 'Noun')]\\nRelation to Text Normalization: Part-of-speech information helps in the processes like lemmatization, which requires knowledge of a word's POS.\\nPractical Utility: Imperative for tasks such as machine translation and improving chatbot interactions.\\nNext Steps: Introduction to machine learning models used for POS tagging and advanced NLP applications.\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 0,\n",
      "            \"purpose_statement\": \"To introduce the foundational concept of part-of-speech tagging within Natural Language Processing (NLP), elucidating its definition, importance, and the role it plays in understanding the grammatical structure of language for text analysis and other NLP applications.\",\n",
      "            \"concepts\": [\n",
      "                \"part-of-speech tagging\",\n",
      "                \"Tokenization of Text\",\n",
      "                \"Natural Language Processing (NLP)\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Unveiling the Layers of Language: Syntax Parsing and Sentence Segmentation\",\n",
      "            \"presentation\": \"Okay, we've had quite the journey through the world of Natural Language Processing so far, haven't we? Now, let's dive into the realm of syntax and structure with our fifth slide, focused on 'Syntax Parsing' and 'Sentence Segmentation Techniques,' pivotal processes that are fundamental in understanding how we analyze language in NLP. Picture syntax parsing as a meticulous linguist, examining each sentence to meticulously identify and classify its components, such as noun phrases and verbs. It's like dissecting a sentence into its building blocks, much like you would deconstruct a complex algorithm in your programming assignments. Take for example the sentence 'The quick brown fox jumps over the lazy dog.' Through syntax parsing, we can build a parse tree that reveals 'jumps' as the main action, surrounded by descriptors of both the subject, the 'quick brown fox,' and the object, 'the lazy dog.' Visualizing this structure helps us understand the grammatical relationships within the sentence, setting the stage for machines to process language in a more human-like manner. Just as you construct code, NLP builds these trees to decode the language. Now, shifting gears to sentence segmentation, this is crucial when we're given a huge block of text and need to break it down into digestible sentences. It's similar to debugging a program by scrutinizing it line by line. The default method might simply look for periods, question marks, and exclamation points, much like looking for semicolons at the end of a statement in your code. But what about tricky cases? Consider this: 'Dr. Smith, who arrived from the U.K., will see you now.' Without understanding context, a basic algorithm might wrongly identify 'Dr.' and 'U.K.' as the end of sentences. However, advanced techniques use contextual clues to discern that these are not sentence boundaries. These segmentation strategies help prepare texts for more advanced NLP tasks. Imagine if you fed a poorly segmented text to a chatbot you're building for your class project\\u2014it might misunderstand the queries. Now, why is all this important? Well, accurate syntax parsing and sentence segmentation are critical precursors to deeper language understanding processes like semantic analysis, machine translation, and named entity recognition that we'll explore soon. These efforts are not end-goals in themselves but rather vital steps in the journey of NLP, much as your foundational computer science courses are stepping stones to more complex topics. Understanding these concepts sets you up for a better grasp of the complexities of language that NLP aims to tackle, a necessity for excelling in your academic endeavors and practical applications alike. Alright, next up, we will look at how these techniques contribute to the robust field of dependency parsing. Ready to move forward?\",\n",
      "            \"content\": \"Syntax Parsing:\\n   - Essential for understanding language structure\\n   - Dissects sentences to identify phrases and sub-phrases\\n   - Creates parse trees to illustrate grammatical relationships\\n   - Example: 'The quick brown fox jumps over the lazy dog.'\\n      Parse tree shows 'jumps' as the central verb with two noun phrases\\nSentence Segmentation Techniques:\\n   - Crucial for dividing text into sentences\\n   - Often uses punctuation as indicators for sentence boundaries\\n   - Deals with ambiguities in punctuation (e.g., abbreviations, quotations)\\n   - Example: 'Dr. Smith, who arrived from the U.K., will see you now.'\\n      Sentence segmentation must recognize 'Dr.' and 'U.K.' do not end sentences\\nImportance in NLP:\\n   - Syntax parsing & sentence segmentation enable higher-level NLP tasks\\n   - Prerequisite for semantic analysis, translation, entity recognition\\nSetting the Stage:\\n   - Lays foundational understanding for advanced NLP topics\\n   - Facilitates text normalization and analysis in processing language\\n   - Preparatory step towards dependency parsing and named entity recognition\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 0,\n",
      "            \"purpose_statement\": \"This slide is designed to introduce the foundational concepts of 'Syntax Parsing' and 'Sentence Segmentation Techniques' in NLP, underscoring their roles in text analysis, their importance in machine understanding of language, and how they pave the way for advanced NLP tasks such as dependency parsing and named entity recognition.\",\n",
      "            \"concepts\": [\n",
      "                \"Syntax Parsing\",\n",
      "                \"Sentence Segmentation Techniques\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Transforming Texts: Mastering Edit Distance & Regular Expressions in NLP\",\n",
      "            \"presentation\": \"Alright, so we've reached an exciting part of our journey through Natural Language Processing\\u2014the point where we transform raw text into meaningful data that can be processed. On this slide, titled 'Transforming Texts: Mastering Edit Distance & Regular Expressions in NLP,' we're going to dive into the details of how Regular Expressions work and why they're such a powerful tool in text processing. Imagine Regular Expressions, or Regex for short, as a way of defining specific search patterns within strings. For example, if we use a regex pattern like /^a...s$/, it can catch any five-letter word that starts with 'a' and ends with 's'. It's like a filter that only lets through text that matches a specific format. Now, let's pivot to some real-world applications of Regex in NLP. Tokenization, which is the process of dividing text into individual words or sentences, greatly relies on these search patterns. Similarly, if we want to tidy up our data by stripping out unwanted characters or formatting, Regex comes to our rescue. Consider a scenario where we have text filled with miscellaneous characters, and we only want the letters and numbers. A simple regex pattern like /[^a-zA-Z0-9]/ will help to scrub the rest away. Moving on to Text Normalization, we take our raw text and transform it into a more uniform and standard format, which is crucial for processing and analysis. Take the phrase 'Hello, World!'\\u2014tokenization turns this into two separate words, 'Hello' and 'World', by cutting out the punctuation. Lemmatization is another technique where we reduce words to their base or dictionary form, like converting 'running' to 'run', and it helps us to consolidate similar forms of a word. Sentence Segmentation is about spotting where sentences begin and end, ensuring that the structure of our text is well-understood. Next, we look at Edit Distance, a concept that might sound more math-y, but it's actually about how similar two strings of text are. If you've ever used a spell checker and wondered how it determines that you probably meant 'kitten' when you typed 'kittne,' it's using Edit Distance. This concept is also applied in places like DNA sequencing, where even a tiny difference can have big implications. And if you've ever interacted with chatbots or voice assistants, Text Normalization is the behind-the-scenes hero that helps them understand human language better. To wrap things up, we're not just learning in theory, we're going to put it all into practice. You'll solve problems using Regex to find patterns in text, and with your newfound understanding of Edit Distance, you'll get to calculate how similar different strings are to each other. So, let's roll up our sleeves, get our hands on these concepts, and watch how they play out in real applications. It might seem complex now, but through practice, you'll master these critical NLP tools in no time.\",\n",
      "            \"content\": \"Regular Expressions (Regex) in NLP:\\n- Sequence of characters that define a search pattern\\n- Example: /^a...s$/ matches any five-letter string starting with 'a' and ending with 's'\\nApplications of Regex:\\n- Tokenization: Splitting text into words or sentences\\n- Cleaning data: Removing unwanted characters or formatting\\n- Example: Removing special characters using regex pattern /[^a-zA-Z0-9]/\\nText Normalization Techniques:\\n- Tokenization: 'Hello, World!' -> ['Hello', 'World']\\n- Lemmatization: Reduces words to their base or dictionary form\\n- Example: 'running' -> 'run'; 'better' -> 'good'\\n- Sentence Segmentation: Recognizes boundaries of sentences\\nEdit Distance - String Similarity:\\n- Definition: Minimum number of operations required to transform one string into another\\n- Operations: Insertion, deletion, or substitution of a single character\\n- Example: Edit distance between 'kitten' and 'sitting' is 3\\nPractical Implementations:\\n- Regex for extracting hashtags: /#(\\\\w+)/g -> Extracts '#hashtag' from text\\n- Edit Distance in spell checkers and DNA sequencing\\n- Text normalization for chatbots and voice assistants\\nInteractive Learning:\\n- Practice problems using regex patterns to match specific text\\n- Calculating edit distances between various strings\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 3,\n",
      "            \"purpose_statement\": \"This slide is tailored to solidify the student's foundational knowledge of Regular Expressions, focusing on their practical applications in text normalization and edit distance calculations in NLP. The content will bridge theoretical concepts with real-world tools to enhance the student's performance in an academic setting, and facilitate the application of these concepts through interactive examples and aligned practice problems.\",\n",
      "            \"concepts\": [\n",
      "                \"Regular Expressions\",\n",
      "                \"Edit Distance\",\n",
      "                \"Text Normalization\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Diving Deeper: Practical Applications of Edit Distance in NLP\",\n",
      "            \"presentation\": \"Now, let's take the Edit Distance concept that we've grasped so far and see it in action with some real-world applications, specifically in Natural Language Processing. Slide 7, titled 'Diving Deeper: Practical Applications of Edit Distance in NLP', will give us a closer look at where this theory meets technology. Remember when we mistype something on our phones or computers? Let's start there. You've probably typed 'recieve' when you meant 'receive'. Annoying, right? But thanks to spell-checking algorithms - which are powered by Edit Distance - these little typos can be caught and corrected almost instantaneously. So, how does this work? Well, each wrong letter you type is a step away from the correct spelling, and Edit Distance is just counting those steps to find the closest correct word it knows. Similarly, think about search engines. Ever typed 'fligth' and seen it suggest 'Did you mean: flight?' That's Edit Distance in action again, making sure we find what we\\u2019re looking for, even when our typing isn\\u2019t perfect. Moving on to another daily use case: smartphone keyboards. They're pretty smart these days! Type 'beleive', which isn't quite right, and see it magically change to 'believe'. These keyboard predictions are constantly using Edit Distance to predict and correct what we intended to write based on the closest matches to our misspelled words. Fascinating, isn't it? But let's shift our perspective from linguistics to biology. Edit Distance is not confined to the digital text world; it's also a workhorse in comparative genomics. Researchers compare genetic sequences like 'AGCT' and 'ACGT' by using Edit Distance to count the mutations needed to go from one to the other, which in this case is 2. This is paramount in understanding evolutionary relationships and uncovering the history written in our genes. What's more exciting is that, by using Edit Distance to study DNA sequences, scientists can measure the genetic divergence between species, aiding in the study of evolutionary biology. Through these deep-dive examples, your understanding of Edit Distance and its application in various fields will broaden. Now, to integrate all this information and apply it, we'll proceed with some practice problems and exercises that I've prepared for you. These will put your knowledge into practice and solidify your learning. Remember, NLP is immensely interesting and important, which is why we're tackling it step by step - coupling theory with practical examples, just like you wanted. Ready for the challenge?\",\n",
      "            \"content\": \"Understanding Edit Distance in spell-checking algorithms:\\n- Key component in error detection and correction tools.\\n- Example: Typing 'recieve' instead of 'receive' prompts correction.\\nEdit Distance in search engines:\\n- Improves user experience by suggesting 'Did you mean?' corrections.\\n- Example: Searching 'fligth' suggests 'flight'.\\nUse in smartphone keyboard predictions:\\n- Auto-correction based on close matches to misspelled words.\\n- Example: Typing 'beleive' might autocorrect to 'believe'.\\nApplications of Edit Distance in comparative genomics:\\n- Aids in the analysis of genetic data for similarities/differences.\\n- Example: Comparing sequences 'AGCT' and 'ACGT' has an edit distance of 2.\\nDNA sequencing and Edit Distance:\\n- Facilitates the study of evolutionary relationships.\\n- Example: Measure genetic divergence between species using edit distance in sequences.\\nAddressing student's objectives and preferences:\\n- Multiple usage examples provided for conceptual understanding.\\n- Practice problems and exercises will follow to solidify learning.\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 2,\n",
      "            \"purpose_statement\": \"This slide uncovers the significant role of Edit Distance in the field of Natural Language Processing, with a focus on real-world applications like spelling correction in digital text analysis and comparative genomics through DNA sequencing. The aim is to provide the student with an understanding of Edit Distance that bridges the gap between academic learning and practical implementation in modern computational linguistics and bioinformatics.\",\n",
      "            \"concepts\": [\n",
      "                \"Edit Distance\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Enhancing Natural Language Processing with Tokenization Techniques\",\n",
      "            \"presentation\": \"Have you ever wondered how our computers manage to understand and process the vast amount of text data available? Well, one crucial technique is called tokenization, and that's going to be our focus today. As a cornerstone of Natural Language Processing, or NLP for short, tokenization is the process where we take a string of text and split it into smaller pieces, known as tokens. These tokens can be words, phrases, symbols, or any meaningful unit of language. Why is this important, you ask? Imagine you're designing a search engine or a language translation application; you need to work with individual words to analyze text or translate phrases. Tokenization is the first step in making this possible. It allows us to break down the text into manageable pieces that we can work with to perform more complex tasks like sentiment analysis - determining whether a piece of text expresses positive, negative, or neutral feelings. Now, to carry out tokenization effectively, we often use a powerful tool called regular expressions, or regex for short. Regex is a sequence of characters that form a search pattern. If you're not familiar with regex, it might look a bit intimidating at first, but it's incredibly useful. For example, if we want to separate words in a sentence, we can use a regex pattern like s.split('\\\\W+'), which will identify all non-word characters and split the text wherever they appear. As a result, 'text-is_tokenized' would become ['text', 'is', 'tokenized']. Next, let's link tokenization to text normalization, another process where we prepare our text for analysis by converting it into a standardized format. It involves tasks like correcting typos, dealing with different forms of a word, and so on. It goes hand-in-hand with tokenization because once we have our tokens, we can start to normalize and refine them. This is where concepts like lemmatization come in - it's essentially chopping off inflections from words to return their base or root form. For instance, 'running', 'runs', 'ran' all become 'run'. This simplification helps in standardizing text data. And have you heard about the edit distance? It's a way of quantifying how different two strings are by counting the minimum number of operations required to transform one string into the other. It's a bit like the game where you have to guess a word and you're told how close your guess is to the actual word. This is absolutely critical in NLP for tasks like spell checking and error correction. To wrap up, tokenization's role in natural language processing is pivotal, and it is used in everyday applications, like when you chat with a virtual assistant or chatbot. They understand and respond to us through the underlying tokenization and normalization processes. So, the next time you interact with such an application, you'll know a bit more about what's happening behind the scenes. I hope this has given you a better understanding of how tokenization enhances NLP tasks and its relevance in the real world. Remember, it's the first step in turning raw text into meaningful data, leading to countless applications that make our digital lives a whole lot easier.\",\n",
      "            \"content\": \"Definition of Tokenization in NLP: Splitting text into tokens (words, phrases, etc.) for further processing.\\nRole of Tokenization: Essential for tasks like search engines, language translation, and sentiment analysis.\\nIntroduction to Regular Expressions (Regex): Powerful tool for pattern matching that facilitates tokenization.\\nRegex Examples: Separating words using regex pattern - s.split('\\\\\\\\W+') # -> ['text', 'is', 'tokenized']\\nText Normalization: Standardizing text into a consistent format for preprocessing.\\nConnecting Tokenization & Normalization: How tokenization leads to lemmatization and edit distance computations.\\nRole of Edit Distance: Measures how different two strings are, critical for tasks like error correction.\\nReal-World Relevance: Discussion of tokenization in everyday NLP applications like chatbots.\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 2,\n",
      "            \"purpose_statement\": \"This slide aims to delve into the specifics of 'Tokenization'\\u2014a fundamental concept in NLP not yet fully explored in our slide set. By exploring different tokenization techniques and their pertinence in various contexts of NLP, the student will gain a deeper understanding of how text is processed at the foundational level for more complex tasks.\",\n",
      "            \"concepts\": [\n",
      "                \"Tokenization\",\n",
      "                \"Regular Expressions\",\n",
      "                \"Text Normalization\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Enhancing Natural Language Processing with Tokenization Techniques\",\n",
      "            \"presentation\": \"Welcome to our discussion about one of the pillars of Natural Language Processing: Tokenization. Think of tokenization as the process of slicing and dicing text into smaller pieces, which we call 'tokens'. Imagine you're organizing a huge book fair. Before you can sort the books, you have to separate them into categories, like fiction, non-fiction, and so on. In the same vein, tokenization helps us categorize parts of text so that a computer can better understand and process it. Now, why is this important? Well, because breaking down text into manageable chunks is critical for any kind of text analysis or natural language understanding. Without it, imagine the computer trying to interpret an entire book at once \\u2013 pretty overwhelming, right? Let's consider the two main techniques according to our slide: first, using whitespace as separators. It's as simple as it sounds. Wherever there's a space, we break. So the phrase 'Hello World' becomes two separate tokens: 'Hello' and 'World.' Secondly, we look at punctuation as token boundaries. Here, something like 'Hello, World!' would become just 'Hello' and 'World' as tokens, stripping away the punctuation. Now, these basic methods are great, but they aren't flexible for all the nuances in language. That's where Regular Expressions or regex comes into play. Regex lets us create our patterns to match sequences. For instance, the regex pattern '\\\\b\\\\w+\\\\b' could extract all the words from a sentence by looking for sequences of characters divided by word boundaries. It's a bit like creating a custom sieve for sorting the types of grains you want. By implementing tokenization techniques in NLP tasks, we're able to construct more efficient search engines that parse through your queries, or language translation services that can break down sentences into translatable words or phrases. This doesn't just simplify text but also paves the way for text normalization and even calculating edit distances, two processes we'll be diving into later. And yes, it works closely with sentence segmentation, breaking down text into sentences, and lemmatization, which helps to reduce words to their base or dictionary form. All these processes interlink and feed into one another, creating a complex, but manageable system for NLP. By grasping tokenization, you're already setting the foundation for understanding how computers can interpret and respond to human language, an essential skill set for your NLP class. Given your interest and future examinations in this topic, seeing how tokenization fits into the bigger picture can help you in not just tackling academic challenges but also in appreciating the fascinating intricacies of language technologies.\",\n",
      "            \"content\": \"Defining Tokenization: Process of breaking down text into smaller 'tokens'.\\nTokenization's Role in NLP: Essential for simplifying text and subsequent analyses.\\nMain Tokenization Techniques:\\n- Whitespace as separators (e.g. 'Hello World' -> ['Hello', 'World']).\\n- Punctuation as token boundaries (e.g. 'Hello, World!' -> ['Hello', 'World']).\\nLeveraging Regular Expressions for Custom Tokenization:\\n- Regex patterns match sequences (e.g. '\\\\b\\\\w+\\\\b' extracts words).\\nImplementing Tokenization in NLP Tasks:\\n- Search engines parsing queries.\\n- Language translation services breaking sentences into words/phrases.\\nImpact on Other NLP Processes:\\n- Facilitates text normalization & edit distance calculations.\\n- Works in tandem with sentence segmentation and lemmatization.\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 2,\n",
      "            \"purpose_statement\": \"This slide aims to delve into the specifics of 'Tokenization'\\u2014a fundamental concept in NLP not yet fully explored in our slide set. By exploring different tokenization techniques and their pertinence in various contexts of NLP, the student will gain a deeper understanding of how text is processed at the foundational level for more complex tasks.\",\n",
      "            \"concepts\": [\n",
      "                \"Tokenization\",\n",
      "                \"Regular Expressions\",\n",
      "                \"Text Normalization\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Navigating the Grammar of Language: part-of-speech tagging in NLP\",\n",
      "            \"presentation\": \"Let's delve into Natural Language Processing, or NLP. It's a lot like teaching a computer to understand human language \\u2013 both the written word and the way we speak. And why is that important? Because it allows software to make sense of the information, it frees us, humans, up from doing mundane tasks. Imagine a world where every Google search, every voice-commanded task, is understood perfectly by computers. That's the goal of NLP. But before a computer can understand language, we need to break down the language into manageable parts, which is where part-of-speech tagging comes into play. Just like in school where you learned about nouns, verbs, adjectives, and so on, we teach computers to identify these in sentences to understand the structure of the language. It's a step towards making sense of sentences and their meaning. Now, think of tokenization as the act of chopping up text into pieces, like puzzle pieces. Each word is a piece, and when put together correctly, you see the full picture. It sets the stage for part-of-speech tagging. Then there's something called syntax parsing. If tokenization is about cutting up the sentence and POS tagging is about identifying the type of words, syntax parsing is about putting them back together in a way that maps out their grammatical relationships. Let's take our example sentence, 'The quick brown fox jumps over the lazy dog.' Here, we've split it into tokens and then tagged each with their part-of-speech, like 'DET' for 'The', which is a determiner, or 'NOUN' for 'dog'. You with me so far? Great, because where this all really comes to life is in practical applications. Say you're using a search engine. POS tagging helps it figure out which words are the most important in your query. Or when you're trying to translate a sentence from one language to another, understanding the role of each word is essential for an accurate translation. And of course, we can't forget about regular expressions and text normalization. You've seen how we use a simple pattern, a regular expression, to find words ending in 'n'. These powerful patterns help us sift through the text to find or manipulate specific parts. And text normalization? It's like giving the text a standard form \\u2013 no matter if you write 'U.S.A.' or 'USA', or whether you use hyphens in 'co-ordinate' or not, it all gets standardized so that the computer isn't confused by variations. Now, how about we make this interactive? Let's look at the sidebar where there's a text. Try to manually identify the parts of speech for each word. Hands-on practice will help cement what we've talked about and give you a clearer picture of how POS tagging works in the wild. Remember, it's not just about passing a class\\u2014it's about grasping a fundamental tool that's shaping the future of technology. Each piece, from tokenization to text normalization, plays a part in making NLP a powerful tool in our digital world.\",\n",
      "            \"content\": \"Overview of Natural Language Processing (NLP) and its significance\\nIntroduction to Part-of-Speech (POS) tagging in NLP\\nThe role of words in syntactic and semantic analysis\\nTokenization as a precursor to POS tagging\\nSyntax parsing and its relationship with POS tagging\\nPractical applications of POS tagging in complex NLP tasks\\nInterrelation of POS tagging with Regular Expressions and Text Normalization\\nInteractive sidebar exercise: Manually tag parts of speech in a given text\\nUnderstanding the academic implications and applications in NLP classwork\\nExamples:\\n  - Sentence: 'The quick brown fox jumps over the lazy dog.'\\n  - Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']\\n  - POS Tags: ['DET', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN']\\n  - Regular Expression: /\\\\b\\\\w+n\\\\b/ matches words ending with 'n'\\n  - Text Normalization: transforming 'U.S.A.' to 'USA', or 'co-ordinate' to 'coordinate'\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 2,\n",
      "            \"purpose_statement\": \"This slide aims to deepen the student\\u2019s understanding of the part-of-speech tagging process within the broader scope of NLP, demonstrating the connection between tagging, sentence structure, and linguistic meaning, as well as its application in real-world text analysis challenges.\",\n",
      "            \"concepts\": [\n",
      "                \"part-of-speech tagging\",\n",
      "                \"Natural Language Processing (NLP)\",\n",
      "                \"Tokenization of Text\",\n",
      "                \"Syntax Parsing\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Practical Insights into Lemmatization and Sentence Segmentation in NLP\",\n",
      "            \"presentation\": \"Alright, on to some nitty-gritty applications of what we've been discussing. Let's talk practical insights into lemmatization and sentence segmentation in Natural Language Processing, a real cornerstone in understanding and developing NLP systems, such as machine translation and information retrieval. First up, lemmatization! Remember that it's all about condensing words down to their base or 'lemma' form. It's like taking the various forms of a verb, say 'organizing', 'organized', and 'organizer', and recognizing that they're all manifestations of the word 'organize'. Now, why don't you give it a try? Take a look at this paragraph on the screen. Try to manually lemmatize the words that you spot can be reduced to their base forms; it's an excellent exercise to understand the concept better. Got it? Great, now type out your answers. Perfect, that's a fantastic start! Moving forward, we have sentence segmentation. This technique helps us in dissecting a wall of text into digestible sentences - very important for subsequent processing. Ever faced a text like, 'Dr. Smith went to the clinic. She arrived before noon.'? Right here, sentence segmentation allows us to neatly split it into two separate sentences for clarity. Why don't you attempt the next interactive exercise? There's a complex piece of text on the screen. I'd like you to mark the sentence boundaries. Yes, exactly like that! Now, let's relate this to machine translation. Have you noticed how translations can sometimes seem off? That's where lemmatization and segmentation come into play to improve accuracy, allowing the system to understand the context better and translate accordingly. The same goes for information retrieval systems - when you type a query, the system uses segmentation to pull out relevant information from immense data pools. Alright, for the home stretch, I've got something that lines up perfectly with your interest in chatbots. Let's analyze a conversation flow. Here's an example: a user says, 'I'm planning to bike because I enjoyed it a lot today.' Notice how we can distill this down to the lemmas 'plan', 'bike', and 'enjoy'? It's fascinating how this simplifies and optimizes the conversation flow, allowing the chatbot to craft a more responsive reply. As we wrap this up, I'd love for you to evaluate and optimize a snippet of dialogue using the techniques we've covered. It's a quick hands-on practice to cement these concepts. Lemmatization and segmentation aren't just theoretical; they are the gears turning behind the scenes of many applications you use every day. So, how do you feel about employing these NLP fundamentals in practical scenarios?\",\n",
      "            \"content\": \"- Review of Lemmatization: Reducing words to their base form considering morphological analysis.\\n  Example: Organizing/Organized/Organizer -> Organize\\n- Sentence Segmentation Techniques: Identifying sentence boundaries for processing.\\n  Example: 'Dr. Smith went to the clinic. She arrived before noon.' -> ['Dr. Smith went to the clinic.', 'She arrived before noon.']\\n- Interactive Exercise: Manually lemmatize words in a provided paragraph.\\n- Interactive Exercise: Segment a complex text into individual sentences.\\n- Application in Machine Translation: How lemmatization and segmentation improve accuracy.\\n- Information Retrieval Systems: Role of segmentation in effective search functions.\\n- Case Study: Chatbots - Optimizing conversation flow with lemmatization and segmentation.\\n  Example: User says, 'I'm planning to bike because I enjoyed it a lot today.' -> Chatbot recognizes 'plan', 'bike', 'enjoy' as key lemmas.\\n- Hands-on Practice: Evaluate and optimize a dialogue snippet using NLP techniques learned.\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 4,\n",
      "            \"purpose_statement\": \"To consolidate the student's theoretical understanding of Lemmatization and Sentence Segmentation Techniques through practical examples and exercises, demonstrating their vital roles in NLP applications such as machine translation and information retrieval systems.\",\n",
      "            \"concepts\": [\n",
      "                \"Lemmatization\",\n",
      "                \"Sentence Segmentation Techniques\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Advanced Utilization of Regular Expressions in NLP\",\n",
      "            \"presentation\": \"Alright, as we continue with our exploration of NLP, let's delve into a more advanced realm of Regular Expressions. Now that you're familiar with the basics, it's time we looked at their utilization in greater depth. Consider the scenario where a massive volume of text data needs to be processed quickly and efficiently for your NLP class project. This is where mastering advanced regex can be a game-changer. Revisiting our base knowledge, the combination of special characters, quantifiers, and anchors in regex allows us to match virtually any text pattern. Let's think of regex as a toolkit. Just as a carpenter selects the right tool for a job, in regex, we choose combinations of these tools to create precise search patterns. As an example, imagine we're sorting through social media posts for your project, trying to identify and categorize different emojis. With regex, we can craft a pattern that looks for the specific Unicode sequences representing those icons. Now, when it comes to optimizing our regex patterns, it's all about efficiency. In large datasets like the posts we just talked about, the difference between a good and a bad regex can be substantial in terms of processing time. This efficiency is dependent on our understanding of concepts like greediness in quantifiers and the strategic use of lookaround assertions. For instance, if we only want to find emojis that appear at the end of a sentence, we'd use a lookahead to specify that condition without including the trailing period in our match. Moving to practical applications, we've got examples such as recognizing and extracting named entities, like the names of people or places from texts, an essential step in understanding the context and meaning behind user inputs in a chatbot. Tokenization and lemmatization using regex is yet another critical NLP process, allowing us to break text into manageable pieces and reduce words to their base forms, a precursor to more complex language modeling. Speaking of chatbots, which you've expressed an interest in given your course, regex is key in parsing and understanding user input. Through pattern matching, the bot can ascertain the user's intent and extract vital information to respond accurately. And then there's the Edit Distance, it's about measuring how 'close' two strings of text are, which complements regex in applications like autocorrect features and detecting duplicates. To cement this knowledge, we'll conclude with hands-on regex exercises such as crafting patterns for syntax highlighting in a text editor, validating the structure of email addresses, and segmenting sentences for further NLP analysis. Imagine enhancing your chatbot to handle different user queries or developing a feature for your final project that highlights code syntax. All of these skills are building towards your goal of excelling in your NLP class and opening up a range of possibilities for automated text analysis.\",\n",
      "            \"content\": \"Revisiting the Bases of Regex: Constructing patterns with special characters, quantifiers, and anchors\\nOptimizing Regex: Understanding greediness, employing lookaheads and lookbehinds, using backreferences for efficiency\\nReal-World Use Cases: Examples of named entity recognition and data structure validations\\nRegex in Tokenization: How to split text data into tokens using regex patterns (e.g., \\\\w+ to match words)\\nRegex in Lemmatization: Utilizing regex to identify word stems (e.g., (\\\\w+)(ing|ed|es) to find root verbs)\\nRegex Application in Chatbots: Pattern matching for user intent recognition and information extraction\\nRegex & Edit Distance: Measuring text similarity and its importance in applications like spell checking\\nHands-on Regex Practices: Exercises on constructing regex for syntax highlighting, email validation, and sentence segmentation\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 3,\n",
      "            \"purpose_statement\": \"To expand the student's knowledge of Regular Expressions beyond basics, covering advanced pattern matching and efficiency in text parsing, and to provide practical applications in NLP.\",\n",
      "            \"concepts\": [\n",
      "                \"Regular Expressions\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Mastery Through Practice: Regular Expressions in Real-World NLP Applications\",\n",
      "            \"presentation\": \"Alright, let's dive into the fascinating world of Regular Expressions, often abbreviated as 'regex', in the context of Natural Language Processing, or NLP. Regex is a powerful tool for text processing\\u2014it's like having a Swiss Army knife for finding, manipulating, and transforming text in very specific ways.\\n\\nImagine you're sifting through a document full of random date formats and you want to standardize them to, say, 'Year-Month-Day'. With regex, we can craft a pattern like '\\\\d{2}-\\\\d{2}-\\\\d{4}', and tweak it so that it locates dates and reformats them to a 'YYYY-MM-DD' style automatically. It's efficient and, honestly, quite cool once you get the hang of it.\\n\\nMoving on to our exercises, think about how often you've seen forms asking for a valid email address. Regex comes to the rescue here by providing us with a way to validate those formats. For example, the regex pattern '\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z]{2,}\\\\b' checks if an email is in the right format. We'll try this out today so you can see regex in action.\\n\\nNow beyond cleanup, regex is superb for breaking down text. If we consider tokenization\\u2014essentially splitting text into pieces like words or phrases\\u2014a regex pattern like '(\\\\w+)' allows us to dismantle 'split, this sentence' into separate words ['split', 'this', 'sentence']. Tokenization is a cornerstone of NLP because it's how computers start to understand our language structure.\\n\\nAnd then there's the concept of Edit Distance\\u2014an algorithm that's fundamentally important for features like spell checkers and search engines optimizing their results. We'll do a demo to calculate the edit distance between the words 'kitten' and 'sitting', illustrating how NLP uses this measure to compare similarity between text strings.\\n\\nFinally, let's consider the practical application in modern NLP technology like chatbots. Regex helps in recognizing patterns, such as commands. For instance, to detect a coffee order in a chatbot, a regex like '/order \\\\d+ cups of (?:coffee|tea)/' can discern the specific request.\\n\\nOur purpose today was to show you how regex isn't just a theoretical concept. It's a robust tool that empowers real-world applications, especially in the NLP field, and the more you practice, the better equipped you'll be for your NLP class and beyond. In a moment, we'll open up for a Q&A session to clear any doubts and reinforce what we've learned.\",\n",
      "            \"content\": \"Intro to Regex: Simplifying text processing using wildcards, classes, quantifiers, anchors\\nRole in NLP: Data cleaning, pattern extraction, text normalization\\nExample: Using regex to standardize date formats (e.g., \\\\d{2}-\\\\d{2}-\\\\d{4} to YYYY-MM-DD)\\nInteractive Exercise: Validate email format (e.g., \\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z]{2,}\\\\b)\\nRegex in Tokenization: Splitting text into words or phrases\\nExample: Tokenizing a sentence (e.g., 'split, this sentence' -> ['split', 'this', 'sentence'] with regex (\\\\w+))\\nEdit Distance Concept: Spell check, search optimization\\nDemo: Calculating edit distance (Example: Edit distance between 'kitten' and 'sitting')\\nPractical Application: Text processing in chatbots\\nExample: Recognizing command patterns (e.g., regex /order \\\\d+ cups of (?:coffee|tea)/ for a chatbot)\\nConclusion: Reflect on regex utility in real-world applications\\nQ&A Segment: Explore doubts, reinforce learning\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 4,\n",
      "            \"purpose_statement\": \"This slide aims to reinforce and enhance the student's theoretical understanding of Regular Expressions through curated examples and exercises, focusing on their implementation in real-world Natural Language Processing applications.\",\n",
      "            \"concepts\": [\n",
      "                \"Regular Expressions\",\n",
      "                \"Natural Language Processing (NLP)\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Synergy of Patterns and Distance: Regular Expressions Meet Edit Distances in NLP\",\n",
      "            \"presentation\": \"We've been journeying through the fascinating world of Natural Language Processing (NLP), and today we've arrived at a very intriguing intersection \\u2013 the synergy between Regular Expressions and Edit Distance. Let's explore how they operate in concert within NLP to enhance text analysis and error correction. Regular Expressions, or regex for short, are like the Swiss Army knife for text. They allow us to specify patterns to match sequences of characters within strings. In NLP, they're incredibly versatile, aiding in tasks such as search and replace operations, text normalization, and sifting through mountains of data to fish out only what's relevant to our query. envision regex as your diligent text detective, scanning through documents and identifying anything from a simple word to complex patterns like email addresses. In fact, regex can spot an email in a document using a pattern like the one we see here, designating a boundary followed by alphanumeric characters, an @ symbol, and the domain format. This ability is indispensable in organizing and parsing out information from our digital texts. Now, let's slot in Edit Distance into our discussion. Think of it as a meticulous editor. It measures \\u2013 literally, in terms of the number of single-character edits \\u2013 how we can transform one string into another through insertions, deletions, or substitutions. This 'distance' tells us a lot about the similarity, or lack thereof, between strings, which is incredibly useful for tasks like spell checking or suggesting corrections in a text. Put regex and Edit Distance together, and you've got a dynamic duo for NLP. Regex does a stellar job at homing in on patterns, which Edit Distance takes up to assess similarity and spot differences, allowing for comparison among various text iterations. Imagine our applications, like a chatbot, where regex might be used to recognize the patterns in the input text and Edit Distance to help us offer the most appropriate suggestions or corrections based on what users intend to say. The synergy of these tools dramatically improves our systems' ability to understand and process human language. As regex helps with text normalization \\u2013 take our example, 'We're going to the zoo!', it neatly packages each word or contraction into clear, discrete tokens. On the other hand, if we look at words like 'intention' and 'execution', Edit Distance shows they're five edits apart; similar length, quite different meaning. In practice, coupling regex with Edit Distance allows systems to efficiently handle instances where users might mistype their queries, ensuring a smooth and intelligent interaction with technology. By understanding and applying these concepts, you're getting adept at not only grasping the foundational theories behind NLP but also how to apply these to real-life tasks \\u2013 like ensuring your chatbot doesn't flinch at typos. That's the power of patterns and distances in the realm of NLP.\",\n",
      "            \"content\": \"Regular Expressions (regex) are patterns used to match character combinations in strings.\\nApplication in NLP includes search and replace, text normalization, and filtering relevant information.\\nEdit Distance measures how many edits (insertions, deletions, substitutions) are needed to change one word into another.\\nSynergy in NLP: regex identifies patterns; Edit Distance measures similarity and errors by comparing text iterations.\\nExample of regex: Finding email addresses in a document with the pattern: \\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\\\\b\\nText normalization with regex: Tokenization of 'We're going to the zoo!' creates tokens: ['We're', 'going', 'to', 'the', 'zoo']\\nExample of Edit Distance: The words 'intention' and 'execution' have an Edit Distance of 5.\\nCombined application case study: Chatbot input normalization and error handling with regex (pattern matching) and Edit Distance (correction suggestions).\",\n",
      "            \"latex_codes\": \"\",\n",
      "            \"purpose\": 3,\n",
      "            \"purpose_statement\": \"To explain the synergistic relationship between Regular Expressions and Edit Distance in the context of Natural Language Processing (NLP) and demonstrate their application in text analysis, error detection, and correction.\",\n",
      "            \"concepts\": [\n",
      "                \"Regular Expressions\",\n",
      "                \"Edit Distance\",\n",
      "                \"Text Normalization\"\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"current_obj_idx\": 0,\n",
      "    \"num_slides\": 14\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "### Slide generation from AITutor\n",
    "notebank = NoteBank()\n",
    "[notebank.add_note(n) for n in current_plan.split(\"\\n\")]\n",
    "slide_planner = SlidePlanner(notebank, concept_db)\n",
    "# Check if the file exists\n",
    "if os.path.exists(f\"Research/temp_data/temp_slideplan_{current_topic}.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(f\"Research/temp_data/temp_slideplan_{current_topic}.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        slide_plans = pkl.load(f)\n",
    "        print(\"\\n\\n\".join([str(slide.format_json()) for slide in slide_plans]))\n",
    "        slide_planner.SlidePlans = slide_plans\n",
    "else:\n",
    "    slide_planner.generate_slide_plan()\n",
    "    with open(f\"Research/temp_data/temp_slideplan_{current_topic}.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(slide_planner.SlidePlans, f)\n",
    "\n",
    "if os.path.exists(f\"Research/temp_data/temp_slides_{current_topic}.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(f\"Research/temp_data/temp_slides_{current_topic}.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        slides = pkl.load(f)\n",
    "        slide_planner.Slides = slides\n",
    "else:\n",
    "    slide_planner.generate_slide_deque()\n",
    "    with open(f\"Research/temp_data/temp_slides_{current_topic}.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(slide_planner.Slides, f)\n",
    "import json\n",
    "print(json.dumps(slide_planner.format_json(), indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "- Preprocessing\n",
    "- Generation of questions from (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Introduction to Natural Language Processing (NLP) and Regular Expressions', 'purpose': 0, 'purpose_statement': 'To give the student an initial overview of NLP with a focus on Regular Expressions and how they play a foundational role in text analysis.', 'concepts': ['Natural Language Processing (NLP)', 'Regular Expressions', 'Text Normalization', 'Tokenization of Text', 'Edit Distance']}\n",
      "\n",
      "{'title': 'Decoding Patterns with Regular Expressions in NLP', 'purpose': 0, 'purpose_statement': 'This slide will serve as an initial deep dive into the world of Regular Expressions, enabling students to understand their syntax, basic operators, and fundamental uses in NLP, setting the groundwork for more sophisticated text processing tasks.', 'concepts': ['Regular Expressions']}\n",
      "\n",
      "{'title': 'Discovering Lemmatization: Enhancing Text Analysis in NLP', 'purpose': 0, 'purpose_statement': \"This slide aims to introduce Lemmatization as an essential NLP text preprocessing technique, building on the student's understanding of Text Normalization and Tokenization, to further enhance their text analysis skills.\", 'concepts': ['Lemmatization', 'Text Normalization', 'Tokenization of Text']}\n",
      "\n",
      "{'title': 'Engaging with Part-of-Speech Tagging in NLP', 'purpose': 0, 'purpose_statement': 'To introduce the foundational concept of part-of-speech tagging within Natural Language Processing (NLP), elucidating its definition, importance, and the role it plays in understanding the grammatical structure of language for text analysis and other NLP applications.', 'concepts': ['part-of-speech tagging', 'Tokenization of Text', 'Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Unveiling the Layers of Language: Syntax Parsing and Sentence Segmentation', 'purpose': 0, 'purpose_statement': \"This slide is designed to introduce the foundational concepts of 'Syntax Parsing' and 'Sentence Segmentation Techniques' in NLP, underscoring their roles in text analysis, their importance in machine understanding of language, and how they pave the way for advanced NLP tasks such as dependency parsing and named entity recognition.\", 'concepts': ['Syntax Parsing', 'Sentence Segmentation Techniques']}\n",
      "\n",
      "{'title': 'Transforming Texts: Mastering Edit Distance & Regular Expressions in NLP', 'purpose': 3, 'purpose_statement': \"This slide is tailored to solidify the student's foundational knowledge of Regular Expressions, focusing on their practical applications in text normalization and edit distance calculations in NLP. The content will bridge theoretical concepts with real-world tools to enhance the student's performance in an academic setting, and facilitate the application of these concepts through interactive examples and aligned practice problems.\", 'concepts': ['Regular Expressions', 'Edit Distance', 'Text Normalization']}\n",
      "\n",
      "{'title': 'Diving Deeper: Practical Applications of Edit Distance in NLP', 'purpose': 2, 'purpose_statement': 'This slide uncovers the significant role of Edit Distance in the field of Natural Language Processing, with a focus on real-world applications like spelling correction in digital text analysis and comparative genomics through DNA sequencing. The aim is to provide the student with an understanding of Edit Distance that bridges the gap between academic learning and practical implementation in modern computational linguistics and bioinformatics.', 'concepts': ['Edit Distance']}\n",
      "\n",
      "{'title': 'Enhancing Natural Language Processing with Tokenization Techniques', 'purpose': 2, 'purpose_statement': \"This slide aims to delve into the specifics of 'Tokenization'â€”a fundamental concept in NLP not yet fully explored in our slide set. By exploring different tokenization techniques and their pertinence in various contexts of NLP, the student will gain a deeper understanding of how text is processed at the foundational level for more complex tasks.\", 'concepts': ['Tokenization', 'Regular Expressions', 'Text Normalization']}\n",
      "\n",
      "{'title': 'Enhancing Natural Language Processing with Tokenization Techniques', 'purpose': 2, 'purpose_statement': \"This slide aims to delve into the specifics of 'Tokenization'â€”a fundamental concept in NLP not yet fully explored in our slide set. By exploring different tokenization techniques and their pertinence in various contexts of NLP, the student will gain a deeper understanding of how text is processed at the foundational level for more complex tasks.\", 'concepts': ['Tokenization', 'Regular Expressions', 'Text Normalization']}\n",
      "\n",
      "{'title': 'Navigating the Grammar of Language: part-of-speech tagging in NLP', 'purpose': 2, 'purpose_statement': 'This slide aims to deepen the studentâ€™s understanding of the part-of-speech tagging process within the broader scope of NLP, demonstrating the connection between tagging, sentence structure, and linguistic meaning, as well as its application in real-world text analysis challenges.', 'concepts': ['part-of-speech tagging', 'Natural Language Processing (NLP)', 'Tokenization of Text', 'Syntax Parsing']}\n",
      "\n",
      "{'title': 'Practical Insights into Lemmatization and Sentence Segmentation in NLP', 'purpose': 4, 'purpose_statement': \"To consolidate the student's theoretical understanding of Lemmatization and Sentence Segmentation Techniques through practical examples and exercises, demonstrating their vital roles in NLP applications such as machine translation and information retrieval systems.\", 'concepts': ['Lemmatization', 'Sentence Segmentation Techniques']}\n",
      "\n",
      "{'title': 'Advanced Utilization of Regular Expressions in NLP', 'purpose': 3, 'purpose_statement': \"To expand the student's knowledge of Regular Expressions beyond basics, covering advanced pattern matching and efficiency in text parsing, and to provide practical applications in NLP.\", 'concepts': ['Regular Expressions']}\n",
      "\n",
      "{'title': 'Mastery Through Practice: Regular Expressions in Real-World NLP Applications', 'purpose': 4, 'purpose_statement': \"This slide aims to reinforce and enhance the student's theoretical understanding of Regular Expressions through curated examples and exercises, focusing on their implementation in real-world Natural Language Processing applications.\", 'concepts': ['Regular Expressions', 'Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Synergy of Patterns and Distance: Regular Expressions Meet Edit Distances in NLP', 'purpose': 3, 'purpose_statement': 'To explain the synergistic relationship between Regular Expressions and Edit Distance in the context of Natural Language Processing (NLP) and demonstrate their application in text analysis, error detection, and correction.', 'concepts': ['Regular Expressions', 'Edit Distance', 'Text Normalization']}\n",
      "{\n",
      "    \"questions\": [\n",
      "        {\n",
      "            \"subject\": 1,\n",
      "            \"type\": 3,\n",
      "            \"data\": {\n",
      "                \"data\": \"In the context of processing tweets or social media text, write a Python function `extract_hashtags` that takes a string as an input and returns a list of hashtags in the text. You are required to use regular expressions in your solution. Consider that a hashtag is defined as a string that begins with a hash symbol (#) and is followed by alphanumeric characters without spaces.\",\n",
      "                \"boilerplate\": \"def extract_hashtags(text):\\n    # TODO: Your code here\\n    pass\",\n",
      "                \"test_cases_script\": \"assert extract_hashtags(\\\"Loving the #AI and #NLP talks at the conference!\\\") == [\\\"#AI\\\", \\\"#NLP\\\"]\\nassert extract_hashtags(\\\"This is a #great_day!\\\") == [\\\"#great_day\\\"]\\nassert extract_hashtags(\\\"Hello world!\\\") == []\",\n",
      "                \"concepts\": [\n",
      "                    \"Regular Expressions\",\n",
      "                    \"Applications of Regular Expressions in NLP\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"subject\": 1,\n",
      "            \"type\": 0,\n",
      "            \"data\": {\n",
      "                \"data\": \"Implement a function `count_vowels` in Python that takes a string input and returns the number of vowels in the string. Consider the following vowels: a, e, i, o, u, and their uppercase counterparts.\",\n",
      "                \"rubric\": \"Rubric: [1 Point] Correct implementation of the function; [1 Point] Correct count of vowels; [1 Point] Correct handling of uppercase vowels; [1 Point] Clean, readable code with appropriate comments.\",\n",
      "                \"concepts\": [\n",
      "                    \"Regular Expressions\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"subject\": 1,\n",
      "            \"type\": 3,\n",
      "            \"data\": {\n",
      "                \"data\": \"Write a Python function to calculate the Edit Distance between two strings using the Levenshtein distance algorithm. Your function should take two arguments (string1, string2) and return the minimum number of operations required to convert string1 into string2. You can limit your solution to insertions, deletions, and substitutions as the allowed edit operations.\",\n",
      "                \"boilerplate\": \"def edit_distance(str1, str2):\\n    # Your code here\\n    pass\",\n",
      "                \"test_cases_script\": \"assert edit_distance('kitten', 'sitting') == 3\\nassert edit_distance('intention', 'execution') == 5\\nassert edit_distance('algorithm', 'altruistic') == 6\",\n",
      "                \"concepts\": [\n",
      "                    \"Edit Distance\",\n",
      "                    \"Applications of Edit Distance Algorithm in NLP\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"subject\": 1,\n",
      "            \"type\": 3,\n",
      "            \"data\": {\n",
      "                \"data\": \"Write a Python function named `lemmatize_tokens` that takes a string as input and returns a list of lemmatized tokens. Your implementation should tokenize the text and then apply lemmatization to each token.\",\n",
      "                \"boilerplate\": \"def lemmatize_tokens(text):\\n    # Your code here\\n    # You may use NLTK library for tokenizing and lemmatization\\n\\n    pass  # Replace with your implementation\",\n",
      "                \"test_cases_script\": \"sample_text = \\\"The striped bats are hanging on their feet for best\\\"\\nassert lemmatize_tokens(sample_text) == [\\\"The\\\", \\\"striped\\\", \\\"bat\\\", \\\"be\\\", \\\"hanging\\\", \\\"on\\\", \\\"their\\\", \\\"foot\\\", \\\"for\\\", \\\"best\\\"], \\\"Test case failed!\\\"\",\n",
      "                \"concepts\": [\n",
      "                    \"Natural Language Processing (NLP)\",\n",
      "                    \"Tokenization of Text\",\n",
      "                    \"Lemmatization\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"subject\": 1,\n",
      "            \"type\": 3,\n",
      "            \"data\": {\n",
      "                \"data\": \"Write a Python function that uses regular expressions to count the number of times a specific word appears in a given text.\",\n",
      "                \"boilerplate\": \"import re\\n\\ndef count_word_occurrences(word, text):\\n\\t# TODO: Complete the function\\n\\tpass\",\n",
      "                \"test_cases_script\": \"text = '''Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum auctor euismod nunc, nec lacinia libero ultrices nec. Nulla vitae sagittis ipsum. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Morbi vitae neque viverra, euismod nisl in, tristique odio. Nulla facilisi. Sed vehicula lorem massa, a ultrices quam iaculis et. Donec pretium viverra accumsan. Proin in lorem velit. Integer nec interdum enim, sit amet condimentum libero. Phasellus sagittis, sapien sed dapibus imperdiet, purus metus bibendum mauris, quis congue ipsum eros quis nibh.'''\\n\\nassert count_word_occurrences('Lorem', text) == 1\\nassert count_word_occurrences('semper', text) == 0\\nassert count_word_occurrences('amet', text) == 2\",\n",
      "                \"concepts\": [\n",
      "                    \"Regular Expressions\",\n",
      "                    \"Uses of Regular Expressions in NLP\",\n",
      "                    \"Basic Syntax and Operators of Regular Expressions\"\n",
      "                ],\n",
      "                \"teaching_note\": \"Regular expressions are a powerful tool for pattern matching in text. In this question, you will use regular expressions to count the number of times a specific word appears in a given text. Remember to use the re module in Python to access the regular expression functions. You can use the re.findall() function to find all occurrences of the word in the text and then use the len() function to count the number of matches.\"\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"current_obj_idx\": -1,\n",
      "    \"num_questions\": 5\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "### Slide generation from AITutor\n",
    "notebank = NoteBank()\n",
    "[notebank.add_note(n) for n in current_plan.split(\"\\n\")]\n",
    "q_suite = QuestionSuite(5, notebank, concept_db)\n",
    "# Check if the file exists\n",
    "if os.path.exists(f\"Research/temp_data/temp_questions_{current_topic}.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(f\"Research/temp_data/temp_questions_{current_topic}.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        questions = pkl.load(f)\n",
    "        print(\"\\n\\n\".join([str(slide.format_json()) for slide in slide_plans]))\n",
    "        q_suite.Questions = questions\n",
    "else:\n",
    "    q_suite.generate_question_data()\n",
    "    with open(f\"Research/temp_data/temp_questions_{current_topic}.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(q_suite.Questions, f)\n",
    "        \n",
    "print(json.dumps(q_suite.format_json(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9867042631283537 0.9690687772047663\n"
     ]
    }
   ],
   "source": [
    "### Testing Concepts\n",
    "\n",
    "coding_concepts_aitutor = \"Research/generation_data/concept_graph/Teachabull/codingConcepts_teachabull.json\"\n",
    "with open(coding_concepts_aitutor, \"r\") as f:\n",
    "    coding_concepts_aitutor = json.load(f)\n",
    "coding_concepts_aitutor = {\"concepts\": [\n",
    "    {\"name\": concept['name']} for concept in coding_concepts_aitutor['concepts']\n",
    "]}\n",
    "\n",
    "# Normalize and create embedding\n",
    "coding_concepts_aitutor = json.dumps(coding_concepts_aitutor, indent=4)\n",
    "coding_concepts_aitutor = create_embeddings(coding_concepts_aitutor)\n",
    "# expert\n",
    "coding_concepts_expert = \"Research/generation_data/concept_graph/Expert/codingConcepts_expert.json\"\n",
    "with open(coding_concepts_expert, \"r\") as f:\n",
    "    coding_concepts_expert = json.load(f)\n",
    "coding_concepts_expert = {\"concepts\": [\n",
    "    {\"name\": concept['name']} for concept in coding_concepts_expert['concepts']\n",
    "]}\n",
    "coding_concepts_expert = json.dumps(coding_concepts_expert, indent=4)\n",
    "coding_concepts_expert = create_embeddings(coding_concepts_expert)\n",
    "\n",
    "# chatgpt\n",
    "coding_concepts_chatgpt = \"Research/generation_data/concept_graph/ChatGPT/codingConcepts_chatgpt4.json\"\n",
    "with open(coding_concepts_chatgpt, \"r\") as f:\n",
    "    coding_concepts_chatgpt = json.load(f)\n",
    "coding_concepts_chatgpt = {\"concepts\": [\n",
    "    {\"name\": concept['name']} for concept in coding_concepts_chatgpt['concepts']\n",
    "]}\n",
    "coding_concepts_chatgpt = json.dumps(coding_concepts_chatgpt, indent=4)\n",
    "coding_concepts_chatgpt = create_embeddings(coding_concepts_chatgpt)\n",
    "\n",
    "coschatgpt = cosine_similarity(coding_concepts_chatgpt[0], coding_concepts_expert[0])\n",
    "cosaitutor = cosine_similarity(coding_concepts_aitutor[0], coding_concepts_expert[0])\n",
    "print(coschatgpt, cosaitutor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9469275623592007 0.9354448040938711\n"
     ]
    }
   ],
   "source": [
    "### Testing Slides\n",
    "\n",
    "coding_slides_aitutor = \"Research/generation_data/slides/Teachabull/codingSlides_aitutor.json\"\n",
    "with open(coding_slides_aitutor, \"r\") as f:\n",
    "    coding_slides_aitutor = json.load(f)\n",
    "coding_slides_aitutor = {\"slides\": [\n",
    "    {\"title\": slide['title'],\"content\": slide['content']} for slide in coding_slides_aitutor['slides']\n",
    "]}\n",
    "\n",
    "# Normalize and create embedding\n",
    "coding_slides_aitutor = json.dumps(coding_slides_aitutor, indent=4)\n",
    "coding_slides_aitutor = create_embeddings(coding_slides_aitutor)\n",
    "# expert\n",
    "coding_slides_expert = \"Research/generation_data/slides/Expert/codingSlides_expert.json\"\n",
    "with open(coding_slides_expert, \"r\") as f:\n",
    "    coding_slides_expert = json.load(f)\n",
    "coding_slides_expert = {\"slides\": [\n",
    "    {\"title\": slide['Title'],\"content\": slide['Description']} for slide in coding_slides_expert['slides']\n",
    "]}\n",
    "coding_slides_expert = json.dumps(coding_slides_expert, indent=4)\n",
    "coding_slides_expert = create_embeddings(coding_slides_expert)\n",
    "\n",
    "# chatgpt\n",
    "coding_slides_chatgpt = \"Research/generation_data/slides/ChatGPT/codingSlides_chatgpt.json\"\n",
    "with open(coding_slides_chatgpt, \"r\") as f:\n",
    "    coding_slides_chatgpt = json.load(f)\n",
    "coding_slides_chatgpt = {\"slides\": [\n",
    "    {\"title\": slide['Title'],\"content\": slide['Description']} for slide in coding_slides_chatgpt['slides']\n",
    "]}\n",
    "coding_slides_chatgpt = json.dumps(coding_slides_chatgpt, indent=4)\n",
    "coding_slides_chatgpt = create_embeddings(coding_slides_chatgpt)\n",
    "\n",
    "coschatgpt = cosine_similarity(coding_slides_chatgpt[0], coding_slides_expert[0])\n",
    "cosaitutor = cosine_similarity(coding_slides_aitutor[0], coding_slides_expert[0])\n",
    "print(coschatgpt, cosaitutor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8775049669925067 0.8033181749423033\n"
     ]
    }
   ],
   "source": [
    "### Testing Questions\n",
    "\n",
    "coding_questions_aitutor = \"Research/generation_data/questions/Teachabull/codingQuestions_aitutor.json\"\n",
    "with open(coding_questions_aitutor, \"r\") as f:\n",
    "    coding_questions_aitutor = json.load(f)\n",
    "s = \"\"\n",
    "for i, question in enumerate(coding_questions_aitutor[\"questions\"]):\n",
    "    s+=f\"{i}.\\n\"\n",
    "    for k, v in question[\"data\"].items():\n",
    "        if isinstance(v, str):\n",
    "            s+=v+\"\\n\"\n",
    "    s+=\"\\n\"\n",
    "coding_questions_aitutor = s\n",
    "\n",
    "# Normalize and create embedding\n",
    "coding_questions_aitutor = create_embeddings(coding_questions_aitutor)\n",
    "\n",
    "# expert\n",
    "coding_questions_expert = \"Research/generation_data/questions/Expert/codingQuestions_expert_RAW.txt\"\n",
    "with open(coding_questions_expert, \"r\") as f:\n",
    "    coding_questions_expert = f.read()\n",
    "\n",
    "coding_questions_expert = create_embeddings(coding_questions_expert)\n",
    "\n",
    "# chatgpt\n",
    "coding_questions_chatgpt = \"Research/generation_data/questions/ChatGPT/codingQuestions_chatgpt_RAW.txt\"\n",
    "with open(coding_questions_chatgpt, \"r\") as f:\n",
    "    coding_questions_chatgpt = f.read()\n",
    "\n",
    "coding_questions_chatgpt = create_embeddings(coding_questions_chatgpt)\n",
    "\n",
    "coschatgpt = cosine_similarity(coding_questions_chatgpt[0], coding_questions_expert[0])\n",
    "cosaitutor = cosine_similarity(coding_questions_aitutor[0], coding_questions_expert[0])\n",
    "print(coschatgpt, cosaitutor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERRORS\n",
    "\n",
    "\n",
    "the thing we are checking for errors is number of api calls per errors. api calls during translation / errors during translation\n",
    "gpt-4 and gpt-3.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCEPTS RATIO OF NUMBER OF RELEVANT CONCEPTS OVER NUMBER OF CONCEPS\n",
    "GPT-3.5 vs GPT-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
