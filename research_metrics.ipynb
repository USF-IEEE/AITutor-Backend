{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricizing LLMaAiTB-E\n",
    "\n",
    "- Our Focus: Generation Quality\n",
    "- Measurement Techniques: \n",
    "    - Vector Comparison\n",
    "    - Human Preference Sample (A/B/C Testing)\n",
    "    - ???\n",
    "- Iterative documents to measure:\n",
    "    - Concepts (Generation phase)\n",
    "    - Slides (Teaching phase)\n",
    "    - Questions (Testing phase)\n",
    "- Resources for testing:\n",
    "    - Expert (From classes)\n",
    "    - GPT4 (Generation)\n",
    "    - LLMaAiTB-E (Teachabull)\n",
    "- Main concepts to cover:\n",
    "    - Object Oriented Programming\n",
    "    - Programming Language Semantics\n",
    "    - Math\n",
    "    - History\n",
    "    - \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Helper Functions\n",
    "We will demonstrate our metrics using OpenAI's Vector Embeddings on our generated documents. We decided to use OpenAI's embeddings due to their large document size capacity. We agreed that this method would prove to be the best while comparing large documents.\n",
    "\n",
    "## LLM Prompt/Text Completion\n",
    "\n",
    "\n",
    "## Vector Comparison\n",
    "Embeddings: OpenAI’s text embeddings measure the relatedness of text strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from AITutor_Backend.src.TutorUtils.concepts import *\n",
    "from AITutor_Backend.src.TutorUtils.notebank import NoteBank\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPENAI HELPER FUNCTIONS \n",
    "def request_output_from_llm(prompt, model: str):\n",
    "    \"\"\"Requests the Concept information from an LLM.\n",
    "\n",
    "    Args:\n",
    "        prompt: (str) - string to get passed to the model\n",
    "        model: (str) - \n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI() \n",
    "    \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "    {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": prompt,\n",
    "    },\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=8000,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slides: \n",
    "- Preprocessing\n",
    "- Generation of Document for (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SLIDE OBJ PROMPTs\n",
    "prompt = ''' #Your task is to create a JSON object from a slide string. View the example Input and output, and then repeat the same for the provided input. \n",
    "Perform the conversion for each slide s in the input string such that s->json_object(s). You should be able to figure out which is the title and which is the description.\n",
    "IMPORTANT: Escape Characters in JSON Data can cause errors if the JSON Object or JSON data contains backslashes, which means they need to be properly escaped\n",
    "Avoid these errors: Invalid \\escape: line 24 column 72 (char 2199)\n",
    "By properly escaping your backslashes ('\\\\')\n",
    "IMPORTANT: If there is two words together, such as \"functionwhere\", without being separated with a white space, that most probably means that there is a new line ('\\n') or space (' ') in between them, e.g. \"function where\".\n",
    "\n",
    "// Input:\n",
    "Page 1 Content:\n",
    "Natural Language ProcessingProfessor John LicatoUniversity of South FloridaChapter 2:RegEx, Edit Distance\n",
    "\n",
    "----------------------------------------\n",
    "Page 2 Content:\n",
    "\"Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When you’re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.\"Regular Expressions\n",
    "----------------------------------------\n",
    "Page 3 Content:\n",
    "The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))Regular Expressions\n",
    "----------------------------------------\n",
    "Page 4 Content:\n",
    "The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')Regular Expressions\n",
    "----------------------------------------\n",
    "Page 5 Content:\n",
    "Creating regex objectsr’ = raw string\\d – placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)\n",
    "----------------------------------------\n",
    "Page 6 Content:\n",
    "Matching regex objects\n",
    "mo = match object – contains the result of our search>>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)>>> mo = phoneNumRegex.search(‘My number is 415-555-4242.’)>>> print(‘Phone number found: ’ + mo.group())Phone number found: 415-555-4242\n",
    "----------------------------------------\n",
    "Page 7 Content:\n",
    "Text Normalization•We will work a lot with large datasets / corpora•We often need to pre-process text•Tokenizing (segmenting) words•Normalizing word formats•Segmenting sentences (e.g. by using punctuation)\n",
    "----------------------------------------\n",
    "Page 8 Content:\n",
    "Tokenization – segmenting running text into words (or word-like units)>>> text = 'That U.S.A. poster-print costs $12.40...'>>> pattern = r\\'\\'\\', (?x)  # set flag to allow verbose regexps...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A....     | \\w+(-\\w+)*      # words with optional internal hyphens...     | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%...     | \\.\\.\\.          # ellipsis...     | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [... \\'\\'\\'>>> nltk.regexp_tokenize(text, pattern)['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n",
    "----------------------------------------\n",
    "Page 9 Content:\n",
    "Subword tokenization•How do we capture relations between words like:–new, newer–blow, blowing–precipitation, precipitate•Often useful to break tokens into *sub*words•Usually split into token learners, and token segmenters\n",
    "----------------------------------------\n",
    "Page 10 Content:\n",
    "Byte-pair encoding (BPE)•A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab VV <- all unique characters in C                  # initial set of tokens is charactersfor i = 1 to k do                                # merge tokens til k times    t_L, t_R <- Most frequent pair of adjacent tokens in C    t_new <- t_L + t_R                           # make new token by concatenating    V <- V + t_new                               # update the vocabulary    Replace each occurrence of t_L, t_R in C with t_new # and update the corpusreturn Vcorpus5 low_2 lowest_6 newer_3 wider_2 new_vocabulary_, d, e, i, l, n, o, r, s, t, wcorpus5 low _2 lowest _6 newer _3 wider _2 new _vocabulary_, d, e, i, l, n, o, r, s, t, w, er\n",
    "----------------------------------------\n",
    "...\n",
    "        \n",
    "// Output:\n",
    "        { \n",
    "                \\\"slides\\\":[\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Natural Language Processing\\\", \n",
    "                                \\\"Description\\\": \\\"Professor John Licato University of South Florida Chapter 2:RegEx, Edit Distance\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When you’re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Creating regex objects\\\", \n",
    "                                \\\"Description\\\": \\\"r’ = raw string\\d – placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Matching regex objects\\\", \n",
    "                                \\\"Description\\\": \\\">>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)>>> mo = phoneNumRegex.search(‘My number is 415-555-4242.’)>>> print(‘Phone number found: ’ + mo.group())Phone number found: 415-555-4242 mo = match object – contains the result of our search\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Text Normalization\\\", \n",
    "                                \\\"Description\\\": \\\"•We will work a lot with large datasets / corpora\\n•We often need to pre-process text\\n•Tokenizing (segmenting) words\\n•Normalizing word formats\\n•Segmenting sentences (e.g. by using punctuation) )\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Tokenization – segmenting running text into words (or word-like units)\\\", \n",
    "                                \\\"Description\\\": \\\">>> text = 'That U.S.A. poster-print costs $12.40...'\\n>>> pattern = r\\'\\'\\', (?x)  # set flag to allow verbose regexps\\n...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A\\n....     | \\w+(-\\w+)*      # words with optional internal hyphens\\n...     | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\\n...     | \\.\\.\\.          # ellipsis...     | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [\\n... \\'\\'\\'\\n>>> nltk.regexp_tokenize(text, pattern)\\n['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Subword tokenization\\\", \n",
    "                                \\\"Description\\\": \\\"•How do we capture relations between words like:\\n–new, newer\\n–blow, blowing\\n–precipitation, precipitate\\n•Often useful to break tokens into *sub*words•Usually split into token learners, and token segmenters\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Byte-pair encoding (BPE)\\\", \n",
    "                                \\\"Description\\\": \\\"•A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab V\\nV <- all unique characters in C                  # initial set of tokens is characters\\nfor i = 1 to k do                                # merge tokens til k times    \\nt_L, t_R <- Most frequent pair of adjacent tokens in C    \\nt_new <- t_L + t_R                           # make new token by concatenating    \\nV <- V + t_new                               # update the vocabulary    \\nReplace each occurrence of t_L, t_R in C with t_new # and update the corpus\\nreturn V\\ncorpus\\n5 low_\\n2 lowest_\\n6 newer_\\n3 wider_\\n2 new_\\nvocabulary\\n_, d, e, i, l, n, o, r, s, t, w\\ncorpus\\n5 low _\\n2 lowest _\\n6 newer _\\n3 wider _\\n2 new _\\nvocabulary\\n_, d, e, i, l, n, o, r, s, t, w, er\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        ...\n",
    "                ]\n",
    "        }\n",
    "Remember! Escape Characters in JSON Data: If the JSON Object or JSON data contains backslashes, they need to be properly escaped.\n",
    "Avoid these errors: Invalid \\escape: line 24 column 72 (char 2199)\n",
    "\n",
    "// Input:\n",
    "        $SLIDE$\n",
    "\n",
    "// Output:\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Slide helper functions\n",
    "import PyPDF2\n",
    "from pptx import Presentation\n",
    "import json\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"Reads a PDF file and prints the content of each page\"\"\"\n",
    "    slide_str = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "\n",
    "        for i in range(num_pages):\n",
    "            page = reader.pages[i]\n",
    "            text = page.extract_text()\n",
    "            slide_str += f\"Page {i+1} Content:\\n{text}\"\n",
    "            slide_str += \"\\n\" + (\"-\" * 40) + \"\\n\"\n",
    "    return slide_str\n",
    "\n",
    "def extract_text_from_slide(slide):\n",
    "    \"\"\"Extracts title and content from a slide\"\"\"\n",
    "    title = slide.shapes.title.text if slide.shapes.title else \"No Title\"\n",
    "    content = []\n",
    "\n",
    "    for shape in slide.shapes:\n",
    "        if hasattr(shape, \"text\"):\n",
    "            content.append(shape.text)\n",
    "\n",
    "    return title, content\n",
    "\n",
    "def read_pptx(file_path):\n",
    "    \"\"\"Reads a pptx file and prints the title and content of each slide\"\"\"\n",
    "    prs = Presentation(file_path)\n",
    "\n",
    "    for slide in prs.slides:\n",
    "        title, content = extract_text_from_slide(slide)\n",
    "        print(f\"Title: {title}\")\n",
    "        print(\"Content:\", \"\\n\".join(content))\n",
    "        print(\"-\" * 40)\n",
    "def get_slide_prompt(slide_template, data):\n",
    "    return slide_template.replace(\"$SLIDE$\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'slides': [{'Title': 'Natural Language Processing', 'Description': 'Professor John Licato University of South Florida Chapter 2:RegEx, Edit Distance', 'Latex': []}, {'Title': 'Regular Expressions', 'Description': '\"Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When you’re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.\"', 'Latex': []}, {'Title': 'Regular Expressions', 'Description': \"The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))\", 'Latex': []}, {'Title': 'Regular Expressions', 'Description': \"The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')\", 'Latex': []}, {'Title': 'Creating regex objects', 'Description': 'r’ = raw string\\\\d – placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(r’\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d\\\\d’)', 'Latex': []}, {'Title': 'Matching regex objects', 'Description': '>>> import re>>> phoneNumRegex = re.compile(r’\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d\\\\d’)>>> mo = phoneNumRegex.search(‘My number is 415-555-4242.’)>>> print(‘Phone number found: ’ + mo.group())Phone number found: 415-555-4242', 'Latex': []}, {'Title': 'Text Normalization', 'Description': '•We will work a lot with large datasets / corpora\\n•We often need to pre-process text\\n•Tokenizing (segmenting) words\\n•Normalizing word formats\\n•Segmenting sentences (e.g. by using punctuation)', 'Latex': []}, {'Title': 'Tokenization – segmenting running text into words (or word-like units)', 'Description': '>>> text = \\'That U.S.A. poster-print costs $12.40...\\'>>> pattern = r\\'\\'\\', (?x)  # set flag to allow verbose regexps\\n...     ([A-Z]\\\\.)+        # abbreviations, e.g. U.S.A....     | \\\\w+(-\\\\w+)*      # words with optional internal hyphens...     | \\\\$?\\\\d+(\\\\.\\\\d+)?%?  # currency and percentages, e.g. $12.40, 82%...     | \\\\.\\\\.\\\\.          # ellipsis...     | [][.,;\"\\'?():-_`]  # these are separate tokens; includes ], [... \\'\\'\\'>>> nltk.regexp_tokenize(text, pattern)[\\'That\\', \\'U.S.A.\\', \\'poster-print\\', \\'costs\\', \\'$12.40\\', \\'...\\']', 'Latex': []}, {'Title': 'Subword tokenization', 'Description': '•How do we capture relations between words like:\\n–new, newer\\n–blow, blowing\\n–precipitation, precipitate\\n•Often useful to break tokens into *sub*words•Usually split into token learners, and token segmenters', 'Latex': []}, {'Title': 'Byte-pair encoding (BPE)', 'Description': '•A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab VV <- all unique characters in C                  # initial set of tokens is charactersfor i = 1 to k do                                # merge tokens til k times    t_L, t_R <- Most frequent pair of adjacent tokens in C    t_new <- t_L + t_R                           # make new token by concatenating    V <- V + t_new                               # update the vocabulary    Replace each occurrence of t_L, t_R in C with t_new # and update the corpusreturn Vcorpus5 low_2 lowest_6 newer_3 wider_2 new_vocabulary_, d, e, i, l, n, o, r, s, t, wcorpus5 low _2 lowest _6 newer _3 wider _2 new _vocabulary_, d, e, i, l, n, o, r, s, t, w, er', 'Latex': []}, {'Title': 'Word normalization', 'Description': '•Case folding – e.g., making everything lowercase•Lemmatization – folding lemmas together if they have the same root (dinner / dinners, am / are / is, etc.). •Stemming – performing lemmatization by removing all but the roots of words (running / runner -> run)', 'Latex': []}, {'Title': 'Minimum Edit Distance', 'Description': 'Definition of Minimum Edit Distance', 'Latex': []}, {'Title': 'How similar are two strings?', 'Description': '•Spell correction–The user typed “graffe”Which is closest? •graf•graft•grail•giraffe•Computational Biology•Align two sequences of nucleotides•Resulting alignment:•Also for Machine Translation, Information Extraction, Speech RecognitionAGGCTATCACCTGACCTCCAGGCCGATGCCCTAGCTATCACGACCGCGGTCGATTTGCCCGAC-AGGCTATCACCTGACCTCCAGGCCGA--TGCCC---TAG-CTATCAC--GACCGC--GGTCGATTTGCCCGAC', 'Latex': []}, {'Title': 'Edit Distance', 'Description': '•The minimum edit distance between two strings•Is the minimum number of editing operations–Insertion–Deletion–Substitution•Needed to transform one into the other', 'Latex': []}, {'Title': 'Minimum Edit Distance', 'Description': '•Two strings and their alignment:INTENTION| | | | | | |*EXECUTION', 'Latex': []}, {'Title': 'Minimum Edit Distance', 'Description': '•If each operation has cost of 1–Distance between these is 5•If substitutions cost 2 (Levenshtein)–Distance between them is 8INTENTION| | | | | | |*EXECUTIONd s s   is s', 'Latex': []}, {'Title': 'Alignment in Computational Biology', 'Description': '•Given a sequence of bases•An alignment:•Given two sequences, align each letter to a letter or gap-AGGCTATCACCTGACCTCCAGGCCGA--TGCCC---TAG-CTATCAC--GACCGC--GGTCGATTTGCCCGACAGGCTATCACCTGACCTCCAGGCCGATGCCCTAGCTATCACGACCGCGGTCGATTTGCCCGAC', 'Latex': []}, {'Title': 'Other uses of Edit Distance in NLP', 'Description': '•Evaluating Machine Translation and speech recognitionR Spokesman confirms    senior government adviser was shotH Spokesman said    the senior            adviser was shot dead              S      I              D                        I•Named Entity Extraction and Entity Coreference–IBM Inc. announced today–IBM profits–Stanford President John Hennessy announced yesterday–for Stanford University President John Hennessy', 'Latex': []}, {'Title': 'How to find the Min Edit Distance?', 'Description': '•Searching for a path (sequence of edits) from the start string to the final string:–Initial state: the word we’re transforming–Operators: insert, delete, substitute–Goal state:  the word we’re trying to get to–Path cost: what we want to minimize: the number of edits37', 'Latex': []}, {'Title': 'Minimum Edit as Search', 'Description': '•But the space of all edit sequences is huge!–We can’t afford to navigate naïvely–Lots of distinct paths wind up at the same state.•We don’t have to keep track of all of them•Just the shortest path to each of those revisted states.38', 'Latex': []}, {'Title': 'Defining Min Edit Distance', 'Description': '•For two strings–X of length n –Y of length m•We define D(i,j)–the edit distance between X[1..i] and Y[1..j] •i.e., the first i characters of X and the first j characters of Y–The edit distance between X and Y is thus D(n,m)', 'Latex': []}, {'Title': 'Dynamic Programming for Minimum Edit Distance', 'Description': '•Dynamic programming: A tabular computation of D(n,m)•Solving problems by combining solutions to subproblems.•Bottom-up–We compute D(i,j) for small i,j –And compute larger D(i,j) based on previously computed smaller values–i.e., compute D(i,j) for all i (0 < i < n)  and j (0 < j < m)', 'Latex': []}, {'Title': 'Defining Min Edit Distance (Levenshtein)', 'Description': '•InitializationD(i,0) = iD(0,j) = j•Recurrence Relation:For each  i = 1…M   For each  j = 1…N                       D(i-1,j) + 1          D(i,j)= min  D(i,j-1) + 1                       D(i-1,j-1) +   2; if X(i) ≠ Y(j)                                         0; if X(i) = Y(j)•Termination:D(N,M) is distance ', 'Latex': []}, {'Title': 'The Edit Distance Table', 'Description': 'The minimum edit distance between two strings is the minimum number of editing operations needed to transform one string into the other. The typical operations allowed are:1.Insertion (Ins): Add one character to the string.2.Deletion (Del): Remove one character from the string.3.Substitution (Sub): Replace one character with another.', 'Latex': []}, {'Title': 'Computing alignments', 'Description': '•Edit distance isn’t sufficient–We often need to align each character of the two strings to each other•We do this by keeping a “backtrace”•Every time we enter a cell, remember where we came from•When we reach the end, –Trace back the path from the upper right corner to read off the alignment', 'Latex': []}, {'Title': 'MinEdit with Backtrace', 'Description': '', 'Latex': []}, {'Title': 'Adding Backtrace to Minimum Edit Distance', 'Description': '•Base conditions:                                                        Termination:D(i,0) = i         D(0,j) = j         D(N,M) is distance •Recurrence Relation:For each  i = 1…M  For each  j = 1…N                      D(i-1,j) + 1         D(i,j)= min  D(i,j-1) + 1                      D(i-1,j-1) +  2; if X(i) ≠ Y(j)                                       0; if X(i) = Y(j)                     LEFT         ptr(i,j)=   DOWN                     DIAGinsertiondeletionsubstitutioninsertiondeletionsubstitution', 'Latex': []}, {'Title': 'The Distance Matrix', 'Description': 'Slide adapted from Serafim Batzoglouy0 ………………………………  yMx0 ……………………  xNEvery non-decreasing path from (0,0) to (M, N) corresponds to an alignment of the two sequencesAn optimal alignment is composed of optimal subalignments', 'Latex': []}, {'Title': 'Result of Backtrace', 'Description': 'Two strings and their alignment:INTENTION| | | | | | |*EXECUTION', 'Latex': []}, {'Title': 'Performance', 'Description': 'Time:    O(nm)Space:    O(nm)Backtrace    O(n+m)', 'Latex': []}, {'Title': 'Hearst Patterns for Hypernymy', 'Description': '•Hyponym – “Is-A” relationship•Hypernym – Opposite of hypernym•Color is a hypernym of red; cat is a hypernym of white cat. •A rule-based way of detecting hypernym relationships in text is through Hearst patterns', 'Latex': []}, {'Title': 'Some Hearst Patterns', 'Description': '•All bolded symbols (a, b, …) are noun phrases•Type 1 - Extract (b,a)–“a is b”–“a is a type of b”–“a is a kind of b”–“a was b”–“a was a type of b”–“a was a kind of b”–“a are b”–“a are a type of b”–“a are a kind of b”•Type 2 - Extract (a,b), (a,c), …, (a,d)–“a, including b”–“a, including b, c, …, and d”–“a, including b, c, …, or d”–“a, such as b”–“a, such as b, c, …, and d”–“a, such as b, c, …, or d”•There are many others!', 'Latex': []}]}\n"
     ]
    }
   ],
   "source": [
    "### TEST SLIDE OBJ GEN FROM GPT FOR EXPERT\n",
    "slide_str = read_pdf('Research/generation_data/slides/Expert/codingSlides_expert_RAW.pdf')\n",
    "\n",
    "\n",
    "curr_prompt = get_slide_prompt(prompt, slide_str)\n",
    "try:\n",
    "    json_data = request_output_from_llm(prompt=curr_prompt, model=\"gpt-3.5-turbo-16k\")\n",
    "    slide_obj = json.loads(json_data)\n",
    "    print(slide_obj)\n",
    "\n",
    "    # Convert the dictionary to a JSON-formatted string\n",
    "    json_str = json.dumps(slide_obj, indent=4)  # indent for pretty-printing\n",
    "\n",
    "    # Write the JSON string to a file\n",
    "    with open(\"Research/generation_data/slides/Expert/codingSlides_expert.json\", \"w\") as f:\n",
    "        f.write(json_str)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts\n",
    "- Preprocessing\n",
    "- Generation of Graph for (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTEBANK from AI Tutor\n",
    "tutor_plan = '''Main Concept: Regular Expressions, Text Normalization, Edit Distance in Natural Language Processing (NLP)\n",
    "Student is a computer science student with no prior knowledge of the topic, requiring an introductory lesson.\n",
    "Student is taking an NLP class, suggesting the lessons are for academic purposes and should cover necessary conceptual detail.\n",
    "Student provided a chapter summary that includes key subtopics; this will be a guide in structuring the lesson plan.\n",
    "Tutor shall educate on the following concepts:\n",
    "Subconcept: Introduction to Regular Expressions\n",
    "Subconcept: Uses of Regular Expressions in NLP\n",
    "Subconcept: Basic Syntax and Operators of Regular Expressions\n",
    "Subconcept: Practical Examples and Exercises Using Regular Expressions\n",
    "Subconcept: Introduction to Text Normalization\n",
    "Subconcept: Tokenization of Text\n",
    "Subconcept: Lemmatization and its Importance\n",
    "Subconcept: Sentence Segmentation Techniques\n",
    "Subconcept: Introduction to Edit Distance\n",
    "Subconcept: Applications of Edit Distance Algorithm in NLP\n",
    "Subconcept: Calculation of Edit Distance and String Alignment\n",
    "Tutor will apply practical examples relevant to modern NLP applications, such as chatbots, using the chapter summary as a conversational context.\n",
    "Tutor will provide hands-on practice problems and ensure the student understands the implementation of the concepts.\n",
    "Student's objective: To gain a foundational understanding of the chapter's main points, to apply this understanding in an academic setting, and to perform well in the NLP class.\n",
    "Since the student might need to have a deep understanding of the class material, the lesson should provide a solid theoretical basis, followed by practical application.\n",
    "Concept: Regular Expressions, Text Normalization, Edit Distance in Natural Language Processing (NLP)\n",
    "Concept: Introduction to Regular Expressions\n",
    "Concept: Uses of Regular Expressions in NLP\n",
    "Concept: Basic Syntax and Operators of Regular Expressions\n",
    "Concept: Practical Examples and Exercises Using Regular Expressions\n",
    "Concept: Introduction to Text Normalization\n",
    "Concept: Tokenization of Text\n",
    "Concept: Lemmatization and its Importance\n",
    "Concept: Sentence Segmentation Techniques\n",
    "Concept: Introduction to Edit Distance\n",
    "Concept: Applications of Edit Distance Algorithm in NLP\n",
    "Concept: Calculation of Edit Distance and String Alignment\n",
    "Concept: Practical Examples and Exercises in Modern NLP Applications (e.g., Chatbots)\n",
    "Concept: Hands-on Practice Problems\n",
    "Concept: Foundational Understanding of Main Points\n",
    "Concept: Academic Application of Concepts\n",
    "Concept: Theoretical Basis Followed by Practical Application\n",
    "Student's Interest Statement: I find natural language processing interesting and important since I am taking it as a course in college where I will be tested\n",
    "Student's Slides Preference Statement: I want to be taught by information and examples\n",
    "Student's Questions Preference Statement: 2 of multiple choice, 2 of free response and 2 coding questions'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m         concept_db\u001b[39m.\u001b[39mConcepts \u001b[39m=\u001b[39m concept\n\u001b[1;32m     13\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     concept_db \u001b[39m=\u001b[39m ConceptDatabase(\u001b[39m\"\u001b[39;49m\u001b[39mNatural Language Processing: Regular Expressions, Text Normalization, Edit Distance\u001b[39;49m\u001b[39m\"\u001b[39;49m,notebank\u001b[39m.\u001b[39;49menv_string())\n\u001b[1;32m     15\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m./temp_concepts.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:  \u001b[39m# 'wb' mode is for writing in binary format\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         pkl\u001b[39m.\u001b[39mdump(concept_db\u001b[39m.\u001b[39mConcepts, f)\n",
      "File \u001b[0;32m~/Documents/IEEE-Teach-A-Bull/AITutor-Backend/AITutor_Backend/src/TutorUtils/concepts.py:77\u001b[0m, in \u001b[0;36mConceptDatabase.__init__\u001b[0;34m(self, main_concept, tutor_plan, generation)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mConcepts \u001b[39m=\u001b[39m []\n\u001b[1;32m     76\u001b[0m \u001b[39mif\u001b[39;00m generation:\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_concept(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmain_concept,)\n",
      "File \u001b[0;32m~/Documents/IEEE-Teach-A-Bull/AITutor-Backend/AITutor_Backend/src/TutorUtils/concepts.py:101\u001b[0m, in \u001b[0;36mConceptDatabase.generate_concept\u001b[0;34m(self, concept_name, max_depth)\u001b[0m\n\u001b[1;32m     99\u001b[0m error_msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThere is no current error detected in the parsing system.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconcept_llm_api\u001b[39m.\u001b[39;49mrequest_concept_data_from_llm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmain_concept, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_concept_list_str(), concept_name, error_msg)\n\u001b[1;32m    102\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m         regex_match \u001b[39m=\u001b[39m ConceptDatabase\u001b[39m.\u001b[39m__CONCEPT_REGEX\u001b[39m.\u001b[39mfindall(llm_output)\n",
      "File \u001b[0;32m~/Documents/IEEE-Teach-A-Bull/AITutor-Backend/AITutor_Backend/src/TutorUtils/concepts.py:52\u001b[0m, in \u001b[0;36mConceptDatabase.ConceptLLMAPI.request_concept_data_from_llm\u001b[0;34m(self, env_main_concept, env_concept_list, concept_name, error_msg)\u001b[0m\n\u001b[1;32m     50\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgpt-4-1106-preview\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m     \u001b[39m# model = \"gpt-3.5-turbo-16k\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     53\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     54\u001b[0m         messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     55\u001b[0m             {\n\u001b[1;32m     56\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     57\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt,\n\u001b[1;32m     58\u001b[0m             },\n\u001b[1;32m     59\u001b[0m         ],\n\u001b[1;32m     60\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m,\n\u001b[1;32m     61\u001b[0m         max_tokens\u001b[39m=\u001b[39;49m\u001b[39m3000\u001b[39;49m,\n\u001b[1;32m     62\u001b[0m         top_p\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m,\n\u001b[1;32m     63\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     64\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     65\u001b[0m     )\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n\u001b[1;32m     67\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/openai/_utils/_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 299\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/openai/resources/chat/completions.py:594\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    549\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    550\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    593\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 594\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    595\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    596\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    597\u001b[0m             {\n\u001b[1;32m    598\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    599\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    600\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    601\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    602\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    603\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    604\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    605\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    606\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    607\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    608\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    609\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    615\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    616\u001b[0m             },\n\u001b[1;32m    617\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    618\u001b[0m         ),\n\u001b[1;32m    619\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    620\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    621\u001b[0m         ),\n\u001b[1;32m    622\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    623\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    624\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    625\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/openai/_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1042\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1043\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1051\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1052\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1053\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1054\u001b[0m     )\n\u001b[0;32m-> 1055\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/openai/_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    826\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    833\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    835\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    836\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    837\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    838\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    839\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    840\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/openai/_base_client.py:858\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_request(request)\n\u001b[1;32m    857\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 858\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(request, auth\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcustom_auth, stream\u001b[39m=\u001b[39;49mstream)\n\u001b[1;32m    859\u001b[0m     log\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    860\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mHTTP Request: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m, request\u001b[39m.\u001b[39mmethod, request\u001b[39m.\u001b[39murl, response\u001b[39m.\u001b[39mstatus_code, response\u001b[39m.\u001b[39mreason_phrase\n\u001b[1;32m    861\u001b[0m     )\n\u001b[1;32m    862\u001b[0m     response\u001b[39m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/httpx/_client.py:901\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    893\u001b[0m follow_redirects \u001b[39m=\u001b[39m (\n\u001b[1;32m    894\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfollow_redirects\n\u001b[1;32m    895\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    896\u001b[0m     \u001b[39melse\u001b[39;00m follow_redirects\n\u001b[1;32m    897\u001b[0m )\n\u001b[1;32m    899\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 901\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[1;32m    902\u001b[0m     request,\n\u001b[1;32m    903\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[1;32m    904\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    905\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m    906\u001b[0m )\n\u001b[1;32m    907\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/httpx/_client.py:929\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    926\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(auth_flow)\n\u001b[1;32m    928\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 929\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[1;32m    930\u001b[0m         request,\n\u001b[1;32m    931\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    932\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[1;32m    933\u001b[0m     )\n\u001b[1;32m    934\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/httpx/_client.py:966\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mrequest\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    964\u001b[0m     hook(request)\n\u001b[0;32m--> 966\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[1;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/httpx/_client.py:1002\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    998\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1001\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1002\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m   1004\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1006\u001b[0m response\u001b[39m.\u001b[39mrequest \u001b[39m=\u001b[39m request\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/httpx/_transports/default.py:228\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    216\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    217\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    226\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 228\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[1;32m    230\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    233\u001b[0m     status_code\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mstatus,\n\u001b[1;32m    234\u001b[0m     headers\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    235\u001b[0m     stream\u001b[39m=\u001b[39mResponseStream(resp\u001b[39m.\u001b[39mstream),\n\u001b[1;32m    236\u001b[0m     extensions\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    237\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[39mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[39m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[39m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[39m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[39m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[39m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/httpcore/_sync/connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connect(request)\n\u001b[1;32m     78\u001b[0m         ssl_object \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39mget_extra_info(\u001b[39m\"\u001b[39m\u001b[39mssl_object\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m         http2_negotiated \u001b[39m=\u001b[39m (\n\u001b[1;32m     80\u001b[0m             ssl_object \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     81\u001b[0m             \u001b[39mand\u001b[39;00m ssl_object\u001b[39m.\u001b[39mselected_alpn_protocol() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mh2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/httpcore/_sync/connection.py:124\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    117\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhost\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_origin\u001b[39m.\u001b[39mhost\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    118\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mport\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_origin\u001b[39m.\u001b[39mport,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msocket_options\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_socket_options,\n\u001b[1;32m    122\u001b[0m     }\n\u001b[1;32m    123\u001b[0m     \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mconnect_tcp\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs) \u001b[39mas\u001b[39;00m trace:\n\u001b[0;32m--> 124\u001b[0m         stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_network_backend\u001b[39m.\u001b[39;49mconnect_tcp(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    125\u001b[0m         trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m stream\n\u001b[1;32m    126\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/site-packages/httpcore/_backends/sync.py:206\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    200\u001b[0m exc_map: ExceptionMapping \u001b[39m=\u001b[39m {\n\u001b[1;32m    201\u001b[0m     socket\u001b[39m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    202\u001b[0m     \u001b[39mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    203\u001b[0m }\n\u001b[1;32m    205\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m--> 206\u001b[0m     sock \u001b[39m=\u001b[39m socket\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    207\u001b[0m         address,\n\u001b[1;32m    208\u001b[0m         timeout,\n\u001b[1;32m    209\u001b[0m         source_address\u001b[39m=\u001b[39;49msource_address,\n\u001b[1;32m    210\u001b[0m     )\n\u001b[1;32m    211\u001b[0m     \u001b[39mfor\u001b[39;00m option \u001b[39min\u001b[39;00m socket_options:\n\u001b[1;32m    212\u001b[0m         sock\u001b[39m.\u001b[39msetsockopt(\u001b[39m*\u001b[39moption)  \u001b[39m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/socket.py:787\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    785\u001b[0m host, port \u001b[39m=\u001b[39m address\n\u001b[1;32m    786\u001b[0m err \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m getaddrinfo(host, port, \u001b[39m0\u001b[39;49m, SOCK_STREAM):\n\u001b[1;32m    788\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n\u001b[1;32m    789\u001b[0m     sock \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.8/socket.py:918\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[39m# We override this function since we want to translate the numeric family\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[39m# and socket type values to enum constants.\u001b[39;00m\n\u001b[1;32m    917\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 918\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[1;32m    919\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n\u001b[1;32m    920\u001b[0m     addrlist\u001b[39m.\u001b[39mappend((_intenum_converter(af, AddressFamily),\n\u001b[1;32m    921\u001b[0m                      _intenum_converter(socktype, SocketKind),\n\u001b[1;32m    922\u001b[0m                      proto, canonname, sa))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Manual Testing:\n",
    "import pickle as pkl\n",
    "notebank = NoteBank()\n",
    "[notebank.add_note(n) for n in tutor_plan.split(\"\\n\")]\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"./temp_concepts.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(\"./temp_concepts.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        concept = pkl.load(f)\n",
    "        concept_db = ConceptDatabase(\"Natural Language Processing: Regular Expressions, Text Normalization, Edit Distance\",notebank.env_string(), False)\n",
    "        concept_db.Concepts = concept\n",
    "else:\n",
    "    concept_db = ConceptDatabase(\"Natural Language Processing: Regular Expressions, Text Normalization, Edit Distance\",notebank.env_string())\n",
    "    with open(\"./temp_concepts.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(concept_db.Concepts, f)\n",
    "\n",
    "print(\"\\n\\n\".join([slide.format_json() for slide in concept_db.Concepts]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "- Preprocessing\n",
    "- Generation of questions from (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
