{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricizing LLMaAiTB-E\n",
    "\n",
    "- Our Focus: Generation Quality\n",
    "- Measurement Techniques: \n",
    "    - Vector Comparison\n",
    "    - Human Preference Sample (A/B/C Testing)\n",
    "    - ???\n",
    "- Iterative documents to measure:\n",
    "    - Concepts (Generation phase)\n",
    "    - Slides (Teaching phase)\n",
    "    - Questions (Testing phase)\n",
    "- Resources for testing:\n",
    "    - Expert (From classes)\n",
    "    - GPT4 (Generation)\n",
    "    - LLMaAiTB-E (Teachabull)\n",
    "- Main concepts to cover:\n",
    "    - Coding NLP slides\n",
    "    - Math Derivatives\n",
    "    - History ?\n",
    "- Questions to test:\n",
    "    - 2 Topic\n",
    "    - 2 Multiple choice\n",
    "    - 2 Conceptual\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Helper Functions\n",
    "We will demonstrate our metrics using OpenAI's Vector Embeddings on our generated documents. We decided to use OpenAI's embeddings due to their large document size capacity. We agreed that this method would prove to be the best while comparing large documents.\n",
    "\n",
    "## LLM Prompt/Text Completion\n",
    "\n",
    "\n",
    "## Vector Comparison\n",
    "Embeddings: OpenAI’s text embeddings measure the relatedness of text strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pickle as pkl\n",
    "from AITutor_Backend.src.TutorUtils.concepts import *\n",
    "from AITutor_Backend.src.TutorUtils.notebank import NoteBank\n",
    "from AITutor_Backend.src.TutorUtils.slides import SlidePlan, Slide, SlidePlanner, Purpose, Concept, ConceptDatabase\n",
    "from AITutor_Backend.src.TutorUtils.questions import QuestionSuite, Question\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPENAI HELPER FUNCTIONS \n",
    "def request_output_from_llm(prompt, model: str):\n",
    "    \"\"\"Requests the Concept information from an LLM.\n",
    "\n",
    "    Args:\n",
    "        prompt: (str) - string to get passed to the model\n",
    "        model: (str) - \n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI() \n",
    "    \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "    {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": prompt,\n",
    "    },\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=8000,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts\n",
    "- Preprocessing\n",
    "- Generation of Graph for (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTEBANK from AI Tutor\n",
    "tutor_plan = '''Main Concept: Derivatives in Calculus\n",
    "Student has expressed a desire to learn about derivatives, including the Definition of Derivatives, Interpretation of the Derivative, Differentiation Formulas, Product and Quotient Rule, Derivatives of Trig Functions, Derivatives of Exponential and Logarithm Functions, Derivatives of Hyperbolic Functions, Chain Rule, Implicit Differentiation, Related Rates, Higher Order Derivatives, and Logarithmic Differentiation.\n",
    "Student has no prior knowledge of these topics.\n",
    "Student is a college student requiring help for a calculus class - implying the need for a detailed and university-level understanding of the subject.\n",
    "The student hopes to perform better in the calculus class by understanding the basic concepts of derivatives, especially the product and quotient rules and related rates.\n",
    "Subconcept: Definition of a Derivative\n",
    "Subconcept: Interpretation of the Derivative\n",
    "Subconcept: Basic Differentiation Formulas\n",
    "Subconcept: The Power Rule\n",
    "Subconcept: Product Rule\n",
    "Subconcept: Quotient Rule\n",
    "Subconcept: Chain Rule\n",
    "Subconcept: Derivatives of Trigonometric Functions\n",
    "Subconcept: Derivatives of Exponential and Logarithm Functions\n",
    "Subconcept: Derivatives of Hyperbolic Functions\n",
    "Subconcept: Implicit Differentiation\n",
    "Subconcept: Related Rates\n",
    "Subconcept: Higher Order Derivatives\n",
    "Subconcept: Logarithmic Differentiation\n",
    "Further questions should assess the student's specific struggles to tailor the examples and practice.\n",
    "Tutor could include practical problems and examples involving these concepts\n",
    "Equations to include: differentiation formulas and rules.\n",
    "Tutor should employ a variety of teaching methods, including visual aids, to enhance understanding of concepts as they are highly visual in nature. Programs like Desmos and GeoGebra could be recommended as tools for student practice.\n",
    "Considering the academic context, the session will cover theory, practice problems, and guide the student towards textbooks or scholarly articles for comprehensive understanding.\n",
    "Formative assessment through problem-solving should be incorporated to measure the student's grasp of each concept.\n",
    "Structured homework problems tailored to the student's coursework will be provided to reinforce lesson material.\n",
    "Include student’s Learning Request R - Mastery of Product and Quotient Rule and Related Rates.\n",
    "Concept: Derivatives in Calculus\n",
    "Concept: Definition of a Derivative\n",
    "Concept: Interpretation of the Derivative\n",
    "Concept: Basic Differentiation Formulas\n",
    "Concept: The Power Rule\n",
    "Concept: Product Rule\n",
    "Concept: Quotient Rule\n",
    "Concept: Chain Rule\n",
    "Concept: Derivatives of Trigonometric Functions\n",
    "Concept: Derivatives of Exponential and Logarithm Functions\n",
    "Concept: Derivatives of Hyperbolic Functions\n",
    "Concept: Implicit Differentiation\n",
    "Concept: Related Rates\n",
    "Concept: Higher Order Derivatives\n",
    "Concept: Logarithmic Differentiation\n",
    "Concept: Further questions should assess the student's specific struggles to tailor the examples and practice.\n",
    "Concept: Including Practical Problems and Examples\n",
    "Concept: Employing a variety of teaching methods, including visual aids\n",
    "Concept: Theory, Practice Problems, and Guiding towards Comprehensive Understanding\n",
    "Concept: Formative Assessment through Problem-Solving\n",
    "Concept: Structured Homework Problems to Reinforce Lesson Material\n",
    "Concept: Mastery of Product and Quotient Rule and Related Rates\n",
    "Student's Interest Statement: I want to learn everythin about derivates I don't have prior derivative knowledge this is very important for me thanks\n",
    "Student's Slides Preference Statement: both Information and Examples\n",
    "Student's Questions Preference Statement: I want multiple choice questions, free response, and math'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"Derivatives in Calculus\",\n",
      "\"definition\": \"In calculus, Derivatives are used to measure how a function's value changes as its input changes. The Definition of a Derivative is the limit of the difference quotient as the interval approaches zero, which provides an exact rate of change at any given point. Understanding the Interpretation of the Derivative is crucial, as it can represent velocity, slope, or rate of change in various contexts. Students must learn Differentiation Formulas , including the Product Rule and Quotient Rule , which are methods for finding the derivatives of functions involving multiplication and division, respectively. The Chain Rule is another fundamental technique for differentiating composite functions. Implicit Differentiation is used when a function is not explicitly solved for one variable in terms of another, and Related Rates problems involve finding a rate at which one quantity changes by relating it to another changing quantity. Students will also explore Higher Order Derivatives , which are derivatives of a function taken multiple times, and Logarithmic Differentiation , which simplifies the process of differentiating functions by using logarithms. Specialized derivative formulas for Derivatives of Trigonometric Functions , Derivatives of Exponential and Logarithm Functions , and Derivatives of Hyperbolic Functions are also part of the curriculum.\",\n",
      "\"latex\": \"Derivatives in Calculus\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Derivatives\",\n",
      "\"definition\": \"In calculus, Derivatives represent the rate at which a function 's Output changes with respect to changes in its Input . This concept involves understanding the Definition of a Derivative , Interpretation of the Derivative , applying Differentiation Formulas , using the Product Rule and Quotient Rule , finding derivatives of Trigonometric Functions , Exponential and Logarithm Functions , Hyperbolic Functions , applying the Chain Rule , performing Implicit Differentiation , solving Related Rates problems, computing Higher Order Derivatives , and using Logarithmic Differentiation .\",\n",
      "\"latex\": \"Derivatives\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Function\",\n",
      "\"definition\": \"In mathematics, a Function is a relation that associates each element from a set of inputs to a single element from another set of outputs . Functions are fundamental to the study of calculus and are necessary for defining the Derivatives .\",\n",
      "\"latex\": \"Function\",\n",
      "}\n",
      "\n",
      "{\"name\": \"relation\",\n",
      "\"definition\": \"In mathematics, a relation is a connection or association between elements from two sets. A Function is a specific type of relation where each element of the first set is associated with exactly one element of the second set.\",\n",
      "\"latex\": \"relation\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Input\",\n",
      "\"definition\": \"In mathematics, the Input of a Function refers to the value that is provided to the Function in order to produce a corresponding output . Input values are essential to the process of evaluating functions and understanding how different Input values can affect the output of a function.\",\n",
      "\"latex\": \"Input\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Output\",\n",
      "\"definition\": \"In the context of functions in mathematics, an Output is the result obtained after applying a Function to a given Input . It is the value that 'comes out' of the function.\",\n",
      "\"latex\": \"Output\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Definition of a Derivative\",\n",
      "\"definition\": \"The Definition of a Derivative in calculus represents the exact rate at which one Output quantity changes with respect to a change in an Input quantity. This is typically expressed as the limit of the ratio of the differences in output to input as the input difference approaches zero. The process of calculating a derivative is known as Differentiation .\",\n",
      "\"latex\": \"Definition of a Derivative\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Differentiation\",\n",
      "\"definition\": \"Differentiation is the process of finding a Derivatives , which represents the rate at which a Function 's Output changes with respect to changes in the Input . It encompasses various rules and techniques such as the Product Rule , Quotient Rule , Chain Rule , and Basic Differentiation Formulas to find derivatives of various types of functions, including Derivatives of Trigonometric Functions , Derivatives of Exponential and Logarithm Functions , Derivatives of Hyperbolic Functions , Implicit Differentiation , and Higher Order Derivatives .\",\n",
      "\"latex\": \"Differentiation\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Interpretation of the Derivative\",\n",
      "\"definition\": \"The Interpretation of the Derivative is a fundamental aspect of calculus that provides an understanding of the derivative beyond its mathematical computation. It often involves the analysis of the rate at which a Function 's Output changes with respect to its Input . In physics, this could be understood as the velocity of an object when the derivative represents the rate of change of position with respect to time. In economics, it could represent the marginal cost or revenue when the derivative corresponds to the rate of change of cost or revenue with respect to production levels.\",\n",
      "\"latex\": \"Interpretation of the Derivative\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Differentiation Formulas\",\n",
      "\"definition\": \"Differentiation Formulas are mathematical expressions that provide the derivative of common functions. These include the derivatives of basic functions like polynomials, exponential functions, and trigonometric functions. They serve as a fundamental toolkit in calculus, enabling the computation of the derivative of more complex functions through Derivatives , Product Rule , Quotient Rule , Chain Rule , and the derivatives of elementary functions.\",\n",
      "\"latex\": \"Differentiation Formulas\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Product Rule\",\n",
      "\"definition\": \"The Product Rule is a Derivatives formula that determines the derivative of a function that is the product of two other Function s. According to this rule, if we have two functions \\( u(x) \\) and \\( v(x) \\), then the derivative of their product \\( u(x)v(x) \\) is given by \\( u'(x)v(x) + u(x)v'(x) \\), where \\( u'(x) \\) and \\( v'(x) \\) are the derivatives of \\( u(x) \\) and \\( v(x) \\) respectively.\",\n",
      "\"latex\": \"Product Rule\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Quotient Rule\",\n",
      "\"definition\": \"In calculus, the Quotient Rule is a Differentiation technique used to find the derivative of a quotient of two Function s. It states that if \\( u \\) and \\( v \\) are functions of \\( x \\), then the derivative of \\( u/v \\) is given by \\( (v \\cdot u' - u \\cdot v') / v^2 \\), where \\( u' \\) and \\( v' \\) are the derivatives of \\( u \\) and \\( v \\) respectively.\",\n",
      "\"latex\": \"Quotient Rule\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Chain Rule\",\n",
      "\"definition\": \"In calculus, the Chain Rule is a formula to compute the derivative of a composite function. If a function \\( y \\) is defined as another function of \\( u \\), which itself is a function of \\( x \\), then the derivative of \\( y \\) with respect to \\( x \\) can be found by taking the derivative of \\( y \\) with respect to \\( u \\) and then multiplying it by the derivative of \\( u \\) with respect to \\( x \\). This rule is fundamental in performing Differentiation when dealing with complex functions that are compositions of simpler functions.\",\n",
      "\"latex\": \"Chain Rule\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Trigonometric Functions\",\n",
      "\"definition\": \"Trigonometric functions are Function s that relate the angles of a triangle to the lengths of its sides. They are fundamental in the study of triangles, geometry, periodic phenomena, and are used in many areas of mathematics and applied science. The basic trigonometric functions include sine, cosine, and tangent, each of which is the ratio of two specific sides of a right-angled triangle given a specific angle, often referred to as the angle of interest or the input angle.\",\n",
      "\"latex\": \"Trigonometric Functions\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Exponential and Logarithm Functions\",\n",
      "\"definition\": \"Exponential functions are mathematical functions of the form Function f(x) = b^x, where 'b' is the base and 'x' is the exponent. Logarithm functions are the inverses of exponential functions and are of the form g(x) = log_b(x), where 'g(x)' gives the exponent as the output when 'b' is raised to that power to get 'x'. These functions have unique properties that differentiate them from other types of functions, and they play a significant role in the calculation of Derivatives .\",\n",
      "\"latex\": \"Exponential and Logarithm Functions\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Hyperbolic Functions\",\n",
      "\"definition\": \"In mathematics, hyperbolic functions are analogs of the ordinary trigonometric, or circular, functions. The basic hyperbolic functions are the hyperbolic sine sinh and the hyperbolic cosine cosh , from which are derived the hyperbolic tangent tanh , hyperbolic cotangent cosh , hyperbolic secant sech , and hyperbolic cosecant cosh . Just like trigonometric functions, hyperbolic functions are important in many areas of science including hyperbolic geometry, special relativity, and calculus, particularly in the form of the Derivatives of Hyperbolic Functions .\",\n",
      "\"latex\": \"Hyperbolic Functions\",\n",
      "}\n",
      "\n",
      "{\"name\": \"sinh\",\n",
      "\"definition\": \"The hyperbolic sine function , denoted as sinh, is an exponential function -based analog of the circular Trigonometric Functions . It is defined for a real number x in terms of the exponential function: sinh(x) = (e^x - e^(-x)) / 2. The Hyperbolic Functions are analogs of trigonometric functions that are based on hyperbolas instead of circles.\",\n",
      "\"latex\": \"sinh\",\n",
      "}\n",
      "\n",
      "{\"name\": \"cosh\",\n",
      "\"definition\": \"In mathematics, particularly in hyperbolic functions, cosh is the hyperbolic cosine function, which is the even part of the exponential function in the imaginary argument. It is defined as the average of e to the power of x and e to the power of negative x . The function is one of the basic Hyperbolic Functions , analogous to the trigonometric cosine function but for a hyperbola rather than a circle.\",\n",
      "\"latex\": \"cosh\",\n",
      "}\n",
      "\n",
      "{\"name\": \"tanh\",\n",
      "\"definition\": \"The hyperbolic tangent function, denoted as tanh , is a Hyperbolic Functions that relates to exponential functions through the formula tanh (x) = (e^x - e^(-x)) / (e^x + e^(-x)). It is the ratio of the sinh function to the cosh function and is important in the context of Derivatives because it has interesting derivative properties that are used in calculus.\",\n",
      "\"latex\": \"tanh\",\n",
      "}\n",
      "\n",
      "{\"name\": \"sech\",\n",
      "\"definition\": \"In mathematics, specifically in trigonometry and calculus, sech is a shorthand notation for the hyperbolic secant function. It is one of the basic Hyperbolic Functions and is related to the conventional trigonometric secant function but has distinct properties. The sech function is defined as the reciprocal of the hyperbolic cosine function, cosh .\",\n",
      "\"latex\": \"sech\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Derivatives of Hyperbolic Functions\",\n",
      "\"definition\": \"The Derivatives of Hyperbolic Functions involves finding the Derivatives of functions like sinh , cosh , tanh , and sech . These hyperbolic functions are analogues of the trigonometric functions but for a hyperbola, rather than a circle. The derivatives of these functions follow similar rules to their trigonometric counterparts and are important in various areas of calculus, including integration, solving differential equations, and hyperbolic geometry.\",\n",
      "\"latex\": \"Derivatives of Hyperbolic Functions\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Implicit Differentiation\",\n",
      "\"definition\": \"In calculus, Implicit Differentiation is a technique used to find the Derivatives of a Function that is not explicitly solved for one variable in terms of another. It involves taking the Derivatives of both sides of an equation with respect to the independent variable and then solving for the Derivatives of the dependent variable. This method is particularly useful for equations where y is defined implicitly in terms of x.\",\n",
      "\"latex\": \"Implicit Differentiation\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Related Rates\",\n",
      "\"definition\": \"Related rates are a set of techniques used to find the rate at which one quantity changes in relation to another quantity's rate of change. These problems typically involve Derivatives and require the application of the Chain Rule to relate the different rates of change described by Differentiation Formulas . They often arise in real-world problems where two or more variables are related by a Function and change over time.\",\n",
      "\"latex\": \"Related Rates\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Higher Order Derivatives\",\n",
      "\"definition\": \"Higher Order Derivatives refer to the derivatives of a Function that have been derived more than once. These include the second derivative, which can provide information about the concavity and inflection points of the Function , and higher derivatives, which can be used to approximate the Function using Taylor series. Understanding Higher Order Derivatives is crucial for topics such as Related Rates , Implicit Differentiation , and the analysis of the behavior of graphs of functions.\",\n",
      "\"latex\": \"Higher Order Derivatives\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Logarithmic Differentiation\",\n",
      "\"definition\": \"Logarithmic Differentiation is a technique used to differentiate functions by taking the natural logarithm (ln) of both sides of an equation and then using the properties of logarithms to simplify the differentiation process. This method is particularly useful when dealing with products, quotients, or powers of functions where the Product Rule , Quotient Rule , or Chain Rule would otherwise be complex to apply. It is often employed for functions where the variable is both in the base and the exponent or in cases of complex Function s involving product s and quotient s of trigonometric , exponential , and logarithmic functions .\",\n",
      "\"latex\": \"Logarithmic Differentiation\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Product\",\n",
      "\"definition\": \"In arithmetic and algebra, the product is the result of multiplying, or an expression that identifies factors to be multiplied. Thus, the product of two numbers, such as Input values, is the result of the multiplication operation. In the context of Derivatives and the Product Rule , the product refers to the result of multiplying two differentiable Function s, which can then be differentiated using the rule.\",\n",
      "\"latex\": \"Product\",\n",
      "}\n",
      "\n",
      "{\"name\": \"quotient\",\n",
      "\"definition\": \"In arithmetic and algebra, the quotient is the result obtained by dividing one number or expression by another. In the context of Derivatives , specifically when dealing with the Quotient Rule , the quotient represents the division of two differentiable Function s.\",\n",
      "\"latex\": \"quotient\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Trigonometric\",\n",
      "\"definition\": \"In mathematics, trigonometric functions are Function s related to angles and triangles, often used to model periodic phenomena. The primary trigonometric functions include sinh , cosh , tanh , which are based on the ratios of the sides of a right-angled triangle. Trigonometry also involves the study of Trigonometric Identities , which are equations involving trigonometric functions that are true for all values of the involved variables.\",\n",
      "\"latex\": \"Trigonometric\",\n",
      "}\n",
      "\n",
      "{\"name\": \"exponential\",\n",
      "\"definition\": \"In mathematics, the term exponential refers to a function of the form \\( f(x) = a^x \\), where \\( a \\) is a constant called the base and \\( x \\) is the argument of the function. Exponential functions are characterized by their rate of growth or decay, which is proportional to the function's value at any point. This concept is pivotal in understanding Derivatives of Exponential and Logarithm Functions and has applications in various fields such as calculus, physics, and finance.\",\n",
      "\"latex\": \"exponential\",\n",
      "}\n",
      "\n",
      "{\"name\": \"logarithmic functions\",\n",
      "\"definition\": \"In mathematics, logarithmic functions are the inverses of exponential functions . They are used to solve equations in which the unknown appears as the exponent of some other quantity. Logarithmic functions are denoted by \\( \\log_b(x) \\), where \\( b \\) is the base and \\( x \\) is the argument of the logarithm. These functions have properties that make them useful for various applications, such as simplifying the process of multiplication and division of numbers into addition and subtraction, respectively, when dealing with exponents.\",\n",
      "\"latex\": \"logarithmic functions\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Derivatives of Trigonometric Functions\",\n",
      "\"definition\": \"The Derivatives of Trigonometric Functions are the rates at which the Trigonometric Functions change with respect to their Input angle. In calculus, common trigonometric functions such as sine, cosine, and tangent have their own specific derivative rules that allow us to compute the derivative for any angle given in radians. These rules are vital for solving various types of calculus problems including those involving Derivatives , Chain Rule , and Related Rates .\",\n",
      "\"latex\": \"Derivatives of Trigonometric Functions\",\n",
      "}\n",
      "\n",
      "{\"name\": \"Derivatives of Exponential and Logarithm Functions\",\n",
      "\"definition\": \"The Derivatives of Exponential and Logarithm Functions involve applying Differentiation rules to functions that have bases of natural constants e (exponential functions) or involve the natural logarithm, ln (logarithm functions). For exponential functions, the derivative of e^x is e^x, while the derivative of a^x (where a is any positive constant) involves an additional factor of ln(a). For logarithm functions, the derivative of ln(x) is 1/x, and for log_a(x) (logarithm with base a), it is 1/(x ln(a)). These derivatives are fundamental in solving calculus problems involving exponential growth and decay, as well as in optimization problems where logarithmic functions are part of the model.\",\n",
      "\"latex\": \"Derivatives of Exponential and Logarithm Functions\",\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Concept generation from AITutor:\n",
    "import pickle as pkl\n",
    "notebank = NoteBank()\n",
    "[notebank.add_note(n) for n in tutor_plan.split(\"\\n\")]\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"./temp_concepts_calculus.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(\"./temp_concepts_calculus.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        concept = pkl.load(f)\n",
    "        concept_db = ConceptDatabase(\"Derivatives,  The Definition of Derivates , Interpretation of the Derivative  Differentiation Formulas , Product and Quotient Rule ,  Derivatives of Trig Functions, Derivatives of Exponential and Logarithm Functions, Derivatives of Hyperbolic Functions, Chain Rule , Implicit Differentiation, Related Rates, Higher Order Derivatives, Logarithmic Differentiation . I have no prior knowledge Derivatives,  The Definition of Derivates , Interpretation of the Derivative  Differentiation Formulas , Product and Quotient Rule ,  Derivatives of Trig Functions, Derivatives of Exponential and Logarithm Functions, Derivatives of Hyperbolic Functions, Chain Rule , Implicit Differentiation, Related Rates, Higher Order Derivatives, Logarithmic Differentiation.\",notebank.env_string(), False)\n",
    "        concept_db.Concepts = concept\n",
    "else:\n",
    "    concept_db = ConceptDatabase(\"Derivatives,  The Definition of Derivates , Interpretation of the Derivative  Differentiation Formulas , Product and Quotient Rule ,  Derivatives of Trig Functions, Derivatives of Exponential and Logarithm Functions, Derivatives of Hyperbolic Functions, Chain Rule , Implicit Differentiation, Related Rates, Higher Order Derivatives, Logarithmic Differentiation . I have no prior knowledge Derivatives,  The Definition of Derivates , Interpretation of the Derivative  Differentiation Formulas , Product and Quotient Rule ,  Derivatives of Trig Functions, Derivatives of Exponential and Logarithm Functions, Derivatives of Hyperbolic Functions, Chain Rule , Implicit Differentiation, Related Rates, Higher Order Derivatives, Logarithmic Differentiation.\",notebank.env_string())\n",
    "    with open(\"./temp_concepts_calculus.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(concept_db.Concepts, f)\n",
    "\n",
    "print(\"\\n\\n\".join([slide.format_json() for slide in concept_db.Concepts]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slides: \n",
    "- Preprocessing\n",
    "- Generation of Document for (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SLIDE OBJ PROMPTs\n",
    "prompt = ''' #Your task is to create a JSON object from a slide string. View the example Input and output, and then repeat the same for the provided input. \n",
    "Perform the conversion for each slide s in the input string such that s->json_object(s). You should be able to figure out which is the title and which is the description.\n",
    "IMPORTANT: Escape Characters in JSON Data can cause errors if the JSON Object or JSON data contains backslashes, which means they need to be properly escaped\n",
    "Avoid these errors: Invalid \\escape: line 24 column 72 (char 2199)\n",
    "By properly escaping your backslashes ('\\\\')\n",
    "IMPORTANT: If there is two words together, such as \"functionwhere\", without being separated with a white space, that most probably means that there is a new line ('\\n') or space (' ') in between them, e.g. \"function where\".\n",
    "\n",
    "// Input:\n",
    "Page 1 Content:\n",
    "Natural Language ProcessingProfessor John LicatoUniversity of South FloridaChapter 2:RegEx, Edit Distance\n",
    "\n",
    "----------------------------------------\n",
    "Page 2 Content:\n",
    "\"Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When you’re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.\"Regular Expressions\n",
    "----------------------------------------\n",
    "Page 3 Content:\n",
    "The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))Regular Expressions\n",
    "----------------------------------------\n",
    "Page 4 Content:\n",
    "The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')Regular Expressions\n",
    "----------------------------------------\n",
    "Page 5 Content:\n",
    "Creating regex objectsr’ = raw string\\d – placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)\n",
    "----------------------------------------\n",
    "Page 6 Content:\n",
    "Matching regex objects\n",
    "mo = match object – contains the result of our search>>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)>>> mo = phoneNumRegex.search(‘My number is 415-555-4242.’)>>> print(‘Phone number found: ’ + mo.group())Phone number found: 415-555-4242\n",
    "----------------------------------------\n",
    "Page 7 Content:\n",
    "Text Normalization•We will work a lot with large datasets / corpora•We often need to pre-process text•Tokenizing (segmenting) words•Normalizing word formats•Segmenting sentences (e.g. by using punctuation)\n",
    "----------------------------------------\n",
    "Page 8 Content:\n",
    "Tokenization – segmenting running text into words (or word-like units)>>> text = 'That U.S.A. poster-print costs $12.40...'>>> pattern = r\\'\\'\\', (?x)  # set flag to allow verbose regexps...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A....     | \\w+(-\\w+)*      # words with optional internal hyphens...     | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%...     | \\.\\.\\.          # ellipsis...     | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [... \\'\\'\\'>>> nltk.regexp_tokenize(text, pattern)['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n",
    "----------------------------------------\n",
    "Page 9 Content:\n",
    "Subword tokenization•How do we capture relations between words like:–new, newer–blow, blowing–precipitation, precipitate•Often useful to break tokens into *sub*words•Usually split into token learners, and token segmenters\n",
    "----------------------------------------\n",
    "Page 10 Content:\n",
    "Byte-pair encoding (BPE)•A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab VV <- all unique characters in C                  # initial set of tokens is charactersfor i = 1 to k do                                # merge tokens til k times    t_L, t_R <- Most frequent pair of adjacent tokens in C    t_new <- t_L + t_R                           # make new token by concatenating    V <- V + t_new                               # update the vocabulary    Replace each occurrence of t_L, t_R in C with t_new # and update the corpusreturn Vcorpus5 low_2 lowest_6 newer_3 wider_2 new_vocabulary_, d, e, i, l, n, o, r, s, t, wcorpus5 low _2 lowest _6 newer _3 wider _2 new _vocabulary_, d, e, i, l, n, o, r, s, t, w, er\n",
    "----------------------------------------\n",
    "...\n",
    "        \n",
    "// Output:\n",
    "        { \n",
    "                \\\"slides\\\":[\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Natural Language Processing\\\", \n",
    "                                \\\"Description\\\": \\\"Professor John Licato University of South Florida Chapter 2:RegEx, Edit Distance\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When you’re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Regular Expressions\\\", \n",
    "                                \\\"Description\\\": \\\"The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Creating regex objects\\\", \n",
    "                                \\\"Description\\\": \\\"r’ = raw string\\d – placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Matching regex objects\\\", \n",
    "                                \\\"Description\\\": \\\">>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)>>> mo = phoneNumRegex.search(‘My number is 415-555-4242.’)>>> print(‘Phone number found: ’ + mo.group())Phone number found: 415-555-4242 mo = match object – contains the result of our search\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Text Normalization\\\", \n",
    "                                \\\"Description\\\": \\\"•We will work a lot with large datasets / corpora\\n•We often need to pre-process text\\n•Tokenizing (segmenting) words\\n•Normalizing word formats\\n•Segmenting sentences (e.g. by using punctuation) )\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        { \n",
    "                                \\\"Title\\\":\\\"Tokenization – segmenting running text into words (or word-like units)\\\", \n",
    "                                \\\"Description\\\": \\\">>> text = 'That U.S.A. poster-print costs $12.40...'\\n>>> pattern = r\\'\\'\\', (?x)  # set flag to allow verbose regexps\\n...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A\\n....     | \\w+(-\\w+)*      # words with optional internal hyphens\\n...     | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\\n...     | \\.\\.\\.          # ellipsis...     | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [\\n... \\'\\'\\'\\n>>> nltk.regexp_tokenize(text, pattern)\\n['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Subword tokenization\\\", \n",
    "                                \\\"Description\\\": \\\"•How do we capture relations between words like:\\n–new, newer\\n–blow, blowing\\n–precipitation, precipitate\\n•Often useful to break tokens into *sub*words•Usually split into token learners, and token segmenters\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        {\n",
    "                                \\\"Title\\\":\\\"Byte-pair encoding (BPE)\\\", \n",
    "                                \\\"Description\\\": \\\"•A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab V\\nV <- all unique characters in C                  # initial set of tokens is characters\\nfor i = 1 to k do                                # merge tokens til k times    \\nt_L, t_R <- Most frequent pair of adjacent tokens in C    \\nt_new <- t_L + t_R                           # make new token by concatenating    \\nV <- V + t_new                               # update the vocabulary    \\nReplace each occurrence of t_L, t_R in C with t_new # and update the corpus\\nreturn V\\ncorpus\\n5 low_\\n2 lowest_\\n6 newer_\\n3 wider_\\n2 new_\\nvocabulary\\n_, d, e, i, l, n, o, r, s, t, w\\ncorpus\\n5 low _\\n2 lowest _\\n6 newer _\\n3 wider _\\n2 new _\\nvocabulary\\n_, d, e, i, l, n, o, r, s, t, w, er\\\",\n",
    "                                \\\"Latex\\\": []\n",
    "                        },\n",
    "                        ...\n",
    "                ]\n",
    "        }\n",
    "Remember! Escape Characters in JSON Data: If the JSON Object or JSON data contains backslashes, they need to be properly escaped.\n",
    "Avoid these errors: Invalid \\escape: line 24 column 72 (char 2199)\n",
    "\n",
    "// Input:\n",
    "        $SLIDE$\n",
    "\n",
    "// Output:\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 Content:\n",
      "Natural Language ProcessingProfessor John LicatoUniversity of South FloridaChapter 2:RegEx, Edit Distance\n",
      "\n",
      "----------------------------------------\n",
      "Page 2 Content:\n",
      "\"Knowing [regular expressions] can mean the difference between solving a problem in 3 steps and solving it in 3,000 steps. When you’re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through.\"Regular Expressions\n",
      "----------------------------------------\n",
      "Page 3 Content:\n",
      "The following function called `isPhoneNumber(text)` is designed to check if the provided string is a phone number in a specific format using regex. def isPhoneNumber(text):    if len(text) != 12:        return False    for i in range(0, 3):        if not text[i].isdecimal():            return False    if text[3] != '-':        return False    for i in range(4, 7):        if not text[i].isdecimal():            return False    if text[7] != '-':        return False    for i in range(8, 12):        if not text[i].isdecimal():            return False    return Trueprint('415-555-4242 is a phone number:')print(isPhoneNumber('415-555-4242'))print('Moshi moshi is a phone number:')print(isPhoneNumber('Moshi moshi'))Regular Expressions\n",
      "----------------------------------------\n",
      "Page 4 Content:\n",
      "The following Python code uses the previously defined `isPhoneNumber` function within a loop to search through a longer string for valid phone number formats. message = 'Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.'for i in range(len(message)):    chunk = message[i:i+12]    if isPhoneNumber(chunk):        print('Phone number found: ' + chunk)print('Done')Regular Expressions\n",
      "----------------------------------------\n",
      "Page 5 Content:\n",
      "Creating regex objectsr’ = raw string\\d – placeholder for a single digit>>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)\n",
      "----------------------------------------\n",
      "Page 6 Content:\n",
      "Matching regex objects\n",
      "mo = match object – contains the result of our search>>> import re>>> phoneNumRegex = re.compile(r’\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d’)>>> mo = phoneNumRegex.search(‘My number is 415-555-4242.’)>>> print(‘Phone number found: ’ + mo.group())Phone number found: 415-555-4242\n",
      "----------------------------------------\n",
      "Page 7 Content:\n",
      "Text Normalization•We will work a lot with large datasets / corpora•We often need to pre-process text•Tokenizing (segmenting) words•Normalizing word formats•Segmenting sentences (e.g. by using punctuation)\n",
      "----------------------------------------\n",
      "Page 8 Content:\n",
      "Tokenization – segmenting running text into words (or word-like units)>>> text = 'That U.S.A. poster-print costs $12.40...'>>> pattern = r''', (?x)  # set flag to allow verbose regexps...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A....     | \\w+(-\\w+)*      # words with optional internal hyphens...     | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%...     | \\.\\.\\.          # ellipsis...     | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [... '''>>> nltk.regexp_tokenize(text, pattern)['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n",
      "----------------------------------------\n",
      "Page 9 Content:\n",
      "Subword tokenization•How do we capture relations between words like:–new, newer–blow, blowing–precipitation, precipitate•Often useful to break tokens into *sub*words•Usually split into token learners, and token segmenters\n",
      "----------------------------------------\n",
      "Page 10 Content:\n",
      "Byte-pair encoding (BPE)•A way of performing subword tokenizationfunction BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab VV <- all unique characters in C                  # initial set of tokens is charactersfor i = 1 to k do                                # merge tokens til k times    t_L, t_R <- Most frequent pair of adjacent tokens in C    t_new <- t_L + t_R                           # make new token by concatenating    V <- V + t_new                               # update the vocabulary    Replace each occurrence of t_L, t_R in C with t_new # and update the corpusreturn Vcorpus5 low_2 lowest_6 newer_3 wider_2 new_vocabulary_, d, e, i, l, n, o, r, s, t, wcorpus5 low _2 lowest _6 newer _3 wider _2 new _vocabulary_, d, e, i, l, n, o, r, s, t, w, er\n",
      "----------------------------------------\n",
      "Page 11 Content:\n",
      "Word normalization•Case folding – e.g., making everything lowercase•Lemmatization – folding lemmas together if they have the same root (dinner / dinners, am / are / is, etc.). •Stemming – performing lemmatization by removing all but the roots of words (running / runner -> run)\n",
      "----------------------------------------\n",
      "Page 12 Content:\n",
      "Minimum Edit DistanceDefinition of Minimum Edit Distance\n",
      "----------------------------------------\n",
      "Page 13 Content:\n",
      "How similar are two strings?•Spell correction–The user typed “graffe”Which is closest? •graf•graft•grail•giraffe•Computational Biology•Align two sequences of nucleotides•Resulting alignment:•Also for Machine Translation, Information Extraction, Speech RecognitionAGGCTATCACCTGACCTCCAGGCCGATGCCCTAGCTATCACGACCGCGGTCGATTTGCCCGAC-AGGCTATCACCTGACCTCCAGGCCGA--TGCCC---TAG-CTATCAC--GACCGC--GGTCGATTTGCCCGAC\n",
      "----------------------------------------\n",
      "Page 14 Content:\n",
      "Edit Distance•The minimum edit distance between two strings•Is the minimum number of editing operations–Insertion–Deletion–Substitution•Needed to transform one into the other\n",
      "----------------------------------------\n",
      "Page 15 Content:\n",
      "Minimum Edit Distance•Two strings and their alignment:INTENTION| | | | | | |*EXECUTION\n",
      "----------------------------------------\n",
      "Page 16 Content:\n",
      "Minimum Edit Distance•If each operation has cost of 1–Distance between these is 5•If substitutions cost 2 (Levenshtein)–Distance between them is 8INTENTION| | | | | | |*EXECUTIONd s s   is s\n",
      "----------------------------------------\n",
      "Page 17 Content:\n",
      "Alignment in Computational Biology•Given a sequence of bases•An alignment:•Given two sequences, align each letter to a letter or gap-AGGCTATCACCTGACCTCCAGGCCGA--TGCCC---TAG-CTATCAC--GACCGC--GGTCGATTTGCCCGACAGGCTATCACCTGACCTCCAGGCCGATGCCCTAGCTATCACGACCGCGGTCGATTTGCCCGAC\n",
      "----------------------------------------\n",
      "Page 18 Content:\n",
      "Other uses of Edit Distance in NLP•Evaluating Machine Translation and speech recognitionR Spokesman confirms    senior government adviser was shotH Spokesman said    the senior            adviser was shot dead              S      I              D                        I•Named Entity Extraction and Entity Coreference–IBM Inc. announced today–IBM profits–Stanford President John Hennessy announced yesterday–for Stanford University President John Hennessy\n",
      "----------------------------------------\n",
      "Page 19 Content:\n",
      "How to find the Min Edit Distance?•Searching for a path (sequence of edits) from the start string to the final string:–Initial state: the word we’re transforming–Operators: insert, delete, substitute–Goal state:  the word we’re trying to get to–Path cost: what we want to minimize: the number of edits37\n",
      "\n",
      "----------------------------------------\n",
      "Page 20 Content:\n",
      "Minimum Edit as Search•But the space of all edit sequences is huge!–We can’t afford to navigate naïvely–Lots of distinct paths wind up at the same state.•We don’t have to keep track of all of them•Just the shortest path to each of those revisted states.\n",
      "38\n",
      "----------------------------------------\n",
      "Page 21 Content:\n",
      "Defining Min Edit Distance•For two strings–X of length n –Y of length m•We define D(i,j)–the edit distance between X[1..i] and Y[1..j] •i.e., the first i characters of X and the first j characters of Y–The edit distance between X and Y is thus D(n,m)\n",
      "----------------------------------------\n",
      "Page 22 Content:\n",
      "Dynamic Programming forMinimum Edit Distance•Dynamic programming: A tabular computation of D(n,m)•Solving problems by combining solutions to subproblems.•Bottom-up–We compute D(i,j) for small i,j –And compute larger D(i,j) based on previously computed smaller values–i.e., compute D(i,j) for all i (0 < i < n)  and j (0 < j < m)\n",
      "----------------------------------------\n",
      "Page 23 Content:\n",
      "Defining Min Edit Distance (Levenshtein)•InitializationD(i,0) = iD(0,j) = j•Recurrence Relation:For each  i = 1…M   For each  j = 1…N                       D(i-1,j) + 1          D(i,j)= min  D(i,j-1) + 1                       D(i-1,j-1) +   2; if X(i) ≠ Y(j)                                         0; if X(i) = Y(j)•Termination:D(N,M) is distance \n",
      "----------------------------------------\n",
      "Page 24 Content:\n",
      "The Edit Distance TableThe minimum edit distance between two strings is the minimum number of editing operations needed to transform one string into the other. The typical operations allowed are:1.Insertion (Ins): Add one character to the string.2.Deletion (Del): Remove one character from the string.3.Substitution (Sub): Replace one character with another.\n",
      "----------------------------------------\n",
      "Page 25 Content:\n",
      "Computing alignments•Edit distance isn’t sufficient–We often need to align each character of the two strings to each other•We do this by keeping a “backtrace”•Every time we enter a cell, remember where we came from•When we reach the end, –Trace back the path from the upper right corner to read off the alignment\n",
      "----------------------------------------\n",
      "Page 26 Content:\n",
      "MinEdit with Backtrace\n",
      "\n",
      "----------------------------------------\n",
      "Page 27 Content:\n",
      "Adding Backtrace to Minimum Edit Distance•Base conditions:                                                        Termination:D(i,0) = i         D(0,j) = j         D(N,M) is distance •Recurrence Relation:For each  i = 1…M  For each  j = 1…N                      D(i-1,j) + 1         D(i,j)= min  D(i,j-1) + 1                      D(i-1,j-1) +  2; if X(i) ≠ Y(j)                                       0; if X(i) = Y(j)                     LEFT         ptr(i,j)=   DOWN                     DIAGinsertiondeletionsubstitutioninsertiondeletionsubstitution\n",
      "----------------------------------------\n",
      "Page 28 Content:\n",
      "The Distance Matrix\n",
      "Slide adapted from Serafim Batzoglouy0 ………………………………  yMx0 ……………………  xNEvery non-decreasing path from (0,0) to (M, N) corresponds to an alignment of the two sequencesAn optimal alignment is composed of optimal subalignments\n",
      "----------------------------------------\n",
      "Page 29 Content:\n",
      "Result of Backtrace•Two strings and their alignment:INTENTION| | | | | | |*EXECUTION\n",
      "----------------------------------------\n",
      "Page 30 Content:\n",
      "Performance•Time:    O(nm)•Space:    O(nm)•Backtrace    O(n+m)\n",
      "----------------------------------------\n",
      "Page 31 Content:\n",
      "Hearst Patterns for Hypernymy•Hyponym – “Is-A” relationship•Hypernym – Opposite of hypernym•Color is a hypernym of red; cat is a hypernym of white cat. •A rule-based way of detecting hypernym relationships in text is through Hearst patterns\n",
      "----------------------------------------\n",
      "Page 32 Content:\n",
      "Some Hearst Patterns•All bolded symbols (a, b, …) are noun phrases•Type 1 - Extract (b,a)–“a is b”–“a is a type of b”–“a is a kind of b”–“a was b”–“a was a type of b”–“a was a kind of b”–“a are b”–“a are a type of b”–“a are a kind of b”•Type 2 - Extract (a,b), (a,c), …, (a,d)–“a, including b”–“a, including b, c, …, and d”–“a, including b, c, …, or d”–“a, such as b”–“a, such as b, c, …, and d”–“a, such as b, c, …, or d”•There are many others!\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Slide helper functions\n",
    "import PyPDF2\n",
    "from pptx import Presentation\n",
    "import json\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"Reads a PDF file and prints the content of each page\"\"\"\n",
    "    slide_str = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "\n",
    "        for i in range(num_pages):\n",
    "            page = reader.pages[i]\n",
    "            text = page.extract_text()\n",
    "            slide_str += f\"Page {i+1} Content:\\n{text}\"\n",
    "            slide_str += \"\\n\" + (\"-\" * 40) + \"\\n\"\n",
    "    return slide_str\n",
    "\n",
    "def extract_text_from_slide(slide):\n",
    "    \"\"\"Extracts title and content from a slide\"\"\"\n",
    "    title = slide.shapes.title.text if slide.shapes.title else \"No Title\"\n",
    "    content = []\n",
    "\n",
    "    for shape in slide.shapes:\n",
    "        if hasattr(shape, \"text\"):\n",
    "            content.append(shape.text)\n",
    "\n",
    "    return title, content\n",
    "\n",
    "def read_pptx(file_path):\n",
    "    \"\"\"Reads a pptx file and prints the title and content of each slide\"\"\"\n",
    "    prs = Presentation(file_path)\n",
    "\n",
    "    for slide in prs.slides:\n",
    "        title, content = extract_text_from_slide(slide)\n",
    "        print(f\"Title: {title}\")\n",
    "        print(\"Content:\", \"\\n\".join(content))\n",
    "        print(\"-\" * 40)\n",
    "def get_slide_prompt(slide_template, data):\n",
    "    return slide_template.replace(\"$SLIDE$\", data)\n",
    "slide_str = read_pdf('Research/generation_data/slides/Expert/codingSlides_Expert.pdf')\n",
    "print(slide_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "Package not found at 'Research/generation_data/slides/Expert/calculusSlides_Expert.ppt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/Users/juanjgomez/Documents/GitHub/AITutor-Backend/research_metrics.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juanjgomez/Documents/GitHub/AITutor-Backend/research_metrics.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m### TEST SLIDE OBJ GEN FROM GPT FOR EXPERT\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/juanjgomez/Documents/GitHub/AITutor-Backend/research_metrics.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m slide_str \u001b[39m=\u001b[39m read_pptx(\u001b[39m'\u001b[39;49m\u001b[39mResearch/generation_data/slides/Expert/calculusSlides_Expert.ppt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juanjgomez/Documents/GitHub/AITutor-Backend/research_metrics.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m curr_prompt \u001b[39m=\u001b[39m get_slide_prompt(prompt, slide_str)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juanjgomez/Documents/GitHub/AITutor-Backend/research_metrics.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m/Users/juanjgomez/Documents/GitHub/AITutor-Backend/research_metrics.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juanjgomez/Documents/GitHub/AITutor-Backend/research_metrics.ipynb#X46sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_pptx\u001b[39m(file_path):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juanjgomez/Documents/GitHub/AITutor-Backend/research_metrics.ipynb#X46sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Reads a pptx file and prints the title and content of each slide\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/juanjgomez/Documents/GitHub/AITutor-Backend/research_metrics.ipynb#X46sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     prs \u001b[39m=\u001b[39m Presentation(file_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juanjgomez/Documents/GitHub/AITutor-Backend/research_metrics.ipynb#X46sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mfor\u001b[39;00m slide \u001b[39min\u001b[39;00m prs\u001b[39m.\u001b[39mslides:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juanjgomez/Documents/GitHub/AITutor-Backend/research_metrics.ipynb#X46sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         title, content \u001b[39m=\u001b[39m extract_text_from_slide(slide)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pptx/api.py:28\u001b[0m, in \u001b[0;36mPresentation\u001b[0;34m(pptx)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m pptx \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     pptx \u001b[39m=\u001b[39m _default_pptx_path()\n\u001b[0;32m---> 28\u001b[0m presentation_part \u001b[39m=\u001b[39m Package\u001b[39m.\u001b[39;49mopen(pptx)\u001b[39m.\u001b[39mmain_document_part\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_pptx_package(presentation_part):\n\u001b[1;32m     31\u001b[0m     tmpl \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is not a PowerPoint file, content type is \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pptx/opc/package.py:73\u001b[0m, in \u001b[0;36mOpcPackage.open\u001b[0;34m(cls, pkg_file)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mcls\u001b[39m, pkg_file):\n\u001b[1;32m     72\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return an |OpcPackage| instance loaded with the contents of `pkg_file`.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(pkg_file)\u001b[39m.\u001b[39;49m_load()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pptx/opc/package.py:157\u001b[0m, in \u001b[0;36mOpcPackage._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    156\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the package after loading all parts and relationships.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     pkg_xml_rels, parts \u001b[39m=\u001b[39m _PackageLoader\u001b[39m.\u001b[39;49mload(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pkg_file, \u001b[39mself\u001b[39;49m)\n\u001b[1;32m    158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rels\u001b[39m.\u001b[39mload_from_xml(PACKAGE_URI, pkg_xml_rels, parts)\n\u001b[1;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pptx/opc/package.py:186\u001b[0m, in \u001b[0;36m_PackageLoader.load\u001b[0;34m(cls, pkg_file, package)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    175\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mcls\u001b[39m, pkg_file, package):\n\u001b[1;32m    176\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return (pkg_xml_rels, parts) pair resulting from loading `pkg_file`.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[39m    The returned `parts` value is a {partname: part} mapping with each part in the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39m    object) to load those relationships into its |_Relationships| object.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(pkg_file, package)\u001b[39m.\u001b[39;49m_load()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pptx/opc/package.py:190\u001b[0m, in \u001b[0;36m_PackageLoader._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    189\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return (pkg_xml_rels, parts) pair resulting from loading pkg_file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     parts, xml_rels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parts, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_xml_rels\n\u001b[1;32m    192\u001b[0m     \u001b[39mfor\u001b[39;00m partname, part \u001b[39min\u001b[39;00m parts\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    193\u001b[0m         part\u001b[39m.\u001b[39mload_rels_from_xml(xml_rels[partname], parts)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pptx/util.py:215\u001b[0m, in \u001b[0;36mlazyproperty.__get__\u001b[0;34m(self, obj, type)\u001b[0m\n\u001b[1;32m    210\u001b[0m value \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     \u001b[39m# ---on first access, __dict__ item will absent. Evaluate fget()\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[39m# ---and store that value in the (otherwise unused) host-object\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# ---__dict__ value of same name ('fget' nominally)\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fget(obj)\n\u001b[1;32m    216\u001b[0m     obj\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m] \u001b[39m=\u001b[39m value\n\u001b[1;32m    217\u001b[0m \u001b[39mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pptx/opc/package.py:219\u001b[0m, in \u001b[0;36m_PackageLoader._parts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m@lazyproperty\u001b[39m\n\u001b[1;32m    211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_parts\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    212\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"dict {partname: Part} populated with parts loading from package.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \n\u001b[1;32m    214\u001b[0m \u001b[39m    Among other duties, this collection is passed to each relationships collection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39m    loaded.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     content_types \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_content_types\n\u001b[1;32m    220\u001b[0m     package \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_package\n\u001b[1;32m    221\u001b[0m     package_reader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_package_reader\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pptx/util.py:215\u001b[0m, in \u001b[0;36mlazyproperty.__get__\u001b[0;34m(self, obj, type)\u001b[0m\n\u001b[1;32m    210\u001b[0m value \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     \u001b[39m# ---on first access, __dict__ item will absent. Evaluate fget()\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[39m# ---and store that value in the (otherwise unused) host-object\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# ---__dict__ value of same name ('fget' nominally)\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fget(obj)\n\u001b[1;32m    216\u001b[0m     obj\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m] \u001b[39m=\u001b[39m value\n\u001b[1;32m    217\u001b[0m \u001b[39mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pptx/opc/package.py:203\u001b[0m, in \u001b[0;36m_PackageLoader._content_types\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39m@lazyproperty\u001b[39m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_content_types\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    199\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"|_ContentTypeMap| object providing content-types for items of this package.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[39m    Provides a content-type (MIME-type) for any given partname.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mreturn\u001b[39;00m _ContentTypeMap\u001b[39m.\u001b[39mfrom_xml(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_package_reader[CONTENT_TYPES_URI])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pptx/opc/serialized.py:35\u001b[0m, in \u001b[0;36mPackageReader.__getitem__\u001b[0;34m(self, pack_uri)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, pack_uri):\n\u001b[1;32m     34\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return bytes for part corresponding to `pack_uri`.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_blob_reader[pack_uri]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pptx/util.py:215\u001b[0m, in \u001b[0;36mlazyproperty.__get__\u001b[0;34m(self, obj, type)\u001b[0m\n\u001b[1;32m    210\u001b[0m value \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     \u001b[39m# ---on first access, __dict__ item will absent. Evaluate fget()\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[39m# ---and store that value in the (otherwise unused) host-object\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# ---__dict__ value of same name ('fget' nominally)\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fget(obj)\n\u001b[1;32m    216\u001b[0m     obj\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m] \u001b[39m=\u001b[39m value\n\u001b[1;32m    217\u001b[0m \u001b[39mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pptx/opc/serialized.py:49\u001b[0m, in \u001b[0;36mPackageReader._blob_reader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m@lazyproperty\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_blob_reader\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     48\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"|_PhysPkgReader| subtype providing read access to the package file.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m _PhysPkgReader\u001b[39m.\u001b[39;49mfactory(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pkg_file)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pptx/opc/serialized.py:135\u001b[0m, in \u001b[0;36m_PhysPkgReader.factory\u001b[0;34m(cls, pkg_file)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mif\u001b[39;00m zipfile\u001b[39m.\u001b[39mis_zipfile(pkg_file):\n\u001b[1;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m _ZipPkgReader(pkg_file)\n\u001b[0;32m--> 135\u001b[0m \u001b[39mraise\u001b[39;00m PackageNotFoundError(\u001b[39m\"\u001b[39m\u001b[39mPackage not found at \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m pkg_file)\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: Package not found at 'Research/generation_data/slides/Expert/calculusSlides_Expert.ppt'"
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid \\escape: line 33 column 283 (char 3025)\n"
     ]
    }
   ],
   "source": [
    "### TEST SLIDE OBJ GEN FROM GPT FOR EXPERT\n",
    "slide_str = read_pptx('Research/generation_data/slides/Expert/calculusSlides_Expert.pptx')\n",
    "\n",
    "\n",
    "curr_prompt = get_slide_prompt(prompt, slide_str)\n",
    "try:\n",
    "    json_data = request_output_from_llm(prompt=curr_prompt, model=\"gpt-3.5-turbo-16k\")\n",
    "    slide_obj = json.loads(json_data)\n",
    "    print(slide_obj)\n",
    "\n",
    "    # Convert the dictionary to a JSON-formatted string\n",
    "    json_str = json.dumps(slide_obj, indent=4)  # indent for pretty-printing\n",
    "\n",
    "    # Write the JSON string to a file\n",
    "    with open(\"Research/generation_data/slides/Expert/codingSlides_expert.json\", \"w\") as f:\n",
    "        f.write(json_str)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Introduction to Regular Expressions in NLP', 'purpose': 0, 'purpose_statement': 'This slide will serve as an entry point to understanding Regular Expressions (Regex) and how they are used in the context of Natural Language Processing (NLP). It aims to establish foundational knowledge for students with no prior experience, as specified in the Notebank. The focus will be on familiarizing the student with the syntax and basic operators of regex, setting the foundation for understanding its applications in NLP tasks.', 'concepts': ['Basic Syntax and Operators of Regular Expressions', 'Regular Expressions']}\n",
      "\n",
      "{'title': 'Fundamentals of Text Normalization in NLP', 'purpose': 0, 'purpose_statement': 'This slide aims to introduce the concept of Text Normalization within the realm of Natural Language Processing (NLP). It will explain what Text Normalization is, why it is critical for processing natural languages, and how it aids in preparing data for further NLP tasks such as Language Modeling or Sentiment Analysis.', 'concepts': ['Text Normalization', 'Tokenization of Text', 'Sentence Segmentation Techniques', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Getting Started with Edit Distance in Natural Language Processing', 'purpose': 0, 'purpose_statement': 'To provide an introduction to the concept of Edit Distance in Natural Language Processing (NLP), emphasizing its definition and utility within the field.', 'concepts': ['Applications of Edit Distance Algorithm in NLP']}\n",
      "\n",
      "{'title': 'Harnessing the Power of Regular Expressions in NLP', 'purpose': 4, 'purpose_statement': \"This slide aims to provide practical examples and exercises demonstrating the application of Regular Expressions within various NLP tasks, reinforcing theoretical knowledge through hands-on learning experiences to solidify the student's understanding.\", 'concepts': ['Uses of Regular Expressions in NLP']}\n",
      "\n",
      "{'title': 'Understanding Edit Distance and Its Relevance in NLP Applications', 'purpose': 0, 'purpose_statement': 'To introduce the concept of Edit Distance in the context of Natural Language Processing (NLP), laying the groundwork for understanding its relevance to real-world NLP applications such as spell checking, plagiarism detection, and genome sequence analysis.', 'concepts': ['Edit Distance', 'Applications of Edit Distance Algorithm in NLP', 'spell checking', 'plagiarism detection', 'genome sequence analysis']}\n",
      "\n",
      "{'title': 'Decoding Edit Distance Calculations in NLP Contexts', 'purpose': 3, 'purpose_statement': \"To provide an in-depth understanding of the Edit Distance concept and its calculation techniques within the Natural Language Processing (NLP) context, focusing on its applications in text analysis problems encountered in the student's academic curriculum.\", 'concepts': ['Edit Distance', 'Levenshtein algorithm', 'Comparative Genomics']}\n",
      "\n",
      "{'title': 'Expanding the Boundaries with Machine Learning in NLP', 'purpose': 2, 'purpose_statement': \"This slide aims to explore the intersection of Machine Learning (ML) and Natural Language Processing (NLP), establishing a foundational understanding of how ML algorithms can enhance NLP tasks such as language modeling, sentiment analysis, and chatbots. The slide also seeks to arouse curiosity and expand the student's knowledge boundaries by introducing the crucial role of ML in modern NLP applications.\", 'concepts': ['Machine Learning', 'language modeling', 'Sentiment Analysis', 'chatbots']}\n",
      "\n",
      "{'title': 'Unveiling the Practicality of Text Normalization in NLP', 'purpose': 4, 'purpose_statement': 'To provide practical examples that demonstrate the application of previous concepts such as Text Normalization and Tokenization and to introduce new related concepts, namely Sentiment Analysis and Speech Recognition, for a deeper theoretical and practical understanding in the context of academic applications.', 'concepts': ['Text Normalization', 'Tokenization of Text', 'Lemmatization', 'Sentiment Analysis', 'Speech Recognition']}\n",
      "\n",
      "{'title': 'Embarking on the Journey of Machine Translation in NLP', 'purpose': 0, 'purpose_statement': 'This slide will serve as a premier introduction to the concept of Machine Translation within the field of Natural Language Processing, setting the stage for students to understand how machines can translate text or speech from one language to another.', 'concepts': ['Machine Translation', 'Algorithms', 'Natural Language Processing (NLP)', 'language modeling', 'text-to-speech']}\n",
      "\n",
      "{'title': 'Mastering the Fundamentals of DNA Sequencing in NLP', 'purpose': 0, 'purpose_statement': 'To introduce the interdisciplinary concepts of DNA sequencing and Bioinformatics and their relation to Natural Language Processing, setting a foundation for understanding the parallels between biological sequence analysis and text processing.', 'concepts': ['DNA sequencing', 'Bioinformatics', 'genetic mapping', 'Comparative Genomics']}\n",
      "\n",
      "{'title': 'Implementing Tokenization and Lemmatization in NLP Workflows', 'purpose': 2, 'purpose_statement': 'This slide aims to provide students with a practical understanding of how Text Tokenization and Lemmatization are implemented in NLP. The focus is on enhancing the student’s practical skills by allowing them to encode these concepts into a workflow, which is crucial for their academic applications and will assist with the NLP class objectives.', 'concepts': ['Tokenization of Text', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Implementing Tokenization and Lemmatization in NLP Workflows', 'purpose': 2, 'purpose_statement': 'This slide aims to provide students with a practical understanding of how Text Tokenization and Lemmatization are implemented in NLP. The focus is on enhancing the student’s practical skills by allowing them to encode these concepts into a workflow, which is crucial for their academic applications and will assist with the NLP class objectives.', 'concepts': ['Tokenization of Text', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Advancing Natural Language Understanding: Text Analysis Essentials', 'purpose': 2, 'purpose_statement': \"This slide aims to bridge foundational NLP concepts with underexplored yet fundamental areas of Text Analysis. It will introduce Text Analysis and elucidate on its various applications in Natural Language Processing, enriching the student's academic curriculum and facilitating better performance in the NLP class through practical, example-driven learning.\", 'concepts': ['Text Analysis', 'language modeling', 'genome', 'Natural Language Processing (NLP)', 'Regular Expressions', 'Text Normalization', 'Edit Distance']}\n",
      "\n",
      "{'title': 'Unlocking Advanced Text Normalization Techniques in NLP', 'purpose': 4, 'purpose_statement': \"To provide practical application and reinforcement of learned concepts in advanced text normalization techniques, enhancing the student's proficiency in text processing for NLP applications.\", 'concepts': ['Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Unlocking Advanced Text Normalization Techniques in NLP', 'purpose': 4, 'purpose_statement': \"To provide practical application and reinforcement of learned concepts in advanced text normalization techniques, enhancing the student's proficiency in text processing for NLP applications.\", 'concepts': ['Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Delving Deeper: Advanced Applications of Edit Distance in NLP', 'purpose': 2, 'purpose_statement': \"This slide aims to build upon the foundational understanding of the Edit Distance concept by exploring its advanced applications in the fields of NLP, specifically focusing on plagiarism detection, genome sequence analysis, and comparative genomics, which have not been fully examined in previous sessions. The goal is to leverage this deeper dive to enhance the student's comprehension for their NLP class and to nourish their interest in wide-ranging practical applications.\", 'concepts': ['Edit Distance', 'plagiarism detection', 'Comparative Genomics', 'genome sequence analysis']}\n",
      "\n",
      "{'title': 'Exploring the Nuances of Sentence Segmentation in Text Analysis', 'purpose': 2, 'purpose_statement': \"This slide will delve into the intricacies of segmenting text into sentences, an important pre-processing step in NLP, and how it paves the way for more advanced text analysis. As we have already touched upon Text Normalization and Tokenization, understanding Sentence Segmentation will enrich the student's comprehension of how texts are prepared for NLP tasks.\", 'concepts': ['Sentence Segmentation Techniques', 'Text Analysis', 'Natural Language Processing (NLP)', 'Regular Expressions']}\n",
      "\n",
      "{'title': 'Exploring the Nuances of Sentence Segmentation in Text Analysis', 'purpose': 2, 'purpose_statement': \"This slide will delve into the intricacies of segmenting text into sentences, an important pre-processing step in NLP, and how it paves the way for more advanced text analysis. As we have already touched upon Text Normalization and Tokenization, understanding Sentence Segmentation will enrich the student's comprehension of how texts are prepared for NLP tasks.\", 'concepts': ['Sentence Segmentation Techniques', 'Text Analysis', 'Natural Language Processing (NLP)', 'Regular Expressions']}\n",
      "{'title': 'Introduction to Regular Expressions in NLP', 'presentation': \"Good day! In this lesson, we will embark on a journey to understand the fascinating world of Regular Expressions (Regex) in the context of Natural Language Processing (NLP). Regular Expressions are incredibly powerful tools used for pattern matching and text processing tasks in NLP. They serve as a sequence of characters that define a search pattern, allowing us to perform complex operations on text. Today, our focus will be on the syntax and operators that form the foundation of Regular Expressions.\\n\\nRegular Expressions consist of various special characters that enable us to describe patterns in text. We will explore some of the most commonly used operators, including literals, wildcards, character classes, quantifiers, anchors, and grouping constructs.\\n\\nFor example, literals are used to search for exact matches of characters or strings. Wildcards allow us to match any character in a certain position. Character classes enable us to define sets of characters to be matched. Quantifiers specify the number of occurrences of a character or a group. Anchors help us identify patterns at the beginning or end of a line. Lastly, grouping constructs allow us to create subexpressions within a larger pattern.\\n\\nTo better understand these concepts, let's consider a practical example that resonates with your interest in chatbots. Imagine we want to create a chatbot that responds to user queries about weather conditions. By using Regular Expressions, we can define specific patterns to recognize weather-related questions. For example, we can use the wildcard operator to match any word followed by the word 'weather,' thus identifying queries like 'What's the weather like today?' or 'How's the weather in New York?'\\n\\nBy mastering the syntax and operators of Regular Expressions, you will gain a solid foundation for numerous NLP applications. These applications include text searching, text substitution, data validation, and various text normalization techniques like Tokenization and Sentence Segmentation. As we progress through this series of lessons, we will delve into practical examples and hands-on exercises that leverage Regular Expressions in modern NLP scenarios.\\n\\nI hope you're as excited as I am to dive into this topic. So without further ado, let's get started by exploring the syntax and operators of Regular Expressions!\", 'content': 'Title: Introduction to Regular Expressions in NLP\\n\\n- Regular Expressions (Regex) are a powerful tool used for pattern matching in Natural Language Processing (NLP).\\n- The basic syntax and operators of Regex form the foundation of pattern matching.\\n- Common operators include literals, wildcards, character classes, quantifiers, anchors, and grouping constructs.\\n- Regex can be used for various NLP tasks such as text searching, data validation, and text normalization.\\n- Understanding the syntax and operators of Regex is essential for developing algorithms in NLP.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'This slide will serve as an entry point to understanding Regular Expressions (Regex) and how they are used in the context of Natural Language Processing (NLP). It aims to establish foundational knowledge for students with no prior experience, as specified in the Notebank. The focus will be on familiarizing the student with the syntax and basic operators of regex, setting the foundation for understanding its applications in NLP tasks.', 'concepts': ['Basic Syntax and Operators of Regular Expressions', 'Regular Expressions']}\n",
      "\n",
      "{'title': 'Fundamentals of Text Normalization in NLP', 'presentation': \"Hello! In this slide, we will be discussing the fundamentals of text normalization in the context of Natural Language Processing (NLP). So let's dive in!\\n\\nText normalization is a crucial process in NLP that involves converting text into a more uniform format. It helps to ensure consistency in language data, making it easier for computers to process and analyze. Think of it as preparing raw text for further NLP tasks like language modeling or sentiment analysis.\\n\\nThe first concept we'll cover is tokenization of text. This is all about breaking down text into smaller elements like words or phrases. It's an essential step before further processing, such as parsing or Part-of-Speech tagging. Tokenization helps in reducing inflectional forms and sometimes derivationally related forms of a word to a common base form. For example, let's take the sentence 'I love cats and dogs.' Tokenization would split it into individual tokens: ['I', 'love', 'cats', 'and', 'dogs'].\\n\\nMoving on, lemmanization is another important aspect of text normalization. It involves reducing a word to its base or root form, called a lemma. Unlike stemming, which simply chops off word endings, lemmatization takes into account the vocabulary and morphological analysis of words. This improves accuracy and is critical for understanding contextual meaning in NLP tasks such as text-to-speech, machine translation, and sentiment analysis.\\n\\nAnother concept we'll explore is sentence segmentation techniques. These methods and algorithms are used to divide a text into its constituent sentences. The goal is to accurately identify sentence boundaries, even in the presence of punctuation or formatting challenges. Common techniques involve using punctuation cues, capitalization, machine learning models, and rules-based systems that consider the linguistic and contextual structure of the text.\\n\\nTo wrap things up, it's important to understand that text normalization plays a significant role in improving the efficiency and accuracy of NLP tasks. It reduces lexical variety and complexity in text, making it more accessible for further analysis. By standardizing and preparing data using text normalization techniques, we can unlock the full potential of NLP in various applications, like text-to-speech synthesis, machine translation, and sentiment analysis.\\n\\nI hope you now have a better understanding of the fundamentals of text normalization in NLP. If you have any questions or would like to explore some practical examples, feel free to ask!\", 'content': 'Slide Content:\\n- What is Text Normalization in NLP?\\n    - The process of converting text into a more uniform format to enhance computational handling.\\n- Tokenization of Text\\n    - Breaking down text into smaller elements like words or phrases.\\n- Lemmatization\\n    - Reducing words to their base or root forms to improve contextual understanding.\\n- Sentence Segmentation Techniques\\n    - Methods for accurately identifying sentence boundaries despite punctuation and formatting challenges.\\n- Significance of Text Normalization in NLP\\n    - Facilitating efficient and accurate NLP tasks like text-to-speech synthesis, machine translation, and sentiment analysis.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'This slide aims to introduce the concept of Text Normalization within the realm of Natural Language Processing (NLP). It will explain what Text Normalization is, why it is critical for processing natural languages, and how it aids in preparing data for further NLP tasks such as Language Modeling or Sentiment Analysis.', 'concepts': ['Text Normalization', 'Tokenization of Text', 'Sentence Segmentation Techniques', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Getting Started with Edit Distance in Natural Language Processing', 'presentation': \"Welcome to an exciting start in the world of Natural Language Processing, or NLP for short. Today, we're going to unveil a fundamental concept that acts as a linchpin for various tasks within this field – the Edit Distance. Imagine you're texting a friend and make a typo. Interestingly, the mechanisms that allow your phone to correct your spelling are rooted in the concept we're exploring today. Edit Distance isn't just about fixing errors. It's about understanding and enabling a myriad of operations that make our interactions with technology smarter. So let's dive in and discover how a simple measure of difference between text strings can have far-reaching implications in technology and beyond.\", 'content': 'In this slide, we will introduce the concept of Edit Distance in Natural Language Processing (NLP). Edit Distance is a measure of the minimum number of operations required to transform one string into another. It is used in various NLP tasks such as spell checking, plagiarism detection, and DNA sequence analysis in bioinformatics. Edit Distance plays a crucial role in comparing and correcting strings within NLP systems, making it an essential concept to understand in the field.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'To provide an introduction to the concept of Edit Distance in Natural Language Processing (NLP), emphasizing its definition and utility within the field.', 'concepts': ['Applications of Edit Distance Algorithm in NLP']}\n",
      "\n",
      "{'title': 'Harnessing the Power of Regular Expressions in NLP', 'presentation': \"Good day! Today, we will explore the fascinating world of regular expressions and their application in Natural Language Processing (NLP). Regular expressions are powerful tools that allow us to effectively search for and match patterns within text. They play a key role in various NLP tasks, such as data extraction, cleaning, and text analysis. So, let's dive in and see how regular expressions can be harnessed in NLP.\\n\\nTo start, let's focus on the basics of regular expressions and their significance in text analysis and cleaning. Regular expressions are like specialized codes that help us extract specific information from large volumes of text. They enable us to create patterns to match words, phrases, or any other desired text segments. For example, if you want to extract phone numbers from a dataset, we can use regular expressions to define a pattern that matches the desired format of a phone number.\\n\\nNow, let's move on to some practical examples. Imagine you have a massive collection of text data and you want to extract specific information, such as all the dates mentioned within the text. By using regular expressions, you can define a pattern that matches the format of a date, allowing you to efficiently extract all the dates in one go. This is just one example of the power regular expressions can bring to NLP tasks!\\n\\nAnother interesting application of regular expressions in NLP is sentiment analysis. Sentiment analysis involves determining the emotional tone of a piece of text, whether it's positive, negative, or neutral. By using regular expressions, we can filter and process user reviews or social media posts to identify the sentiment expressed. For example, we can define patterns that match specific positive or negative words, allowing us to quickly classify the sentiment of a given text sample.\\n\\nRegular expressions are also crucial in text normalization, which involves standardizing and transforming text data for various NLP tasks. One aspect of text normalization is sentence segmentation, where we split a paragraph into individual sentences. Regular expressions can help us detect sentence boundaries based on specific punctuation marks or capitalization patterns.\\n\\nNow, let's explore some practical exercises together. I will provide you with different scenarios related to chatbots, where regular expressions can be utilized to extract specific information from user queries. You will have the opportunity to write your own regular expressions based on the given task. This hands-on practice will enhance your understanding and reinforce the knowledge you've learned so far.\\n\\nTo summarize, regular expressions are indispensable in NLP, enabling us to efficiently search, match, and extract information from text data. They have wide-ranging applications, including text analysis, sentiment analysis, and text normalization. By the end of this lesson, you will gain a thorough understanding of regular expressions and how they can be practically applied in NLP tasks. So, let's get started and put our regular expression skills to the test!\", 'content': 'Slide 1: Harnessing the Power of Regular Expressions in NLP\\n\\n- Introduction to Regular Expressions and their application in Natural Language Processing (NLP)\\n- Basics of regular expressions for text analysis, cleaning, and normalization\\n- Practical examples showcasing the use of regular expressions in tasks such as data extraction, sentiment analysis, and text segmentation\\n- Hands-on exercises for students to write their own regular expressions\\n- Discussion on the practical relevance of regular expressions in modern NLP applications, including chatbots and speech recognition', 'latex_codes': '', 'purpose': 4, 'purpose_statement': \"This slide aims to provide practical examples and exercises demonstrating the application of Regular Expressions within various NLP tasks, reinforcing theoretical knowledge through hands-on learning experiences to solidify the student's understanding.\", 'concepts': ['Uses of Regular Expressions in NLP']}\n",
      "\n",
      "{'title': 'Understanding Edit Distance and Its Relevance in NLP Applications', 'presentation': \"Hey there! Today, we're going to delve into the concept of Edit Distance and its relevance in the field of Natural Language Processing (NLP). Edit Distance is a way to measure how different two strings, like words or sentences, are from each other based on the minimum number of operations required to transform one string into the other. These operations can include inserting, deleting, or substituting a single character.\\n\\nTo help us understand the concept better, let's look at an example. Consider the words 'kitten' and 'sitting'. These two words have an edit distance of 3. Here's why: we need to substitute 'k' with 's', 'e' with 'i', and insert 'g' to transform 'kitten' into 'sitting'.\\n\\nNow, let's talk about the practical use cases of Edit Distance in NLP. One important application is spell checking. Edit Distance helps in comparing words against a dictionary of correct spellings. By calculating the edit distance between a misspelled word and correctly spelled words, we can suggest possible correct spellings and improve the accuracy of spell-checking systems.\\n\\nAnother area where Edit Distance is utilized is plagiarism detection. Here, it is used to compare documents and measure their similarity. By calculating the edit distance between two texts, we can identify instances of text copying without proper attribution.\\n\\nIn addition to that, Edit Distance also has applications in genome sequence analysis. It helps in examining the sequence of DNA in a genome to understand its structure, function, and evolution. Scientists can use Edit Distance to identify different genomes, determine their functions, and detect mutations that may lead to diseases.\\n\\nOverall, understanding Edit Distance is crucial in various NLP tasks such as spell checking, plagiarism detection, and genome sequence analysis. It allows us to quantify the similarity between strings and provides a foundation for building smarter systems in the field of NLP, like chatbots and text-to-speech. So, let's dive deeper into the world of Edit Distance and explore its practical implementation in modern NLP applications. Does that make sense? Let me know if you have any questions!\", 'content': 'Title: Understanding Edit Distance and Its Relevance in NLP Applications\\n\\n- Edit Distance is a way of quantifying the dissimilarity between two strings by counting the minimal number of operations required to transform one string into another.\\n\\n- It is a vital concept in Natural Language Processing (NLP) used in applications such as spell checking, plagiarism detection, and genome sequence analysis.\\n\\n- Edit Distance is calculated by considering the operations of insertion, deletion, and substitution of characters.\\n\\n- In spell checking, Edit Distance is used to compare words against a dictionary for accurate text representation.\\n\\n- Edit Distance is also crucial in genome sequence analysis where it helps to measure the mutation variations in DNA and identify potential diseases.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'To introduce the concept of Edit Distance in the context of Natural Language Processing (NLP), laying the groundwork for understanding its relevance to real-world NLP applications such as spell checking, plagiarism detection, and genome sequence analysis.', 'concepts': ['Edit Distance', 'Applications of Edit Distance Algorithm in NLP', 'spell checking', 'plagiarism detection', 'genome sequence analysis']}\n",
      "\n",
      "{'title': 'Decoding Edit Distance Calculations in NLP Contexts', 'presentation': \"Good day! Today, we'll be diving into the fascinating world of Edit Distance calculations in the context of Natural Language Processing (NLP). This concept holds vital significance in tasks such as spell checking, plagiarism detection, and even genome sequence analysis. So, let's explore the intricacies of Edit Distance and how it plays a crucial role in the NLP domain.\\n\\nTo begin, let's define Edit Distance. In simple terms, it measures the dissimilarity between two strings, such as words, by counting the minimum number of operations required to transform one string into the other. These operations include insertions, deletions, or substitutions of a single character. Understanding Edit Distance is essential for various NLP tasks, as it enables us to quantify the similarity or difference between pieces of text.\\n\\nNow, let's delve into the Levenshtein algorithm, which is the algorithm used to compute Edit Distance. This algorithm calculates the minimum number of single-character edits required to change one word into another. It follows a step-by-step process to evaluate the difference between two sequences, whether they are words or genetic sequences. By counting the number of insertions, deletions, and substitutions, we can determine the Edit Distance.\\n\\nMoving on, we'll explore the practical applications of the Edit Distance algorithm in the context of Comparative Genomics. Comparative genomics is a field where we compare the genomic features of different organisms. It helps us understand the structure, function, and evolutionary processes that affect genomes. By aligning genome sequences and using the Edit Distance algorithm, we can identify similarities and differences, ultimately inferring the evolutionary relationships between species.\\n\\nTo make this concept more relatable, imagine you're working on a research project involving genome sequence analysis. By using the Edit Distance algorithm, you'll be able to compare DNA sequences and identify the genetic variations that distinguish one species from another. This knowledge is invaluable in fields such as Bioinformatics, genetic mapping, and DNA sequencing.\\n\\nThroughout this presentation, we'll illustrate Edit Distance with graphical representations and practical examples relevant to your studies in Natural Language Processing. By the end of this lesson, you will have gained a solid theoretical foundation in Edit Distance calculations and understand its practical applications in modern NLP.\\n\\nNow that we have introduced the topic, let's continue our journey into the world of Edit Distance calculations in NLP! If you have any questions or need further clarification along the way, feel free to ask. Remember, the goal is for you to gain a profound understanding of this concept to excel in your NLP course.\", 'content': 'Slide Content\\n\\n- Slide Title: Decoding Edit Distance Calculations in NLP Contexts\\n\\n- Edit Distance: In the context of Natural Language Processing (NLP), Edit Distance is a way of quantifying how dissimilar two strings (e.g., words) are. It counts the minimum number of operations required to transform one string into the other, such as insertions, deletions, or substitutions of a single character. Edit Distance is essential for tasks like spell checking, plagiarism detection, and genome sequence analysis.\\n\\n- Levenshtein algorithm: The Levenshtein algorithm, also known as the Levenshtein distance, is a measure of the difference between two sequences. It calculates the minimum number of single-character edits (insertions, deletions, or substitutions) needed to change one word into the other. The Levenshtein algorithm is a key component of Edit Distance calculations in applications like spell checking, plagiarism detection, and genome sequence analysis.\\n\\n- Comparative Genomics: Comparative genomics is the field of biological research that compares the genomic features of different organisms. It plays a crucial role in genome sequence analysis, helping to understand the structure, function, and evolutionary processes that impact genomes. Comparative genomics involves aligning genome sequences and identifying similarities and differences to infer the evolutionary relationships between species. It intersects with disciplines like Bioinformatics, genetic mapping, and DNA sequencing.', 'latex_codes': '', 'purpose': 3, 'purpose_statement': \"To provide an in-depth understanding of the Edit Distance concept and its calculation techniques within the Natural Language Processing (NLP) context, focusing on its applications in text analysis problems encountered in the student's academic curriculum.\", 'concepts': ['Edit Distance', 'Levenshtein algorithm', 'Comparative Genomics']}\n",
      "\n",
      "{'title': 'Expanding the Boundaries with Machine Learning in NLP', 'presentation': \"Welcome to the slide on 'Expanding the Boundaries with Machine Learning in NLP'. This slide aims to explore the intersection of Machine Learning (ML) and Natural Language Processing (NLP) and introduce the role of ML in enhancing NLP tasks.\\n\\nMachine Learning is a branch of Artificial Intelligence that enables systems to learn and improve from experience without explicit programming. It involves developing algorithms that can make predictions or decisions based on data. In the context of NLP, ML plays a crucial role in various aspects.\\n\\nOne of the key areas where ML enhances NLP is language modeling. Language modeling is the task of predicting the likelihood of word sequences in natural language. For example, ML algorithms can power auto-complete features or assist in predictive texting. By understanding the context and patterns within language, these models can generate more accurate and contextual predictions.\\n\\nAnother NLP task where ML shines is sentiment analysis. Sentiment analysis involves analyzing textual data to extract subjective information, such as opinions, attitudes, and emotions. ML algorithms can classify texts into categories like positive, negative, or neutral sentiment, providing insights into consumer attitudes in fields like business analytics and customer feedback.\\n\\nLastly, ML algorithms play a crucial role in the development of chatbots. Chatbots are automated dialogue systems that use NLP techniques to simulate conversational experiences. ML algorithms enable chatbots to understand user input, process it through techniques like Regular Expressions, Text Normalization, and Edit Distance, and generate appropriate responses. They find applications in customer service, personal assistants, and language modeling scenarios.\\n\\nIn summary, the slide demonstrates the powerful synergy between Machine Learning and Natural Language Processing. By leveraging ML algorithms, NLP tasks like language modeling, sentiment analysis, and chatbots can be significantly enhanced. Through this exploration, we hope to expand your knowledge boundaries and deepen your understanding of the role of ML in modern NLP applications.\", 'content': '1. Title: Expanding the Boundaries with Machine Learning in NLP\\n\\n2. Definition of Machine Learning as a subset of AI that enables systems to learn and improve from experience without explicit programming\\n\\n3. Explanation of language modeling as the task of predicting the likelihood of word sequences in natural language and its applications in speech recognition, machine translation, and text generation\\n\\n4. Overview of sentiment analysis, which involves the use of ML algorithms to extract subjective information from textual data, including examples of positive, negative, and neutral sentiment\\n\\n5. Exploration of chatbot technology and its utilization of ML techniques like Regular Expressions, Text Normalization, and Edit Distance to simulate conversational experiences', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide aims to explore the intersection of Machine Learning (ML) and Natural Language Processing (NLP), establishing a foundational understanding of how ML algorithms can enhance NLP tasks such as language modeling, sentiment analysis, and chatbots. The slide also seeks to arouse curiosity and expand the student's knowledge boundaries by introducing the crucial role of ML in modern NLP applications.\", 'concepts': ['Machine Learning', 'language modeling', 'Sentiment Analysis', 'chatbots']}\n",
      "\n",
      "{'title': 'Unveiling the Practicality of Text Normalization in NLP', 'presentation': \"Hey there! Today, we're going to delve into the practicality of Text Normalization in Natural Language Processing (NLP). So, what exactly is Text Normalization? It's a process that involves converting text into a more uniform format, which is crucial for various NLP tasks. This slide will provide practical examples to demonstrate the importance and application of Text Normalization in NLP.\\n\\nLet's start with the concept of Sentiment Analysis. Imagine we have a bunch of texts that we want to classify as positive, negative, or neutral sentiments. Text Normalization plays a crucial role in accurately classifying these texts. We'll explore how techniques like Tokenization, Lemmatization, and Sentence Segmentation, which are part of Text Normalization, can affect sentiment analysis.\\n\\nNext, we'll move on to another exciting application: Speech Recognition. As you know, speech recognition involves converting spoken language into text. Here, too, Text Normalization comes into play. We'll analyze examples of raw spoken language and compare them to their normalized versions. This will help us understand how Text Normalization, along with language modeling and Machine Translation, contributes to efficient speech-to-text technologies.\\n\\nBy connecting previously discussed concepts like Tokenization of Text and Lemmatization to the new concepts of Sentiment Analysis and Speech Recognition, we'll see how these building blocks come together to empower these applications. For example, we'll observe how recognizing different word forms using Lemmatization can impact sentiment extraction.\\n\\nThroughout the presentation, we'll explore not only the theory behind Text Normalization but also the practical considerations and challenges that arise when performing Text Normalization. This will provide a real-world perspective on the application of these techniques.\\n\\nBy the end of this presentation, you'll have a solid understanding of the theory behind Text Normalization and how it's implemented in NLP applications like Sentiment Analysis and Speech Recognition. Get ready to unravel the practicality of Text Normalization and uncover its significance in the world of NLP!\", 'content': '1. Introduction to Text Normalization:\\n- Process of converting text into a uniform format.\\n- Includes Tokenization, Lemmatization, and Sentence Segmentation.\\n2. Practical Example 1: Sentiment Analysis:\\n- Show how Text Normalization enhances sentiment classification.\\n- Demonstrate the impact of Tokenization, Lemmatization, and Sentence Segmentation.\\n3. Practical Example 2: Speech Recognition:\\n- Illustrate the role of Text Normalization in speech-to-text technologies.\\n- Compare raw spoken language with normalized versions.\\n- Address challenges in normalizing slang and colloquial expressions.', 'latex_codes': '', 'purpose': 4, 'purpose_statement': 'To provide practical examples that demonstrate the application of previous concepts such as Text Normalization and Tokenization and to introduce new related concepts, namely Sentiment Analysis and Speech Recognition, for a deeper theoretical and practical understanding in the context of academic applications.', 'concepts': ['Text Normalization', 'Tokenization of Text', 'Lemmatization', 'Sentiment Analysis', 'Speech Recognition']}\n",
      "\n",
      "{'title': 'Embarking on the Journey of Machine Translation in NLP', 'presentation': \"Hello, and welcome to our introductory slide on Machine Translation in the field of Natural Language Processing, or NLP. In this presentation, we will take the first step of our educational journey, exploring how machines can translate text or speech from one language to another.\\n\\nImagine you are standing at the beginning of a vast landscape, with different paths leading to various languages. This visual representation symbolizes the concept of translation and the interconnected nature of global communication.\\n\\nAs we delve into the topic of Machine Translation, it's important to understand that it is a subfield of NLP. Machine Translation focuses on the problem of automatically translating text or speech using software. We employ complex algorithms and language modeling techniques to achieve accurate and efficient translation.\\n\\nYou might wonder, what is language modeling? Well, language modeling involves predicting the likelihood of a sequence of words in a natural language. It is a fundamental concept in NLP, allowing us to build probabilistic models that can generate or determine the probability distribution of linguistic units, such as words or sentences. Language modeling is the backbone of various NLP applications, including machine translation.\\n\\nThink of language modeling as a tool that empowers machines to 'understand' human languages and generate coherent translations. This technology has revolutionized our ability to bridge language barriers and enable smooth communication across different cultures and regions.\\n\\nMoreover, Machine Translation is just one aspect of NLP. This field encompasses a wide range of techniques and tools, such as text analysis, sentiment analysis, and speech recognition. It serves as the foundation for applications like speech-to-text systems, chatbots, and text-to-speech technology.\\n\\nTo give you a glimpse of the practical side of NLP, imagine a real-time translation app on your phone. You speak into your device in one language, and the app automatically translates your speech into another language. Amazing, right?\\n\\nAs we continue our journey through the world of NLP, we'll explore the fascinating concepts of Regular Expressions, Text Normalization, and Edit Distance, which all contribute to Machine Translation and play crucial roles in understanding and processing human language.\\n\\nBy the end of this presentation, I hope you have gained a clear understanding of Machine Translation as a subfield of NLP and its vital role in facilitating cross-linguistic communication. We are excited to continue this educational adventure with you as we explore the depths of NLP and its vast applications. Let's get started!\", 'content': 'This slide serves as a premier introduction to the concept of Machine Translation within the field of Natural Language Processing. Machine Translation (MT) is a subfield of Natural Language Processing (NLP) that focuses on the problem of automatically translating text or speech from one language to another. It involves the use of software and complex algorithms to perform the translation task. MT can be approached in several ways, including rule-based, statistical, and neural methods. Language modeling is a fundamental concept in NLP, which involves developing probabilistic models that can generate or determine the probability distribution of linguistic units. TTS (Text-to-Speech) is often used in NLP for applications such as assistive technology, speech recognition, and chatbots. The ultimate objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both meaningful and useful.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'This slide will serve as a premier introduction to the concept of Machine Translation within the field of Natural Language Processing, setting the stage for students to understand how machines can translate text or speech from one language to another.', 'concepts': ['Machine Translation', 'Algorithms', 'Natural Language Processing (NLP)', 'language modeling', 'text-to-speech']}\n",
      "\n",
      "{'title': 'Mastering the Fundamentals of DNA Sequencing in NLP', 'presentation': \"Hey there! Welcome to the slide on mastering the fundamentals of DNA sequencing in NLP. In this slide, we will introduce you to the fascinating world of DNA sequencing and its relation to natural language processing. Are you ready to dive in? Great! Let's get started.\\n\\nDNA sequencing is the process of determining the precise order of nucleotides within a DNA molecule. It's like reading the genetic code of life! By knowing the order of the four bases—adenine, guanine, cytosine, and thymine—we can unlock valuable information about genes, genetic traits, and even diseases. This process has revolutionized the field of genomics and has numerous applications in genetic testing, biomedical research, and forensic biology.\\n\\nNow, let's talk about bioinformatics. Bioinformatics is an interdisciplinary field that combines computer science, biology, chemistry, statistics, and mathematics. It helps us interpret and analyze biological data, including the data obtained through DNA sequencing. By using computational methods and algorithms, bioinformatics allows us to make sense of complex genetic information and understand how genes and genomes work.\\n\\nOne important application of DNA sequencing is genetic mapping. Genetic mapping involves determining the location and chemical sequence of specific genes on a DNA strand. With the help of genome sequence analysis, scientists can associate particular segments of DNA with specific characteristics or diseases. This helps in tasks like genome editing, plagiarism detection in academic research, and understanding the genetic basis of diseases.\\n\\nAnother fascinating area is comparative genomics. Comparative genomics is all about comparing the genomic features of different organisms. By aligning genome sequences and identifying similarities and differences, scientists can gain insights into the structure, function, and evolutionary relationships between species. Comparative genomics intersects with bioinformatics, genetic mapping, and DNA sequencing, creating a synergy of knowledge and discoveries.\\n\\nNow, how does all of this relate to natural language processing? Well, believe it or not, there are parallels between DNA sequencing and text processing. Just like in comparative genomics, text processing in NLP involves using algorithms and methods like regular expressions for pattern matching and edit distance for measuring similarity between text sequences. Genetic mapping's resemblance to plagiarism detection tools in NLP shows how both fields use algorithms to identify patterns and copy-paste within datasets.\\n\\nSo, the fundamentals of DNA sequencing in NLP lay a solid foundation for understanding how computational methods can help us explore and exploit the vast amounts of genomic and textual data. With this knowledge, you'll be prepared to dive deeper into the fascinating world of genomics and natural language processing. I hope you're as excited as I am to explore this fascinating intersection of fields!\", 'content': 'Title: Mastering the Fundamentals of DNA Sequencing in NLP\\n\\n1. Introduction to DNA Sequencing: DNA sequencing is the process of determining the order of nucleotides in a DNA molecule. It has revolutionized fields like genomics, genetic testing, biomedical research, and forensic biology.\\n\\n2. Bioinformatics: Bioinformatics is an interdisciplinary field that combines computer science, biology, chemistry, statistics, and mathematics to analyze and interpret biological data. It is essential for genome sequence analysis and DNA sequencing.\\n\\n3. Genetic Mapping: Genetic mapping involves determining the location and chemical sequence of specific genes on a DNA strand. It aids in tasks like genome editing, plagiarism detection, and understanding the genetic basis of diseases.\\n\\n4. Comparative Genomics: Comparative genomics compares the genomic features of different organisms to understand their structure, function, and evolutionary processes. It intersects with bioinformatics, genetic mapping, and DNA sequencing.\\n\\n5. DNA Sequencing in NLP: DNA sequencing and NLP share similarities, such as the use of regular expressions for pattern matching and edit distance for measuring similarity between sequences of text. These connections bring together computational methods with biological and textual datasets.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'To introduce the interdisciplinary concepts of DNA sequencing and Bioinformatics and their relation to Natural Language Processing, setting a foundation for understanding the parallels between biological sequence analysis and text processing.', 'concepts': ['DNA sequencing', 'Bioinformatics', 'genetic mapping', 'Comparative Genomics']}\n",
      "\n",
      "{'title': 'Implementing Tokenization and Lemmatization in NLP Workflows', 'presentation': \"Good day! Today, we will delve into the intriguing world of Natural Language Processing, focusing on the implementation of two essential concepts: Tokenization of Text and Lemmatization. These concepts are fundamental for various NLP tasks, making them crucial to understand for our academic objectives. So, let's get started!\\n\\nTo begin, let's clarify the purpose of tokenization. It is the process of breaking down a sequence of characters into individual tokens, which can be words, phrases, symbols, or other meaningful elements. Tokenization is an initial step that allows us to analyze text further, like parsing or part-of-speech tagging. It plays a critical role in text normalization by reducing inflectional and derivational forms of words to a common base form.\\n\\nMoving on to lemmatization, it involves reducing a word to its base or root form, known as a lemma. Unlike stemming, which simply chops off word endings, lemmatization takes into account the vocabulary and morphological analysis of words, making it more sophisticated and accurate. Lemmatization is essential for various NLP tasks such as text-to-speech, machine translation, and sentiment analysis.\\n\\nNow, let's explore how tokenization and lemmatization are implemented in NLP workflows. Imagine we're working on a chatbot that needs to understand and respond to user inputs. The tokenization process would break down the user's message into individual words or tokens, allowing the chatbot to analyze and interpret each word effectively. Following that, lemmatization would then convert these tokens to their base form, ensuring a standardized representation of the text for further processing and analysis.\\n\\nTo demonstrate the practical application of these concepts, let's take a practical example. Suppose we have the following sentence: 'The cats are playing in the garden.' First, tokenization would split this sentence into individual tokens, resulting in ['The', 'cats', 'are', 'playing', 'in', 'the', 'garden']. Then, lemmatization would convert these tokens to their base form, giving us ['the', 'cat', 'be', 'play', 'in', 'the', 'garden']. By normalizing the text through tokenization and lemmatization, we derive a structured representation of the sentence that can be used for various NLP tasks.\\n\\nIn summary, tokenization and lemmatization are key concepts in NLP. Tokenization helps us break down text into meaningful units, while lemmatization ensures each token is reduced to its base form. By understanding and implementing these concepts, we can enhance our NLP workflows and effectively process and analyze text data for different applications. I hope this practical understanding will enable you to excel in your NLP studies and perform well in your academic pursuits. Thank you!\", 'content': '1. Tokenization of Text is the process of converting a sequence of characters into tokens, such as words, phrases, or symbols. It is an essential step in NLP for further processing and text normalization.\\n2. Lemmatization involves reducing a word to its base or root form, called a lemma, considering the vocabulary and morphology. It plays a crucial role in various NLP tasks like text-to-speech, machine translation, and sentiment analysis.\\n3. Tokenization and Lemmatization are vital in NLP workflows, ensuring text analysis and normalization, reducing complexity and variability of natural language.\\n4. We can use a popular NLP library like NLTK or spaCy to implement tokenization and lemmatization effectively.\\n5. A practical example will be demonstrated, showcasing the implementation of tokenization and lemmatization using code, highlighting the impact on text normalization and analysis.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': 'This slide aims to provide students with a practical understanding of how Text Tokenization and Lemmatization are implemented in NLP. The focus is on enhancing the student’s practical skills by allowing them to encode these concepts into a workflow, which is crucial for their academic applications and will assist with the NLP class objectives.', 'concepts': ['Tokenization of Text', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Implementing Tokenization and Lemmatization in NLP Workflows', 'presentation': \"Good afternoon! On this slide, we're going to explore the fascinating world of Natural Language Processing (NLP) and dive into two important concepts: Tokenization of Text and Lemmatization. These processes play a crucial role in NLP workflows and have practical applications in various fields.\\n\\nTokenization is the first step in NLP, where we break down a sequence of characters into smaller units called tokens. These tokens can be words, phrases, symbols, or other meaningful elements. Ultimately, tokenization allows us to convert text into a format suitable for further analysis, such as parsing and part-of-speech tagging.\\n\\nLemmatization, on the other hand, goes beyond simple stemming by reducing words to their base or root form, called a lemma. This process takes into account the word's morphological analysis and its part of speech in a sentence. It's a more sophisticated approach that ensures higher accuracy when normalizing words.\\n\\nNow, let's move beyond theory and look at the practical side of things. We'll explore how tokenization and lemmatization are implemented in NLP workflows. For example, we'll show you code snippets and workflow diagrams to demonstrate how these processes are used in tasks like text-to-speech, machine translation, and sentiment analysis.\\n\\nBy the end of this slide, you'll have a solid understanding of how tokenization and lemmatization fit into the bigger picture of NLP. You'll be able to connect the theoretical concepts with real-world applications, and see how they contribute to successful outcomes in various NLP tasks. So, let's dive in and explore the fascinating world of tokenization and lemmatization in NLP!\", 'content': '1. Tokenization of Text: The process of converting text into a sequence of tokens, which are smaller units such as words or phrases. Tokenization is a fundamental step in NLP and helps with tasks like parsing and Part-of-Speech tagging.\\n\\n2. Lemmatization: A process in NLP that reduces words to their base or dictionary form, known as a lemma. Unlike stemming, lemmatization considers the vocabulary and morphological analysis, resulting in higher accuracy.\\n\\n3. Practical Application: Tokenization and lemmatization are implemented in NLP workflows. They play a crucial role in text normalization and are used in tasks like text-to-speech, machine translation, and sentiment analysis.\\n\\n4. Code Examples: Code snippets and workflow diagrams will demonstrate how to implement tokenization and lemmatization in an NLP workflow.\\n\\n5. Academic Relevance: Understanding and applying tokenization and lemmatization are important for succeeding in NLP coursework and future NLP projects.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': 'This slide aims to provide students with a practical understanding of how Text Tokenization and Lemmatization are implemented in NLP. The focus is on enhancing the student’s practical skills by allowing them to encode these concepts into a workflow, which is crucial for their academic applications and will assist with the NLP class objectives.', 'concepts': ['Tokenization of Text', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Advancing Natural Language Understanding: Text Analysis Essentials', 'presentation': \"Welcome to the slide titled 'Advancing Natural Language Understanding: Text Analysis Essentials'. In this slide, we will delve into the intriguing world of Text Analysis as part of Natural Language Processing (NLP). Our aim is to enhance your understanding and practical knowledge of this field.\\n\\nText Analysis involves extracting meaningful information from natural language text. It comprises various methodologies and technologies used in NLP, such as Regular Expressions, Text Normalization, and understanding the Edit Distance between strings. By using these techniques, we can structure unstructured text data and perform tasks like sentiment analysis, topic modeling, and named entity recognition.\\n\\nOne of the fundamental concepts in NLP is language modeling. Language modeling focuses on predicting the likelihood of word sequences in a natural language. This concept serves as the basis for various NLP applications, including speech recognition, machine translation, and text generation.\\n\\nIn this slide, we will explore the key concepts of Regular Expressions, Text Normalization, and Edit Distance in the context of NLP. Regular Expressions are a powerful tool for text processing, allowing us to define search patterns with various basic syntax and operators. These patterns are used in tasks such as text searching, data validation, and tokenization of text.\\n\\nText Normalization, on the other hand, involves converting text into a more uniform format. It includes tasks like tokenization of text, lemmatization, and sentence segmentation techniques. Text Normalization helps in reducing lexical variety and complexity, making NLP tasks more efficient and accurate.\\n\\nEdit Distance is a measure of how dissimilar two strings (e.g., words) are by counting the minimum number of operations required to transform one string into the other. It is essential for tasks such as spell checking, plagiarism detection, and genome sequence analysis. We will explore the calculation of Edit Distance and its applications in NLP.\\n\\nThroughout the slide, we will provide practical examples and exercises to help solidify your understanding of these concepts. We will also discuss their applications in modern NLP, such as chatbots.\\n\\nBy the end of this slide, you will have a solid foundation in the key concepts of text analysis in NLP, and you will be equipped with the knowledge necessary to excel in your NLP class. Let's dive in and discover the fascinating world of Text Analysis!\", 'content': '1. Introduction to Text Analysis: Extracting meaningful information from natural language text using various methodologies and technologies in NLP.\\n\\n2. Regular Expressions: Powerful tools for text processing, allowing the specification of complex search patterns for tasks like text searching and substitution.\\n\\n3. Text Normalization: Process of converting text into a more uniform format, including tasks such as tokenization, lemmatization, and sentence segmentation.\\n\\n4. Edit Distance: Measure of dissimilarity between strings, used in spell checking, plagiarism detection, and genome sequence analysis.\\n\\n5. Practical Examples and Applications: Demonstration of how these concepts are applied in modern NLP applications like chatbots, with hands-on exercises and exercises to reinforce the understanding of the material.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide aims to bridge foundational NLP concepts with underexplored yet fundamental areas of Text Analysis. It will introduce Text Analysis and elucidate on its various applications in Natural Language Processing, enriching the student's academic curriculum and facilitating better performance in the NLP class through practical, example-driven learning.\", 'concepts': ['Text Analysis', 'language modeling', 'genome', 'Natural Language Processing (NLP)', 'Regular Expressions', 'Text Normalization', 'Edit Distance']}\n",
      "\n",
      "{'title': 'Unlocking Advanced Text Normalization Techniques in NLP', 'presentation': \"Welcome to the slide 'Unlocking Advanced Text Normalization Techniques in NLP'. In this slide, we will explore advanced techniques used in Natural Language Processing or NLP. These techniques are essential for text processing in NLP applications and involve regular expressions, text normalization, and edit distance.\\n\\nLet's begin by talking about regular expressions. Regular expressions are powerful tools used to match patterns in text. They play a crucial role in text cleaning and preprocessing for NLP. You can think of them as a kind of language for specifying patterns in text. Regular expressions consist of a combination of symbols and metacharacters that allow you to perform various operations on a text.\\n\\nThe use of regular expressions in NLP extends beyond just basic text matching. They enable us to perform tasks like tokenization, sentence segmentation, and even more complex operations like named entity recognition. By understanding and utilizing regular expressions, you will gain the ability to process and manipulate text in powerful and efficient ways, which is fundamental in NLP.\\n\\nMoving on to the concept of text normalization. Text normalization is the process of converting text into its canonical or standard form. It involves tasks like tokenization and lemmatization. Tokenization is the process of breaking text into individual tokens or words. This is essential for further analysis as it allows us to work with smaller units of text. Lemmatization, on the other hand, aims to reduce words to their base or root form. This helps to reduce information redundancy and ensures consistent interpretation of text by NLP models.\\n\\nLastly, we will discuss the concept of edit distance. Edit distance is a measure of the similarity between two strings. It quantifies the minimum number of operations required to transform one string into another. This concept is particularly useful in tasks like spell-checking systems, plagiarism detection, and even DNA sequence alignment. By understanding edit distance, you will gain insight into how similarity among texts can be measured and how it can be applied in various NLP applications.\\n\\nTo reinforce your understanding of these techniques, we will provide practical examples and real-world applications using regular expressions, text normalization, and edit distance. We will explore how these concepts can be applied in tasks like chatbot development, where understanding and normalizing user input are crucial for effective communication.\\n\\nBy the end of this slide, you will have a solid understanding of advanced text normalization techniques in NLP and their practical applications. You will be well-equipped to work with text data and apply these techniques in your own NLP projects. So let's dive in and unlock the world of advanced text normalization in NLP!\", 'content': '1. Introduction to Regular Expressions in NLP, covering basic syntax and operators. \\n\\n2. Practical examples showcasing Regular Expressions in text normalization, including sentence segmentation and tokenization. \\n\\n3. Importance of lemmatization and its role in reducing information redundancy and ensuring consistent text interpretation. \\n\\n4. Exploration of Edit Distance concept, its formal equation, and real-world applications such as spell-check systems and plagiarism detection. \\n\\n5. A case study demonstrating the practical applications of the discussed concepts in developing a chatbot, highlighting text normalization, regular expressions, and Edit Distance for handling user input variations.', 'latex_codes': '', 'purpose': 4, 'purpose_statement': \"To provide practical application and reinforcement of learned concepts in advanced text normalization techniques, enhancing the student's proficiency in text processing for NLP applications.\", 'concepts': ['Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Unlocking Advanced Text Normalization Techniques in NLP', 'presentation': \"Good day! Today, we'll be exploring the fascinating world of advanced text normalization techniques in Natural Language Processing (NLP). In this session, we'll delve into the concepts of regular expressions, text normalization, and edit distance, understanding how they play a crucial role in NLP applications.\\n\\nTo begin, let's take a quick review of the foundational concepts of regular expressions, text normalization, and edit distance within NLP. Regular expressions are powerful patterns used to search, match, and manipulate text data. Text normalization involves transforming raw text into a standardized format, making it easier to process. Edit distance, on the other hand, measures the similarity between two strings, benefiting tasks like spell checking and duplicate detection.\", 'content': '1. Foundational concepts: Regular Expressions, Text Normalization, and Edit Distance in NLP.\\n2. Application of concepts in modern NLP tasks and their impact on language model performance.\\n3. Practical examples of advanced text normalization techniques, including handling typos, slang, and other irregularities in text data.\\n4. Tokenization strategies and the importance of lemmatization in text preprocessing.\\n5. Edit Distance algorithm, its role in measuring similarity between strings, and its application in spell-checking and duplicate detection.\\n6. Interactive exercises to apply the concepts, particularly in chatbot scenarios.', 'latex_codes': '', 'purpose': 4, 'purpose_statement': \"To provide practical application and reinforcement of learned concepts in advanced text normalization techniques, enhancing the student's proficiency in text processing for NLP applications.\", 'concepts': ['Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Delving Deeper: Advanced Applications of Edit Distance in NLP', 'presentation': \"Good day! In our journey through Natural Language Processing, we've encountered a concept called Edit Distance, which measures the dissimilarity between two text strings. Today, we will delve deeper into Edit Distance and explore its advanced applications in the fields of NLP.\\n\\nOne of the fascinating applications of Edit Distance is in plagiarism detection. By employing Edit Distance algorithms alongside Text Normalization techniques, we can compare textual content and enhance the accuracy of plagiarism detection software. This is particularly important for academic integrity, ensuring that proper credit is given and intellectual property is respected.\\n\\nMoving from the textual realm to the biological realm, Edit Distance also plays a crucial role in genome sequence analysis. In the subfield of Comparative Genomics, Edit Distance helps us understand the evolutionary relationships between different species by aligning genome sequences and identifying similarities and differences. With this powerful computational tool, scientists can gain insights into the structure, function, and evolutionary processes of genomes.\\n\\nTo further solidify your understanding, let's explore some practical examples. Imagine a chatbot that uses Edit Distance to analyze human conversation and respond intelligently. This application showcases the syntactic and semantic understanding that Edit Distance can bring to chatbots, making them more conversational and engaging.\\n\\nSimilarly, Edit Distance is essential for detecting plagiarism in scholarly works, ensuring that originality is preserved and academic honesty is maintained. By applying Edit Distance algorithms, we can identify similar strings of text and determine the level of similarity between documents. This is crucial for verifying the authenticity of written work.\\n\\nAdditionally, Edit Distance is widely used for genome comparison tasks, where it helps researchers analyze genetic sequences and detect mutations that may lead to diseases. By aligning and comparing genome sequences, scientists can gain insights into the structure and function of different genes, ultimately advancing our understanding of human health and biology.\\n\\nOverall, Edit Distance holds great significance in the field of NLP, stretching its arms out to diverse applications in plagiarism detection, genome sequence analysis, and beyond. As you explore these advanced applications, remember that Edit Distance is not just an abstract concept but a powerful tool that spans various disciplines, connecting academia and real-world implications. Let's embrace the depth and breadth of Edit Distance in our exciting journey through Natural Language Processing!\", 'content': '1. Edit Distance:  Edit Distance is a way of quantifying the dissimilarity between two strings by counting the minimum number of operations (insertions, deletions, substitutions) required to transform one string into the other.\\n\\n2. Advanced Applications in NLP:  Edit Distance finds advanced applications in NLP, such as plagiarism detection and genome sequence analysis.\\n\\n3. Plagiarism Detection: Edit Distance algorithms combined with Text Normalization techniques result in more accurate plagiarism detection, aiding in identifying instances where text has been copied without proper authorization and attribution.\\n\\n4. Genome Sequence Analysis: Edit Distance plays a crucial role in Comparative Genomics, offering insights into the genetic relationships between species by identifying similarities and differences in their DNA sequences.\\n\\n5. Real-world Applications: Edit Distance is used in various real-world applications, such as creating chatbots with better syntactic and semantic understanding, enhancing plagiarism detection software, and assisting comparative genomics researchers in understanding evolutionary pathways.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide aims to build upon the foundational understanding of the Edit Distance concept by exploring its advanced applications in the fields of NLP, specifically focusing on plagiarism detection, genome sequence analysis, and comparative genomics, which have not been fully examined in previous sessions. The goal is to leverage this deeper dive to enhance the student's comprehension for their NLP class and to nourish their interest in wide-ranging practical applications.\", 'concepts': ['Edit Distance', 'plagiarism detection', 'Comparative Genomics', 'genome sequence analysis']}\n",
      "\n",
      "{'title': 'Exploring the Nuances of Sentence Segmentation in Text Analysis', 'presentation': \"Alright, let's dive into the fascinating world of sentence segmentation in text analysis. Today, we'll explore the nuances of breaking down text into meaningful sentences, an essential step in Natural Language Processing (NLP). We'll build on what you've already learned about text normalization and tokenization to enhance your understanding of how texts are prepared for NLP tasks. So, let's get started!\\n\\nSentence segmentation is all about dividing a block of text into its constituent sentences. This may sound simple, but it can actually be quite challenging due to various factors like punctuation, capitalization, and linguistic intricacies. To truly grasp the complexities, we'll cover different techniques and strategies used to accurately identify sentence boundaries.\\n\\nFirst, let's understand why sentence segmentation is crucial in text analysis. It lays the foundation for many NLP applications, such as sentiment analysis, topic modeling, and named entity recognition. By breaking text into sentences, we can extract meaningful information from unstructured data.\\n\\nOne technique we'll explore is the use of regular expressions. These powerful search patterns help identify potential sentence boundaries by detecting punctuation patterns. For example, we can look for periods followed by a space and an uppercase letter. We'll dive into the basic syntax and operators of regular expressions and provide practical examples to solidify your understanding.\\n\\nAdditionally, we'll discuss how capitalization cues and linguistic patterns play a role in sentence segmentation. Sometimes, punctuation alone is not enough, and we need to consider contextual elements. We'll examine how linguistic and grammatical patterns can guide the segmentation process.\\n\\nIt's important to note that inaccuracies in sentence segmentation can have a significant impact on downstream NLP tasks. We'll address common challenges and discuss strategies to improve accuracy, combining multiple techniques to achieve better results.\\n\\nTo make the concepts more tangible, we'll explore how machine learning models can be leveraged for sentence segmentation. These models learn from linguistic context beyond predetermined rules and patterns, enhancing the segmentation process. We'll provide visual representations and examples, allowing you to see the segmentation in action.\\n\\nThroughout the presentation, I'll make connections to real-world applications, such as chatbots and text-to-speech systems. This will highlight the practical relevance of precise sentence segmentation and prepare you for scenarios you may encounter in your NLP journey.\\n\\nBy the end of this presentation, you'll have a deeper understanding of sentence segmentation, its challenges, and its importance in NLP. You'll be well-prepared to tackle academic evaluations and apply your knowledge to real-world NLP tasks. So, let's embark on this exploration together and unlock the secrets of sentence segmentation!\", 'content': '1. Introduction to Sentence Segmentation Techniques:\\n- Sentence Segmentation Techniques involve methods and algorithms used in Natural Language Processing (NLP) to divide a text into its constituent sentences.\\n\\n2. Importance of Sentence Segmentation:\\n- Discuss the significance of accurate sentence segmentation as a crucial preprocessing step in NLP tasks.\\n\\n3. Challenges in Sentence Segmentation:\\n- Highlight the complexities posed by punctuation, capitalization, and linguistic structures, and the impact of inaccuracies in subsequent NLP tasks.\\n\\n4. Common Sentence Segmentation Techniques:\\n- Explore techniques such as punctuation cues, capitalization, rule-based systems, and machine learning models.\\n\\n5. Practical Applications of Sentence Segmentation:\\n- Demonstrate the importance of accurate segmentation in modern NLP applications like chatbots and text-to-speech systems.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide will delve into the intricacies of segmenting text into sentences, an important pre-processing step in NLP, and how it paves the way for more advanced text analysis. As we have already touched upon Text Normalization and Tokenization, understanding Sentence Segmentation will enrich the student's comprehension of how texts are prepared for NLP tasks.\", 'concepts': ['Sentence Segmentation Techniques', 'Text Analysis', 'Natural Language Processing (NLP)', 'Regular Expressions']}\n",
      "\n",
      "{'title': 'Exploring the Nuances of Sentence Segmentation in Text Analysis', 'presentation': \"Good day! Today, we are going to explore the nuances of sentence segmentation in text analysis. Sentence segmentation involves dividing a text into its constituent sentences, and it is an important pre-processing step in Natural Language Processing (NLP). By accurately identifying sentence boundaries, we can unlock more advanced text analysis techniques.\\n\\nImagine you have a text that you want to analyze. You need to be able to separate it into individual sentences so you can perform tasks like sentiment analysis, topic modeling, and named entity recognition. For example, let's say you want to analyze customer reviews of a product to determine overall sentiment. Being able to isolate each sentence allows you to assess sentiment on a sentence-by-sentence basis, giving you more granular insights.\\n\\nBut why is sentence segmentation challenging? Punctuation marks can complicate the process. For instance, periods can indicate the end of a sentence, but they can also appear in abbreviations and decimal numbers. Capitalization can also be helpful, but it may not always be a reliable indicator, especially in different languages and text corpora.\\n\\nTo overcome these challenges, various techniques have been developed. One approach is to rely on punctuation cues to identify sentence boundaries. Another is to analyze capitalization patterns in the text. Additionally, there are machine learning models that can be trained to detect sentence boundaries, as well as rule-based systems that consider the linguistic and contextual structure of the text.\\n\\nThink about your objectives as a computer science student and how sentence segmentation can enhance your NLP skills. Mastering this technique will enable you to better understand and analyze text data, empowering you to develop more sophisticated NLP applications in the future.\\n\\nBy the end of this lesson, you will have a solid understanding of sentence segmentation techniques and how they fit into the broader field of text analysis. Are you ready to dive in?\", 'content': '1. Introduction to Sentence Segmentation Techniques\\n2. Challenges in identifying sentence boundaries\\n3. Common methods for sentence segmentation\\n4. Connection to Text Normalization and Tokenization\\n5. Importance of sentence segmentation in advanced text analysis', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide will delve into the intricacies of segmenting text into sentences, an important pre-processing step in NLP, and how it paves the way for more advanced text analysis. As we have already touched upon Text Normalization and Tokenization, understanding Sentence Segmentation will enrich the student's comprehension of how texts are prepared for NLP tasks.\", 'concepts': ['Sentence Segmentation Techniques', 'Text Analysis', 'Natural Language Processing (NLP)', 'Regular Expressions']}\n",
      "{'title': 'Introduction to Regular Expressions in NLP', 'presentation': \"Good day! In this lesson, we will embark on a journey to understand the fascinating world of Regular Expressions (Regex) in the context of Natural Language Processing (NLP). Regular Expressions are incredibly powerful tools used for pattern matching and text processing tasks in NLP. They serve as a sequence of characters that define a search pattern, allowing us to perform complex operations on text. Today, our focus will be on the syntax and operators that form the foundation of Regular Expressions.\\n\\nRegular Expressions consist of various special characters that enable us to describe patterns in text. We will explore some of the most commonly used operators, including literals, wildcards, character classes, quantifiers, anchors, and grouping constructs.\\n\\nFor example, literals are used to search for exact matches of characters or strings. Wildcards allow us to match any character in a certain position. Character classes enable us to define sets of characters to be matched. Quantifiers specify the number of occurrences of a character or a group. Anchors help us identify patterns at the beginning or end of a line. Lastly, grouping constructs allow us to create subexpressions within a larger pattern.\\n\\nTo better understand these concepts, let's consider a practical example that resonates with your interest in chatbots. Imagine we want to create a chatbot that responds to user queries about weather conditions. By using Regular Expressions, we can define specific patterns to recognize weather-related questions. For example, we can use the wildcard operator to match any word followed by the word 'weather,' thus identifying queries like 'What's the weather like today?' or 'How's the weather in New York?'\\n\\nBy mastering the syntax and operators of Regular Expressions, you will gain a solid foundation for numerous NLP applications. These applications include text searching, text substitution, data validation, and various text normalization techniques like Tokenization and Sentence Segmentation. As we progress through this series of lessons, we will delve into practical examples and hands-on exercises that leverage Regular Expressions in modern NLP scenarios.\\n\\nI hope you're as excited as I am to dive into this topic. So without further ado, let's get started by exploring the syntax and operators of Regular Expressions!\", 'content': 'Title: Introduction to Regular Expressions in NLP\\n\\n- Regular Expressions (Regex) are a powerful tool used for pattern matching in Natural Language Processing (NLP).\\n- The basic syntax and operators of Regex form the foundation of pattern matching.\\n- Common operators include literals, wildcards, character classes, quantifiers, anchors, and grouping constructs.\\n- Regex can be used for various NLP tasks such as text searching, data validation, and text normalization.\\n- Understanding the syntax and operators of Regex is essential for developing algorithms in NLP.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'This slide will serve as an entry point to understanding Regular Expressions (Regex) and how they are used in the context of Natural Language Processing (NLP). It aims to establish foundational knowledge for students with no prior experience, as specified in the Notebank. The focus will be on familiarizing the student with the syntax and basic operators of regex, setting the foundation for understanding its applications in NLP tasks.', 'concepts': ['Basic Syntax and Operators of Regular Expressions', 'Regular Expressions']}\n",
      "\n",
      "{'title': 'Fundamentals of Text Normalization in NLP', 'presentation': \"Hello! In this slide, we will be discussing the fundamentals of text normalization in the context of Natural Language Processing (NLP). So let's dive in!\\n\\nText normalization is a crucial process in NLP that involves converting text into a more uniform format. It helps to ensure consistency in language data, making it easier for computers to process and analyze. Think of it as preparing raw text for further NLP tasks like language modeling or sentiment analysis.\\n\\nThe first concept we'll cover is tokenization of text. This is all about breaking down text into smaller elements like words or phrases. It's an essential step before further processing, such as parsing or Part-of-Speech tagging. Tokenization helps in reducing inflectional forms and sometimes derivationally related forms of a word to a common base form. For example, let's take the sentence 'I love cats and dogs.' Tokenization would split it into individual tokens: ['I', 'love', 'cats', 'and', 'dogs'].\\n\\nMoving on, lemmanization is another important aspect of text normalization. It involves reducing a word to its base or root form, called a lemma. Unlike stemming, which simply chops off word endings, lemmatization takes into account the vocabulary and morphological analysis of words. This improves accuracy and is critical for understanding contextual meaning in NLP tasks such as text-to-speech, machine translation, and sentiment analysis.\\n\\nAnother concept we'll explore is sentence segmentation techniques. These methods and algorithms are used to divide a text into its constituent sentences. The goal is to accurately identify sentence boundaries, even in the presence of punctuation or formatting challenges. Common techniques involve using punctuation cues, capitalization, machine learning models, and rules-based systems that consider the linguistic and contextual structure of the text.\\n\\nTo wrap things up, it's important to understand that text normalization plays a significant role in improving the efficiency and accuracy of NLP tasks. It reduces lexical variety and complexity in text, making it more accessible for further analysis. By standardizing and preparing data using text normalization techniques, we can unlock the full potential of NLP in various applications, like text-to-speech synthesis, machine translation, and sentiment analysis.\\n\\nI hope you now have a better understanding of the fundamentals of text normalization in NLP. If you have any questions or would like to explore some practical examples, feel free to ask!\", 'content': 'Slide Content:\\n- What is Text Normalization in NLP?\\n    - The process of converting text into a more uniform format to enhance computational handling.\\n- Tokenization of Text\\n    - Breaking down text into smaller elements like words or phrases.\\n- Lemmatization\\n    - Reducing words to their base or root forms to improve contextual understanding.\\n- Sentence Segmentation Techniques\\n    - Methods for accurately identifying sentence boundaries despite punctuation and formatting challenges.\\n- Significance of Text Normalization in NLP\\n    - Facilitating efficient and accurate NLP tasks like text-to-speech synthesis, machine translation, and sentiment analysis.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'This slide aims to introduce the concept of Text Normalization within the realm of Natural Language Processing (NLP). It will explain what Text Normalization is, why it is critical for processing natural languages, and how it aids in preparing data for further NLP tasks such as Language Modeling or Sentiment Analysis.', 'concepts': ['Text Normalization', 'Tokenization of Text', 'Sentence Segmentation Techniques', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Getting Started with Edit Distance in Natural Language Processing', 'presentation': \"Welcome to an exciting start in the world of Natural Language Processing, or NLP for short. Today, we're going to unveil a fundamental concept that acts as a linchpin for various tasks within this field – the Edit Distance. Imagine you're texting a friend and make a typo. Interestingly, the mechanisms that allow your phone to correct your spelling are rooted in the concept we're exploring today. Edit Distance isn't just about fixing errors. It's about understanding and enabling a myriad of operations that make our interactions with technology smarter. So let's dive in and discover how a simple measure of difference between text strings can have far-reaching implications in technology and beyond.\", 'content': 'In this slide, we will introduce the concept of Edit Distance in Natural Language Processing (NLP). Edit Distance is a measure of the minimum number of operations required to transform one string into another. It is used in various NLP tasks such as spell checking, plagiarism detection, and DNA sequence analysis in bioinformatics. Edit Distance plays a crucial role in comparing and correcting strings within NLP systems, making it an essential concept to understand in the field.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'To provide an introduction to the concept of Edit Distance in Natural Language Processing (NLP), emphasizing its definition and utility within the field.', 'concepts': ['Applications of Edit Distance Algorithm in NLP']}\n",
      "\n",
      "{'title': 'Harnessing the Power of Regular Expressions in NLP', 'presentation': \"Good day! Today, we will explore the fascinating world of regular expressions and their application in Natural Language Processing (NLP). Regular expressions are powerful tools that allow us to effectively search for and match patterns within text. They play a key role in various NLP tasks, such as data extraction, cleaning, and text analysis. So, let's dive in and see how regular expressions can be harnessed in NLP.\\n\\nTo start, let's focus on the basics of regular expressions and their significance in text analysis and cleaning. Regular expressions are like specialized codes that help us extract specific information from large volumes of text. They enable us to create patterns to match words, phrases, or any other desired text segments. For example, if you want to extract phone numbers from a dataset, we can use regular expressions to define a pattern that matches the desired format of a phone number.\\n\\nNow, let's move on to some practical examples. Imagine you have a massive collection of text data and you want to extract specific information, such as all the dates mentioned within the text. By using regular expressions, you can define a pattern that matches the format of a date, allowing you to efficiently extract all the dates in one go. This is just one example of the power regular expressions can bring to NLP tasks!\\n\\nAnother interesting application of regular expressions in NLP is sentiment analysis. Sentiment analysis involves determining the emotional tone of a piece of text, whether it's positive, negative, or neutral. By using regular expressions, we can filter and process user reviews or social media posts to identify the sentiment expressed. For example, we can define patterns that match specific positive or negative words, allowing us to quickly classify the sentiment of a given text sample.\\n\\nRegular expressions are also crucial in text normalization, which involves standardizing and transforming text data for various NLP tasks. One aspect of text normalization is sentence segmentation, where we split a paragraph into individual sentences. Regular expressions can help us detect sentence boundaries based on specific punctuation marks or capitalization patterns.\\n\\nNow, let's explore some practical exercises together. I will provide you with different scenarios related to chatbots, where regular expressions can be utilized to extract specific information from user queries. You will have the opportunity to write your own regular expressions based on the given task. This hands-on practice will enhance your understanding and reinforce the knowledge you've learned so far.\\n\\nTo summarize, regular expressions are indispensable in NLP, enabling us to efficiently search, match, and extract information from text data. They have wide-ranging applications, including text analysis, sentiment analysis, and text normalization. By the end of this lesson, you will gain a thorough understanding of regular expressions and how they can be practically applied in NLP tasks. So, let's get started and put our regular expression skills to the test!\", 'content': 'Slide 1: Harnessing the Power of Regular Expressions in NLP\\n\\n- Introduction to Regular Expressions and their application in Natural Language Processing (NLP)\\n- Basics of regular expressions for text analysis, cleaning, and normalization\\n- Practical examples showcasing the use of regular expressions in tasks such as data extraction, sentiment analysis, and text segmentation\\n- Hands-on exercises for students to write their own regular expressions\\n- Discussion on the practical relevance of regular expressions in modern NLP applications, including chatbots and speech recognition', 'latex_codes': '', 'purpose': 4, 'purpose_statement': \"This slide aims to provide practical examples and exercises demonstrating the application of Regular Expressions within various NLP tasks, reinforcing theoretical knowledge through hands-on learning experiences to solidify the student's understanding.\", 'concepts': ['Uses of Regular Expressions in NLP']}\n",
      "\n",
      "{'title': 'Understanding Edit Distance and Its Relevance in NLP Applications', 'presentation': \"Hey there! Today, we're going to delve into the concept of Edit Distance and its relevance in the field of Natural Language Processing (NLP). Edit Distance is a way to measure how different two strings, like words or sentences, are from each other based on the minimum number of operations required to transform one string into the other. These operations can include inserting, deleting, or substituting a single character.\\n\\nTo help us understand the concept better, let's look at an example. Consider the words 'kitten' and 'sitting'. These two words have an edit distance of 3. Here's why: we need to substitute 'k' with 's', 'e' with 'i', and insert 'g' to transform 'kitten' into 'sitting'.\\n\\nNow, let's talk about the practical use cases of Edit Distance in NLP. One important application is spell checking. Edit Distance helps in comparing words against a dictionary of correct spellings. By calculating the edit distance between a misspelled word and correctly spelled words, we can suggest possible correct spellings and improve the accuracy of spell-checking systems.\\n\\nAnother area where Edit Distance is utilized is plagiarism detection. Here, it is used to compare documents and measure their similarity. By calculating the edit distance between two texts, we can identify instances of text copying without proper attribution.\\n\\nIn addition to that, Edit Distance also has applications in genome sequence analysis. It helps in examining the sequence of DNA in a genome to understand its structure, function, and evolution. Scientists can use Edit Distance to identify different genomes, determine their functions, and detect mutations that may lead to diseases.\\n\\nOverall, understanding Edit Distance is crucial in various NLP tasks such as spell checking, plagiarism detection, and genome sequence analysis. It allows us to quantify the similarity between strings and provides a foundation for building smarter systems in the field of NLP, like chatbots and text-to-speech. So, let's dive deeper into the world of Edit Distance and explore its practical implementation in modern NLP applications. Does that make sense? Let me know if you have any questions!\", 'content': 'Title: Understanding Edit Distance and Its Relevance in NLP Applications\\n\\n- Edit Distance is a way of quantifying the dissimilarity between two strings by counting the minimal number of operations required to transform one string into another.\\n\\n- It is a vital concept in Natural Language Processing (NLP) used in applications such as spell checking, plagiarism detection, and genome sequence analysis.\\n\\n- Edit Distance is calculated by considering the operations of insertion, deletion, and substitution of characters.\\n\\n- In spell checking, Edit Distance is used to compare words against a dictionary for accurate text representation.\\n\\n- Edit Distance is also crucial in genome sequence analysis where it helps to measure the mutation variations in DNA and identify potential diseases.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'To introduce the concept of Edit Distance in the context of Natural Language Processing (NLP), laying the groundwork for understanding its relevance to real-world NLP applications such as spell checking, plagiarism detection, and genome sequence analysis.', 'concepts': ['Edit Distance', 'Applications of Edit Distance Algorithm in NLP', 'spell checking', 'plagiarism detection', 'genome sequence analysis']}\n",
      "\n",
      "{'title': 'Decoding Edit Distance Calculations in NLP Contexts', 'presentation': \"Good day! Today, we'll be diving into the fascinating world of Edit Distance calculations in the context of Natural Language Processing (NLP). This concept holds vital significance in tasks such as spell checking, plagiarism detection, and even genome sequence analysis. So, let's explore the intricacies of Edit Distance and how it plays a crucial role in the NLP domain.\\n\\nTo begin, let's define Edit Distance. In simple terms, it measures the dissimilarity between two strings, such as words, by counting the minimum number of operations required to transform one string into the other. These operations include insertions, deletions, or substitutions of a single character. Understanding Edit Distance is essential for various NLP tasks, as it enables us to quantify the similarity or difference between pieces of text.\\n\\nNow, let's delve into the Levenshtein algorithm, which is the algorithm used to compute Edit Distance. This algorithm calculates the minimum number of single-character edits required to change one word into another. It follows a step-by-step process to evaluate the difference between two sequences, whether they are words or genetic sequences. By counting the number of insertions, deletions, and substitutions, we can determine the Edit Distance.\\n\\nMoving on, we'll explore the practical applications of the Edit Distance algorithm in the context of Comparative Genomics. Comparative genomics is a field where we compare the genomic features of different organisms. It helps us understand the structure, function, and evolutionary processes that affect genomes. By aligning genome sequences and using the Edit Distance algorithm, we can identify similarities and differences, ultimately inferring the evolutionary relationships between species.\\n\\nTo make this concept more relatable, imagine you're working on a research project involving genome sequence analysis. By using the Edit Distance algorithm, you'll be able to compare DNA sequences and identify the genetic variations that distinguish one species from another. This knowledge is invaluable in fields such as Bioinformatics, genetic mapping, and DNA sequencing.\\n\\nThroughout this presentation, we'll illustrate Edit Distance with graphical representations and practical examples relevant to your studies in Natural Language Processing. By the end of this lesson, you will have gained a solid theoretical foundation in Edit Distance calculations and understand its practical applications in modern NLP.\\n\\nNow that we have introduced the topic, let's continue our journey into the world of Edit Distance calculations in NLP! If you have any questions or need further clarification along the way, feel free to ask. Remember, the goal is for you to gain a profound understanding of this concept to excel in your NLP course.\", 'content': 'Slide Content\\n\\n- Slide Title: Decoding Edit Distance Calculations in NLP Contexts\\n\\n- Edit Distance: In the context of Natural Language Processing (NLP), Edit Distance is a way of quantifying how dissimilar two strings (e.g., words) are. It counts the minimum number of operations required to transform one string into the other, such as insertions, deletions, or substitutions of a single character. Edit Distance is essential for tasks like spell checking, plagiarism detection, and genome sequence analysis.\\n\\n- Levenshtein algorithm: The Levenshtein algorithm, also known as the Levenshtein distance, is a measure of the difference between two sequences. It calculates the minimum number of single-character edits (insertions, deletions, or substitutions) needed to change one word into the other. The Levenshtein algorithm is a key component of Edit Distance calculations in applications like spell checking, plagiarism detection, and genome sequence analysis.\\n\\n- Comparative Genomics: Comparative genomics is the field of biological research that compares the genomic features of different organisms. It plays a crucial role in genome sequence analysis, helping to understand the structure, function, and evolutionary processes that impact genomes. Comparative genomics involves aligning genome sequences and identifying similarities and differences to infer the evolutionary relationships between species. It intersects with disciplines like Bioinformatics, genetic mapping, and DNA sequencing.', 'latex_codes': '', 'purpose': 3, 'purpose_statement': \"To provide an in-depth understanding of the Edit Distance concept and its calculation techniques within the Natural Language Processing (NLP) context, focusing on its applications in text analysis problems encountered in the student's academic curriculum.\", 'concepts': ['Edit Distance', 'Levenshtein algorithm', 'Comparative Genomics']}\n",
      "\n",
      "{'title': 'Expanding the Boundaries with Machine Learning in NLP', 'presentation': \"Welcome to the slide on 'Expanding the Boundaries with Machine Learning in NLP'. This slide aims to explore the intersection of Machine Learning (ML) and Natural Language Processing (NLP) and introduce the role of ML in enhancing NLP tasks.\\n\\nMachine Learning is a branch of Artificial Intelligence that enables systems to learn and improve from experience without explicit programming. It involves developing algorithms that can make predictions or decisions based on data. In the context of NLP, ML plays a crucial role in various aspects.\\n\\nOne of the key areas where ML enhances NLP is language modeling. Language modeling is the task of predicting the likelihood of word sequences in natural language. For example, ML algorithms can power auto-complete features or assist in predictive texting. By understanding the context and patterns within language, these models can generate more accurate and contextual predictions.\\n\\nAnother NLP task where ML shines is sentiment analysis. Sentiment analysis involves analyzing textual data to extract subjective information, such as opinions, attitudes, and emotions. ML algorithms can classify texts into categories like positive, negative, or neutral sentiment, providing insights into consumer attitudes in fields like business analytics and customer feedback.\\n\\nLastly, ML algorithms play a crucial role in the development of chatbots. Chatbots are automated dialogue systems that use NLP techniques to simulate conversational experiences. ML algorithms enable chatbots to understand user input, process it through techniques like Regular Expressions, Text Normalization, and Edit Distance, and generate appropriate responses. They find applications in customer service, personal assistants, and language modeling scenarios.\\n\\nIn summary, the slide demonstrates the powerful synergy between Machine Learning and Natural Language Processing. By leveraging ML algorithms, NLP tasks like language modeling, sentiment analysis, and chatbots can be significantly enhanced. Through this exploration, we hope to expand your knowledge boundaries and deepen your understanding of the role of ML in modern NLP applications.\", 'content': '1. Title: Expanding the Boundaries with Machine Learning in NLP\\n\\n2. Definition of Machine Learning as a subset of AI that enables systems to learn and improve from experience without explicit programming\\n\\n3. Explanation of language modeling as the task of predicting the likelihood of word sequences in natural language and its applications in speech recognition, machine translation, and text generation\\n\\n4. Overview of sentiment analysis, which involves the use of ML algorithms to extract subjective information from textual data, including examples of positive, negative, and neutral sentiment\\n\\n5. Exploration of chatbot technology and its utilization of ML techniques like Regular Expressions, Text Normalization, and Edit Distance to simulate conversational experiences', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide aims to explore the intersection of Machine Learning (ML) and Natural Language Processing (NLP), establishing a foundational understanding of how ML algorithms can enhance NLP tasks such as language modeling, sentiment analysis, and chatbots. The slide also seeks to arouse curiosity and expand the student's knowledge boundaries by introducing the crucial role of ML in modern NLP applications.\", 'concepts': ['Machine Learning', 'language modeling', 'Sentiment Analysis', 'chatbots']}\n",
      "\n",
      "{'title': 'Unveiling the Practicality of Text Normalization in NLP', 'presentation': \"Hey there! Today, we're going to delve into the practicality of Text Normalization in Natural Language Processing (NLP). So, what exactly is Text Normalization? It's a process that involves converting text into a more uniform format, which is crucial for various NLP tasks. This slide will provide practical examples to demonstrate the importance and application of Text Normalization in NLP.\\n\\nLet's start with the concept of Sentiment Analysis. Imagine we have a bunch of texts that we want to classify as positive, negative, or neutral sentiments. Text Normalization plays a crucial role in accurately classifying these texts. We'll explore how techniques like Tokenization, Lemmatization, and Sentence Segmentation, which are part of Text Normalization, can affect sentiment analysis.\\n\\nNext, we'll move on to another exciting application: Speech Recognition. As you know, speech recognition involves converting spoken language into text. Here, too, Text Normalization comes into play. We'll analyze examples of raw spoken language and compare them to their normalized versions. This will help us understand how Text Normalization, along with language modeling and Machine Translation, contributes to efficient speech-to-text technologies.\\n\\nBy connecting previously discussed concepts like Tokenization of Text and Lemmatization to the new concepts of Sentiment Analysis and Speech Recognition, we'll see how these building blocks come together to empower these applications. For example, we'll observe how recognizing different word forms using Lemmatization can impact sentiment extraction.\\n\\nThroughout the presentation, we'll explore not only the theory behind Text Normalization but also the practical considerations and challenges that arise when performing Text Normalization. This will provide a real-world perspective on the application of these techniques.\\n\\nBy the end of this presentation, you'll have a solid understanding of the theory behind Text Normalization and how it's implemented in NLP applications like Sentiment Analysis and Speech Recognition. Get ready to unravel the practicality of Text Normalization and uncover its significance in the world of NLP!\", 'content': '1. Introduction to Text Normalization:\\n- Process of converting text into a uniform format.\\n- Includes Tokenization, Lemmatization, and Sentence Segmentation.\\n2. Practical Example 1: Sentiment Analysis:\\n- Show how Text Normalization enhances sentiment classification.\\n- Demonstrate the impact of Tokenization, Lemmatization, and Sentence Segmentation.\\n3. Practical Example 2: Speech Recognition:\\n- Illustrate the role of Text Normalization in speech-to-text technologies.\\n- Compare raw spoken language with normalized versions.\\n- Address challenges in normalizing slang and colloquial expressions.', 'latex_codes': '', 'purpose': 4, 'purpose_statement': 'To provide practical examples that demonstrate the application of previous concepts such as Text Normalization and Tokenization and to introduce new related concepts, namely Sentiment Analysis and Speech Recognition, for a deeper theoretical and practical understanding in the context of academic applications.', 'concepts': ['Text Normalization', 'Tokenization of Text', 'Lemmatization', 'Sentiment Analysis', 'Speech Recognition']}\n",
      "\n",
      "{'title': 'Embarking on the Journey of Machine Translation in NLP', 'presentation': \"Hello, and welcome to our introductory slide on Machine Translation in the field of Natural Language Processing, or NLP. In this presentation, we will take the first step of our educational journey, exploring how machines can translate text or speech from one language to another.\\n\\nImagine you are standing at the beginning of a vast landscape, with different paths leading to various languages. This visual representation symbolizes the concept of translation and the interconnected nature of global communication.\\n\\nAs we delve into the topic of Machine Translation, it's important to understand that it is a subfield of NLP. Machine Translation focuses on the problem of automatically translating text or speech using software. We employ complex algorithms and language modeling techniques to achieve accurate and efficient translation.\\n\\nYou might wonder, what is language modeling? Well, language modeling involves predicting the likelihood of a sequence of words in a natural language. It is a fundamental concept in NLP, allowing us to build probabilistic models that can generate or determine the probability distribution of linguistic units, such as words or sentences. Language modeling is the backbone of various NLP applications, including machine translation.\\n\\nThink of language modeling as a tool that empowers machines to 'understand' human languages and generate coherent translations. This technology has revolutionized our ability to bridge language barriers and enable smooth communication across different cultures and regions.\\n\\nMoreover, Machine Translation is just one aspect of NLP. This field encompasses a wide range of techniques and tools, such as text analysis, sentiment analysis, and speech recognition. It serves as the foundation for applications like speech-to-text systems, chatbots, and text-to-speech technology.\\n\\nTo give you a glimpse of the practical side of NLP, imagine a real-time translation app on your phone. You speak into your device in one language, and the app automatically translates your speech into another language. Amazing, right?\\n\\nAs we continue our journey through the world of NLP, we'll explore the fascinating concepts of Regular Expressions, Text Normalization, and Edit Distance, which all contribute to Machine Translation and play crucial roles in understanding and processing human language.\\n\\nBy the end of this presentation, I hope you have gained a clear understanding of Machine Translation as a subfield of NLP and its vital role in facilitating cross-linguistic communication. We are excited to continue this educational adventure with you as we explore the depths of NLP and its vast applications. Let's get started!\", 'content': 'This slide serves as a premier introduction to the concept of Machine Translation within the field of Natural Language Processing. Machine Translation (MT) is a subfield of Natural Language Processing (NLP) that focuses on the problem of automatically translating text or speech from one language to another. It involves the use of software and complex algorithms to perform the translation task. MT can be approached in several ways, including rule-based, statistical, and neural methods. Language modeling is a fundamental concept in NLP, which involves developing probabilistic models that can generate or determine the probability distribution of linguistic units. TTS (Text-to-Speech) is often used in NLP for applications such as assistive technology, speech recognition, and chatbots. The ultimate objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both meaningful and useful.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'This slide will serve as a premier introduction to the concept of Machine Translation within the field of Natural Language Processing, setting the stage for students to understand how machines can translate text or speech from one language to another.', 'concepts': ['Machine Translation', 'Algorithms', 'Natural Language Processing (NLP)', 'language modeling', 'text-to-speech']}\n",
      "\n",
      "{'title': 'Mastering the Fundamentals of DNA Sequencing in NLP', 'presentation': \"Hey there! Welcome to the slide on mastering the fundamentals of DNA sequencing in NLP. In this slide, we will introduce you to the fascinating world of DNA sequencing and its relation to natural language processing. Are you ready to dive in? Great! Let's get started.\\n\\nDNA sequencing is the process of determining the precise order of nucleotides within a DNA molecule. It's like reading the genetic code of life! By knowing the order of the four bases—adenine, guanine, cytosine, and thymine—we can unlock valuable information about genes, genetic traits, and even diseases. This process has revolutionized the field of genomics and has numerous applications in genetic testing, biomedical research, and forensic biology.\\n\\nNow, let's talk about bioinformatics. Bioinformatics is an interdisciplinary field that combines computer science, biology, chemistry, statistics, and mathematics. It helps us interpret and analyze biological data, including the data obtained through DNA sequencing. By using computational methods and algorithms, bioinformatics allows us to make sense of complex genetic information and understand how genes and genomes work.\\n\\nOne important application of DNA sequencing is genetic mapping. Genetic mapping involves determining the location and chemical sequence of specific genes on a DNA strand. With the help of genome sequence analysis, scientists can associate particular segments of DNA with specific characteristics or diseases. This helps in tasks like genome editing, plagiarism detection in academic research, and understanding the genetic basis of diseases.\\n\\nAnother fascinating area is comparative genomics. Comparative genomics is all about comparing the genomic features of different organisms. By aligning genome sequences and identifying similarities and differences, scientists can gain insights into the structure, function, and evolutionary relationships between species. Comparative genomics intersects with bioinformatics, genetic mapping, and DNA sequencing, creating a synergy of knowledge and discoveries.\\n\\nNow, how does all of this relate to natural language processing? Well, believe it or not, there are parallels between DNA sequencing and text processing. Just like in comparative genomics, text processing in NLP involves using algorithms and methods like regular expressions for pattern matching and edit distance for measuring similarity between text sequences. Genetic mapping's resemblance to plagiarism detection tools in NLP shows how both fields use algorithms to identify patterns and copy-paste within datasets.\\n\\nSo, the fundamentals of DNA sequencing in NLP lay a solid foundation for understanding how computational methods can help us explore and exploit the vast amounts of genomic and textual data. With this knowledge, you'll be prepared to dive deeper into the fascinating world of genomics and natural language processing. I hope you're as excited as I am to explore this fascinating intersection of fields!\", 'content': 'Title: Mastering the Fundamentals of DNA Sequencing in NLP\\n\\n1. Introduction to DNA Sequencing: DNA sequencing is the process of determining the order of nucleotides in a DNA molecule. It has revolutionized fields like genomics, genetic testing, biomedical research, and forensic biology.\\n\\n2. Bioinformatics: Bioinformatics is an interdisciplinary field that combines computer science, biology, chemistry, statistics, and mathematics to analyze and interpret biological data. It is essential for genome sequence analysis and DNA sequencing.\\n\\n3. Genetic Mapping: Genetic mapping involves determining the location and chemical sequence of specific genes on a DNA strand. It aids in tasks like genome editing, plagiarism detection, and understanding the genetic basis of diseases.\\n\\n4. Comparative Genomics: Comparative genomics compares the genomic features of different organisms to understand their structure, function, and evolutionary processes. It intersects with bioinformatics, genetic mapping, and DNA sequencing.\\n\\n5. DNA Sequencing in NLP: DNA sequencing and NLP share similarities, such as the use of regular expressions for pattern matching and edit distance for measuring similarity between sequences of text. These connections bring together computational methods with biological and textual datasets.', 'latex_codes': '', 'purpose': 0, 'purpose_statement': 'To introduce the interdisciplinary concepts of DNA sequencing and Bioinformatics and their relation to Natural Language Processing, setting a foundation for understanding the parallels between biological sequence analysis and text processing.', 'concepts': ['DNA sequencing', 'Bioinformatics', 'genetic mapping', 'Comparative Genomics']}\n",
      "\n",
      "{'title': 'Implementing Tokenization and Lemmatization in NLP Workflows', 'presentation': \"Good day! Today, we will delve into the intriguing world of Natural Language Processing, focusing on the implementation of two essential concepts: Tokenization of Text and Lemmatization. These concepts are fundamental for various NLP tasks, making them crucial to understand for our academic objectives. So, let's get started!\\n\\nTo begin, let's clarify the purpose of tokenization. It is the process of breaking down a sequence of characters into individual tokens, which can be words, phrases, symbols, or other meaningful elements. Tokenization is an initial step that allows us to analyze text further, like parsing or part-of-speech tagging. It plays a critical role in text normalization by reducing inflectional and derivational forms of words to a common base form.\\n\\nMoving on to lemmatization, it involves reducing a word to its base or root form, known as a lemma. Unlike stemming, which simply chops off word endings, lemmatization takes into account the vocabulary and morphological analysis of words, making it more sophisticated and accurate. Lemmatization is essential for various NLP tasks such as text-to-speech, machine translation, and sentiment analysis.\\n\\nNow, let's explore how tokenization and lemmatization are implemented in NLP workflows. Imagine we're working on a chatbot that needs to understand and respond to user inputs. The tokenization process would break down the user's message into individual words or tokens, allowing the chatbot to analyze and interpret each word effectively. Following that, lemmatization would then convert these tokens to their base form, ensuring a standardized representation of the text for further processing and analysis.\\n\\nTo demonstrate the practical application of these concepts, let's take a practical example. Suppose we have the following sentence: 'The cats are playing in the garden.' First, tokenization would split this sentence into individual tokens, resulting in ['The', 'cats', 'are', 'playing', 'in', 'the', 'garden']. Then, lemmatization would convert these tokens to their base form, giving us ['the', 'cat', 'be', 'play', 'in', 'the', 'garden']. By normalizing the text through tokenization and lemmatization, we derive a structured representation of the sentence that can be used for various NLP tasks.\\n\\nIn summary, tokenization and lemmatization are key concepts in NLP. Tokenization helps us break down text into meaningful units, while lemmatization ensures each token is reduced to its base form. By understanding and implementing these concepts, we can enhance our NLP workflows and effectively process and analyze text data for different applications. I hope this practical understanding will enable you to excel in your NLP studies and perform well in your academic pursuits. Thank you!\", 'content': '1. Tokenization of Text is the process of converting a sequence of characters into tokens, such as words, phrases, or symbols. It is an essential step in NLP for further processing and text normalization.\\n2. Lemmatization involves reducing a word to its base or root form, called a lemma, considering the vocabulary and morphology. It plays a crucial role in various NLP tasks like text-to-speech, machine translation, and sentiment analysis.\\n3. Tokenization and Lemmatization are vital in NLP workflows, ensuring text analysis and normalization, reducing complexity and variability of natural language.\\n4. We can use a popular NLP library like NLTK or spaCy to implement tokenization and lemmatization effectively.\\n5. A practical example will be demonstrated, showcasing the implementation of tokenization and lemmatization using code, highlighting the impact on text normalization and analysis.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': 'This slide aims to provide students with a practical understanding of how Text Tokenization and Lemmatization are implemented in NLP. The focus is on enhancing the student’s practical skills by allowing them to encode these concepts into a workflow, which is crucial for their academic applications and will assist with the NLP class objectives.', 'concepts': ['Tokenization of Text', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Implementing Tokenization and Lemmatization in NLP Workflows', 'presentation': \"Good afternoon! On this slide, we're going to explore the fascinating world of Natural Language Processing (NLP) and dive into two important concepts: Tokenization of Text and Lemmatization. These processes play a crucial role in NLP workflows and have practical applications in various fields.\\n\\nTokenization is the first step in NLP, where we break down a sequence of characters into smaller units called tokens. These tokens can be words, phrases, symbols, or other meaningful elements. Ultimately, tokenization allows us to convert text into a format suitable for further analysis, such as parsing and part-of-speech tagging.\\n\\nLemmatization, on the other hand, goes beyond simple stemming by reducing words to their base or root form, called a lemma. This process takes into account the word's morphological analysis and its part of speech in a sentence. It's a more sophisticated approach that ensures higher accuracy when normalizing words.\\n\\nNow, let's move beyond theory and look at the practical side of things. We'll explore how tokenization and lemmatization are implemented in NLP workflows. For example, we'll show you code snippets and workflow diagrams to demonstrate how these processes are used in tasks like text-to-speech, machine translation, and sentiment analysis.\\n\\nBy the end of this slide, you'll have a solid understanding of how tokenization and lemmatization fit into the bigger picture of NLP. You'll be able to connect the theoretical concepts with real-world applications, and see how they contribute to successful outcomes in various NLP tasks. So, let's dive in and explore the fascinating world of tokenization and lemmatization in NLP!\", 'content': '1. Tokenization of Text: The process of converting text into a sequence of tokens, which are smaller units such as words or phrases. Tokenization is a fundamental step in NLP and helps with tasks like parsing and Part-of-Speech tagging.\\n\\n2. Lemmatization: A process in NLP that reduces words to their base or dictionary form, known as a lemma. Unlike stemming, lemmatization considers the vocabulary and morphological analysis, resulting in higher accuracy.\\n\\n3. Practical Application: Tokenization and lemmatization are implemented in NLP workflows. They play a crucial role in text normalization and are used in tasks like text-to-speech, machine translation, and sentiment analysis.\\n\\n4. Code Examples: Code snippets and workflow diagrams will demonstrate how to implement tokenization and lemmatization in an NLP workflow.\\n\\n5. Academic Relevance: Understanding and applying tokenization and lemmatization are important for succeeding in NLP coursework and future NLP projects.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': 'This slide aims to provide students with a practical understanding of how Text Tokenization and Lemmatization are implemented in NLP. The focus is on enhancing the student’s practical skills by allowing them to encode these concepts into a workflow, which is crucial for their academic applications and will assist with the NLP class objectives.', 'concepts': ['Tokenization of Text', 'Lemmatization']}\n",
      "\n",
      "{'title': 'Advancing Natural Language Understanding: Text Analysis Essentials', 'presentation': \"Welcome to the slide titled 'Advancing Natural Language Understanding: Text Analysis Essentials'. In this slide, we will delve into the intriguing world of Text Analysis as part of Natural Language Processing (NLP). Our aim is to enhance your understanding and practical knowledge of this field.\\n\\nText Analysis involves extracting meaningful information from natural language text. It comprises various methodologies and technologies used in NLP, such as Regular Expressions, Text Normalization, and understanding the Edit Distance between strings. By using these techniques, we can structure unstructured text data and perform tasks like sentiment analysis, topic modeling, and named entity recognition.\\n\\nOne of the fundamental concepts in NLP is language modeling. Language modeling focuses on predicting the likelihood of word sequences in a natural language. This concept serves as the basis for various NLP applications, including speech recognition, machine translation, and text generation.\\n\\nIn this slide, we will explore the key concepts of Regular Expressions, Text Normalization, and Edit Distance in the context of NLP. Regular Expressions are a powerful tool for text processing, allowing us to define search patterns with various basic syntax and operators. These patterns are used in tasks such as text searching, data validation, and tokenization of text.\\n\\nText Normalization, on the other hand, involves converting text into a more uniform format. It includes tasks like tokenization of text, lemmatization, and sentence segmentation techniques. Text Normalization helps in reducing lexical variety and complexity, making NLP tasks more efficient and accurate.\\n\\nEdit Distance is a measure of how dissimilar two strings (e.g., words) are by counting the minimum number of operations required to transform one string into the other. It is essential for tasks such as spell checking, plagiarism detection, and genome sequence analysis. We will explore the calculation of Edit Distance and its applications in NLP.\\n\\nThroughout the slide, we will provide practical examples and exercises to help solidify your understanding of these concepts. We will also discuss their applications in modern NLP, such as chatbots.\\n\\nBy the end of this slide, you will have a solid foundation in the key concepts of text analysis in NLP, and you will be equipped with the knowledge necessary to excel in your NLP class. Let's dive in and discover the fascinating world of Text Analysis!\", 'content': '1. Introduction to Text Analysis: Extracting meaningful information from natural language text using various methodologies and technologies in NLP.\\n\\n2. Regular Expressions: Powerful tools for text processing, allowing the specification of complex search patterns for tasks like text searching and substitution.\\n\\n3. Text Normalization: Process of converting text into a more uniform format, including tasks such as tokenization, lemmatization, and sentence segmentation.\\n\\n4. Edit Distance: Measure of dissimilarity between strings, used in spell checking, plagiarism detection, and genome sequence analysis.\\n\\n5. Practical Examples and Applications: Demonstration of how these concepts are applied in modern NLP applications like chatbots, with hands-on exercises and exercises to reinforce the understanding of the material.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide aims to bridge foundational NLP concepts with underexplored yet fundamental areas of Text Analysis. It will introduce Text Analysis and elucidate on its various applications in Natural Language Processing, enriching the student's academic curriculum and facilitating better performance in the NLP class through practical, example-driven learning.\", 'concepts': ['Text Analysis', 'language modeling', 'genome', 'Natural Language Processing (NLP)', 'Regular Expressions', 'Text Normalization', 'Edit Distance']}\n",
      "\n",
      "{'title': 'Unlocking Advanced Text Normalization Techniques in NLP', 'presentation': \"Welcome to the slide 'Unlocking Advanced Text Normalization Techniques in NLP'. In this slide, we will explore advanced techniques used in Natural Language Processing or NLP. These techniques are essential for text processing in NLP applications and involve regular expressions, text normalization, and edit distance.\\n\\nLet's begin by talking about regular expressions. Regular expressions are powerful tools used to match patterns in text. They play a crucial role in text cleaning and preprocessing for NLP. You can think of them as a kind of language for specifying patterns in text. Regular expressions consist of a combination of symbols and metacharacters that allow you to perform various operations on a text.\\n\\nThe use of regular expressions in NLP extends beyond just basic text matching. They enable us to perform tasks like tokenization, sentence segmentation, and even more complex operations like named entity recognition. By understanding and utilizing regular expressions, you will gain the ability to process and manipulate text in powerful and efficient ways, which is fundamental in NLP.\\n\\nMoving on to the concept of text normalization. Text normalization is the process of converting text into its canonical or standard form. It involves tasks like tokenization and lemmatization. Tokenization is the process of breaking text into individual tokens or words. This is essential for further analysis as it allows us to work with smaller units of text. Lemmatization, on the other hand, aims to reduce words to their base or root form. This helps to reduce information redundancy and ensures consistent interpretation of text by NLP models.\\n\\nLastly, we will discuss the concept of edit distance. Edit distance is a measure of the similarity between two strings. It quantifies the minimum number of operations required to transform one string into another. This concept is particularly useful in tasks like spell-checking systems, plagiarism detection, and even DNA sequence alignment. By understanding edit distance, you will gain insight into how similarity among texts can be measured and how it can be applied in various NLP applications.\\n\\nTo reinforce your understanding of these techniques, we will provide practical examples and real-world applications using regular expressions, text normalization, and edit distance. We will explore how these concepts can be applied in tasks like chatbot development, where understanding and normalizing user input are crucial for effective communication.\\n\\nBy the end of this slide, you will have a solid understanding of advanced text normalization techniques in NLP and their practical applications. You will be well-equipped to work with text data and apply these techniques in your own NLP projects. So let's dive in and unlock the world of advanced text normalization in NLP!\", 'content': '1. Introduction to Regular Expressions in NLP, covering basic syntax and operators. \\n\\n2. Practical examples showcasing Regular Expressions in text normalization, including sentence segmentation and tokenization. \\n\\n3. Importance of lemmatization and its role in reducing information redundancy and ensuring consistent text interpretation. \\n\\n4. Exploration of Edit Distance concept, its formal equation, and real-world applications such as spell-check systems and plagiarism detection. \\n\\n5. A case study demonstrating the practical applications of the discussed concepts in developing a chatbot, highlighting text normalization, regular expressions, and Edit Distance for handling user input variations.', 'latex_codes': '', 'purpose': 4, 'purpose_statement': \"To provide practical application and reinforcement of learned concepts in advanced text normalization techniques, enhancing the student's proficiency in text processing for NLP applications.\", 'concepts': ['Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Unlocking Advanced Text Normalization Techniques in NLP', 'presentation': \"Good day! Today, we'll be exploring the fascinating world of advanced text normalization techniques in Natural Language Processing (NLP). In this session, we'll delve into the concepts of regular expressions, text normalization, and edit distance, understanding how they play a crucial role in NLP applications.\\n\\nTo begin, let's take a quick review of the foundational concepts of regular expressions, text normalization, and edit distance within NLP. Regular expressions are powerful patterns used to search, match, and manipulate text data. Text normalization involves transforming raw text into a standardized format, making it easier to process. Edit distance, on the other hand, measures the similarity between two strings, benefiting tasks like spell checking and duplicate detection.\", 'content': '1. Foundational concepts: Regular Expressions, Text Normalization, and Edit Distance in NLP.\\n2. Application of concepts in modern NLP tasks and their impact on language model performance.\\n3. Practical examples of advanced text normalization techniques, including handling typos, slang, and other irregularities in text data.\\n4. Tokenization strategies and the importance of lemmatization in text preprocessing.\\n5. Edit Distance algorithm, its role in measuring similarity between strings, and its application in spell-checking and duplicate detection.\\n6. Interactive exercises to apply the concepts, particularly in chatbot scenarios.', 'latex_codes': '', 'purpose': 4, 'purpose_statement': \"To provide practical application and reinforcement of learned concepts in advanced text normalization techniques, enhancing the student's proficiency in text processing for NLP applications.\", 'concepts': ['Natural Language Processing (NLP)']}\n",
      "\n",
      "{'title': 'Delving Deeper: Advanced Applications of Edit Distance in NLP', 'presentation': \"Good day! In our journey through Natural Language Processing, we've encountered a concept called Edit Distance, which measures the dissimilarity between two text strings. Today, we will delve deeper into Edit Distance and explore its advanced applications in the fields of NLP.\\n\\nOne of the fascinating applications of Edit Distance is in plagiarism detection. By employing Edit Distance algorithms alongside Text Normalization techniques, we can compare textual content and enhance the accuracy of plagiarism detection software. This is particularly important for academic integrity, ensuring that proper credit is given and intellectual property is respected.\\n\\nMoving from the textual realm to the biological realm, Edit Distance also plays a crucial role in genome sequence analysis. In the subfield of Comparative Genomics, Edit Distance helps us understand the evolutionary relationships between different species by aligning genome sequences and identifying similarities and differences. With this powerful computational tool, scientists can gain insights into the structure, function, and evolutionary processes of genomes.\\n\\nTo further solidify your understanding, let's explore some practical examples. Imagine a chatbot that uses Edit Distance to analyze human conversation and respond intelligently. This application showcases the syntactic and semantic understanding that Edit Distance can bring to chatbots, making them more conversational and engaging.\\n\\nSimilarly, Edit Distance is essential for detecting plagiarism in scholarly works, ensuring that originality is preserved and academic honesty is maintained. By applying Edit Distance algorithms, we can identify similar strings of text and determine the level of similarity between documents. This is crucial for verifying the authenticity of written work.\\n\\nAdditionally, Edit Distance is widely used for genome comparison tasks, where it helps researchers analyze genetic sequences and detect mutations that may lead to diseases. By aligning and comparing genome sequences, scientists can gain insights into the structure and function of different genes, ultimately advancing our understanding of human health and biology.\\n\\nOverall, Edit Distance holds great significance in the field of NLP, stretching its arms out to diverse applications in plagiarism detection, genome sequence analysis, and beyond. As you explore these advanced applications, remember that Edit Distance is not just an abstract concept but a powerful tool that spans various disciplines, connecting academia and real-world implications. Let's embrace the depth and breadth of Edit Distance in our exciting journey through Natural Language Processing!\", 'content': '1. Edit Distance:  Edit Distance is a way of quantifying the dissimilarity between two strings by counting the minimum number of operations (insertions, deletions, substitutions) required to transform one string into the other.\\n\\n2. Advanced Applications in NLP:  Edit Distance finds advanced applications in NLP, such as plagiarism detection and genome sequence analysis.\\n\\n3. Plagiarism Detection: Edit Distance algorithms combined with Text Normalization techniques result in more accurate plagiarism detection, aiding in identifying instances where text has been copied without proper authorization and attribution.\\n\\n4. Genome Sequence Analysis: Edit Distance plays a crucial role in Comparative Genomics, offering insights into the genetic relationships between species by identifying similarities and differences in their DNA sequences.\\n\\n5. Real-world Applications: Edit Distance is used in various real-world applications, such as creating chatbots with better syntactic and semantic understanding, enhancing plagiarism detection software, and assisting comparative genomics researchers in understanding evolutionary pathways.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide aims to build upon the foundational understanding of the Edit Distance concept by exploring its advanced applications in the fields of NLP, specifically focusing on plagiarism detection, genome sequence analysis, and comparative genomics, which have not been fully examined in previous sessions. The goal is to leverage this deeper dive to enhance the student's comprehension for their NLP class and to nourish their interest in wide-ranging practical applications.\", 'concepts': ['Edit Distance', 'plagiarism detection', 'Comparative Genomics', 'genome sequence analysis']}\n",
      "\n",
      "{'title': 'Exploring the Nuances of Sentence Segmentation in Text Analysis', 'presentation': \"Alright, let's dive into the fascinating world of sentence segmentation in text analysis. Today, we'll explore the nuances of breaking down text into meaningful sentences, an essential step in Natural Language Processing (NLP). We'll build on what you've already learned about text normalization and tokenization to enhance your understanding of how texts are prepared for NLP tasks. So, let's get started!\\n\\nSentence segmentation is all about dividing a block of text into its constituent sentences. This may sound simple, but it can actually be quite challenging due to various factors like punctuation, capitalization, and linguistic intricacies. To truly grasp the complexities, we'll cover different techniques and strategies used to accurately identify sentence boundaries.\\n\\nFirst, let's understand why sentence segmentation is crucial in text analysis. It lays the foundation for many NLP applications, such as sentiment analysis, topic modeling, and named entity recognition. By breaking text into sentences, we can extract meaningful information from unstructured data.\\n\\nOne technique we'll explore is the use of regular expressions. These powerful search patterns help identify potential sentence boundaries by detecting punctuation patterns. For example, we can look for periods followed by a space and an uppercase letter. We'll dive into the basic syntax and operators of regular expressions and provide practical examples to solidify your understanding.\\n\\nAdditionally, we'll discuss how capitalization cues and linguistic patterns play a role in sentence segmentation. Sometimes, punctuation alone is not enough, and we need to consider contextual elements. We'll examine how linguistic and grammatical patterns can guide the segmentation process.\\n\\nIt's important to note that inaccuracies in sentence segmentation can have a significant impact on downstream NLP tasks. We'll address common challenges and discuss strategies to improve accuracy, combining multiple techniques to achieve better results.\\n\\nTo make the concepts more tangible, we'll explore how machine learning models can be leveraged for sentence segmentation. These models learn from linguistic context beyond predetermined rules and patterns, enhancing the segmentation process. We'll provide visual representations and examples, allowing you to see the segmentation in action.\\n\\nThroughout the presentation, I'll make connections to real-world applications, such as chatbots and text-to-speech systems. This will highlight the practical relevance of precise sentence segmentation and prepare you for scenarios you may encounter in your NLP journey.\\n\\nBy the end of this presentation, you'll have a deeper understanding of sentence segmentation, its challenges, and its importance in NLP. You'll be well-prepared to tackle academic evaluations and apply your knowledge to real-world NLP tasks. So, let's embark on this exploration together and unlock the secrets of sentence segmentation!\", 'content': '1. Introduction to Sentence Segmentation Techniques:\\n- Sentence Segmentation Techniques involve methods and algorithms used in Natural Language Processing (NLP) to divide a text into its constituent sentences.\\n\\n2. Importance of Sentence Segmentation:\\n- Discuss the significance of accurate sentence segmentation as a crucial preprocessing step in NLP tasks.\\n\\n3. Challenges in Sentence Segmentation:\\n- Highlight the complexities posed by punctuation, capitalization, and linguistic structures, and the impact of inaccuracies in subsequent NLP tasks.\\n\\n4. Common Sentence Segmentation Techniques:\\n- Explore techniques such as punctuation cues, capitalization, rule-based systems, and machine learning models.\\n\\n5. Practical Applications of Sentence Segmentation:\\n- Demonstrate the importance of accurate segmentation in modern NLP applications like chatbots and text-to-speech systems.', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide will delve into the intricacies of segmenting text into sentences, an important pre-processing step in NLP, and how it paves the way for more advanced text analysis. As we have already touched upon Text Normalization and Tokenization, understanding Sentence Segmentation will enrich the student's comprehension of how texts are prepared for NLP tasks.\", 'concepts': ['Sentence Segmentation Techniques', 'Text Analysis', 'Natural Language Processing (NLP)', 'Regular Expressions']}\n",
      "\n",
      "{'title': 'Exploring the Nuances of Sentence Segmentation in Text Analysis', 'presentation': \"Good day! Today, we are going to explore the nuances of sentence segmentation in text analysis. Sentence segmentation involves dividing a text into its constituent sentences, and it is an important pre-processing step in Natural Language Processing (NLP). By accurately identifying sentence boundaries, we can unlock more advanced text analysis techniques.\\n\\nImagine you have a text that you want to analyze. You need to be able to separate it into individual sentences so you can perform tasks like sentiment analysis, topic modeling, and named entity recognition. For example, let's say you want to analyze customer reviews of a product to determine overall sentiment. Being able to isolate each sentence allows you to assess sentiment on a sentence-by-sentence basis, giving you more granular insights.\\n\\nBut why is sentence segmentation challenging? Punctuation marks can complicate the process. For instance, periods can indicate the end of a sentence, but they can also appear in abbreviations and decimal numbers. Capitalization can also be helpful, but it may not always be a reliable indicator, especially in different languages and text corpora.\\n\\nTo overcome these challenges, various techniques have been developed. One approach is to rely on punctuation cues to identify sentence boundaries. Another is to analyze capitalization patterns in the text. Additionally, there are machine learning models that can be trained to detect sentence boundaries, as well as rule-based systems that consider the linguistic and contextual structure of the text.\\n\\nThink about your objectives as a computer science student and how sentence segmentation can enhance your NLP skills. Mastering this technique will enable you to better understand and analyze text data, empowering you to develop more sophisticated NLP applications in the future.\\n\\nBy the end of this lesson, you will have a solid understanding of sentence segmentation techniques and how they fit into the broader field of text analysis. Are you ready to dive in?\", 'content': '1. Introduction to Sentence Segmentation Techniques\\n2. Challenges in identifying sentence boundaries\\n3. Common methods for sentence segmentation\\n4. Connection to Text Normalization and Tokenization\\n5. Importance of sentence segmentation in advanced text analysis', 'latex_codes': '', 'purpose': 2, 'purpose_statement': \"This slide will delve into the intricacies of segmenting text into sentences, an important pre-processing step in NLP, and how it paves the way for more advanced text analysis. As we have already touched upon Text Normalization and Tokenization, understanding Sentence Segmentation will enrich the student's comprehension of how texts are prepared for NLP tasks.\", 'concepts': ['Sentence Segmentation Techniques', 'Text Analysis', 'Natural Language Processing (NLP)', 'Regular Expressions']}\n"
     ]
    }
   ],
   "source": [
    "### Slide generation from AITutor\n",
    "notebank = NoteBank()\n",
    "[notebank.add_note(n) for n in tutor_plan.split(\"\\n\")]\n",
    "slide_planner = SlidePlanner(notebank, concept_db)\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"./temp_slideplan.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(\"./temp_slideplan.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        slide_plans = pkl.load(f)\n",
    "        print(\"\\n\\n\".join([str(slide.format_json()) for slide in slide_plans]))\n",
    "        slide_planner.SlidePlans = slide_plans\n",
    "else:\n",
    "    slide_planner.generate_slide_plan()\n",
    "    with open(\"./temp_slideplan.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(slide_planner.SlidePlans, f)\n",
    "\n",
    "\n",
    "if os.path.exists(\"./temp_slides.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(\"./temp_slides.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        slides = pkl.load(f)\n",
    "        print(\"\\n\\n\".join([str(slide.format_json()) for slide in slides]))\n",
    "        slide_planner.Slides = slides\n",
    "else:\n",
    "    slide_planner.generate_slide_deque()\n",
    "    with open(\"./temp_slides.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(slide_planner.Slides, f)\n",
    "\n",
    "print(\"\\n\\n\".join([str(slide.format_json()) for slide in slide_planner.Slides]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "- Preprocessing\n",
    "- Generation of questions from (Expert, GPT, LLMaAiT-BE)\n",
    "- Comparison and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subject': 1, 'type': 3, 'data': {'data': \"Write a Python function 'calculate_edit_distance' that takes in two strings and returns the Edit Distance between them using the Levenshtein algorithm. Comment each step to explain your logic.\", 'boilerplate': 'def calculate_edit_distance(str1, str2):\\n    # Start by setting up a table to hold the edit distances between all prefixes of the first string and all prefixes of the second\\n    # HINT: Consider the base cases - where one of the strings is empty\\n    pass # Your code here', 'test_cases_script': \"assert calculate_edit_distance('kitten', 'sitting') == 3; assert calculate_edit_distance('intention', 'execution') == 5; assert calculate_edit_distance('Saturday', 'Sunday') == 3;\", 'concepts': ['Basic Syntax and Operators of Regular Expressions', 'Applications of Edit Distance Algorithm in NLP']}}\n",
      "\n",
      "{'subject': 1, 'type': 3, 'data': {'data': 'Given a noisy text input, write a Python function `normalize_and_segment(text)` that uses regular expressions to normalize the text by removing special characters and excessive white spaces. Then, segment the normalized text into sentences. Assume a sentence ends with a period followed by a space.', 'boilerplate': 'import re\\ndef normalize_and_segment(text):\\n    # Your code here\\n    return sentences', 'test_cases_script': \"assert normalize_and_segment('Hello! This is an  example   text... ')==['Hello', 'This is an example text']\", 'concepts': ['Regular Expressions', 'Text Normalization', 'Sentence Segmentation Techniques']}}\n",
      "\n",
      "{'subject': 1, 'type': 3, 'data': {'data': 'Write a Python function called `tokenize_text` that uses regular expressions to normalize and tokenize an input string. The function should convert the text to lowercase, remove punctuation, and split the text into a list of tokens (words). Provide a brief comment within your code explaining the purpose of the regular expression used.', 'boilerplate': 'def tokenize_text(text):\\n\\t# TODO: Implement the function using a regular expression\\n\\tpass', 'test_cases_script': 'assert tokenize_text(\"Hello, World!\") == [\"hello\", \"world\"]\\nassert tokenize_text(\"NLP is fun!\") == [\"nlp\", \"is\", \"fun\"]', 'concepts': ['Natural Language Processing (NLP)', 'Regular Expressions', 'Basic Syntax and Operators of Regular Expressions', 'Text Normalization', 'Tokenization of Text']}}\n",
      "\n",
      "{'subject': 1, 'type': 0, 'data': {'data': 'Define regular expressions and explain their role in text processing within the domain of Natural Language Processing. Provide an example that illustrates how a regular expression is used in NLP.', 'rubric': 'Rubric: [1 Points] Student provides a clear and concise definition of regular expressions. [2 Points] Student explains the role of regular expressions in NLP. [2 Points] Student provides a correct and relevant example of regular expressions usage in NLP.', 'concepts': ['Regular Expressions', 'Uses of Regular Expressions in NLP']}}\n",
      "\n",
      "{'subject': 1, 'type': 3, 'data': {'data': 'Implement a Python function that uses regular expressions to segment text into sentences and then calculate the edit distance between the first two segmented sentences. Assume you are provided with a simple edit distance function that takes in two strings.', 'boilerplate': 'import re\\n\\n# Function to segment text into sentences using regular expressions\\ndef segment_sentences(text):\\n    pass  # TODO: Implement sentence segmentation logic using regular expressions\\n\\n# Function to calculate the edit distance between two strings\\ndef edit_distance(s1, s2):\\n    pass  # TODO: Implement edit distance calculation\\n\\n# Example usage\\nsentences = segment_sentences(text)\\nedit_dist = edit_distance(sentences[0], sentences[1])\\nprint(edit_dist)\\n', 'test_cases_script': 'text = \"This is sentence 1. This is sentence 2. This is sentence 3.\";\\n\\n# Test case 1: Check if the sentences are correctly segmented\\nsentences = segment_sentences(text)\\nassert sentences == [\"This is sentence 1.\", \"This is sentence 2.\", \"This is sentence 3.\"], f\"Expected sentences to be [\\'This is sentence 1.\\', \\'This is sentence 2.\\', \\'This is sentence 3.\\'], but got {sentences}\"\\n\\n# Test case 2: Check if the edit distance is correctly calculated\\nedit_dist = edit_distance(sentences[0], sentences[1])\\nassert edit_dist == 0, f\"Expected edit distance to be 0, but got {edit_dist}\"\\n\\n# Test case 3: Check if the edit distance is correctly calculated for different sentences\\nedit_dist = edit_distance(sentences[0], sentences[2])\\nassert edit_dist == 1, f\"Expected edit distance to be 1, but got {edit_dist}\"\\n\\nprint(\"All test cases passed!\")\\n', 'concepts': ['Regular Expressions', 'Sentence Segmentation Techniques', 'Edit Distance']}}\n",
      "\n",
      "{'subject': 1, 'type': 3, 'data': {'data': \"Regular expressions are essential for text normalization tasks in NLP. Write a Python function named `normalize_text` that takes a string as input and normalizes it using regular expressions. Your function should convert text to lowercase, remove leading and trailing whitespace, replace email addresses with the string '<EMAIL>', and replace website URLs with '<URL>'.\", 'boilerplate': 'import re\\n\\ndef normalize_text(text):\\n    # Convert to lowercase\\n    text = text.lower()\\n\\n    # Remove leading and trailing whitespace\\n    text = text.strip()\\n\\n    # Replace email addresses and websites\\n    # Your code here\\n\\n    return text', 'test_cases_script': \"assert normalize_text('  Contact: John.Doe@example.com ') == 'contact: <EMAIL>'\\nassert normalize_text('Visit us at https://www.example.com for more info.') == 'visit us at <URL> for more info.'\\nassert normalize_text('  Hello world   ') == 'hello world'\", 'concepts': ['Regular Expressions', 'Text Normalization']}}\n"
     ]
    }
   ],
   "source": [
    "notebank = NoteBank()\n",
    "[notebank.add_note(n) for n in tutor_plan.split(\"\\n\")]\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"./temp_questions.pkl\"):\n",
    "    # Load the object from the file\n",
    "    with open(\"./temp_questions.pkl\", \"rb\") as f:  # 'rb' mode is for reading in binary format\n",
    "        question = pkl.load(f)\n",
    "        question_db = QuestionSuite(6, notebank, concept_db)\n",
    "        question_db.Questions = question\n",
    "else:\n",
    "    question_db = QuestionSuite(6, notebank, concept_db)\n",
    "    question_db.generate_question_data()\n",
    "    with open(\"./temp_questions.pkl\", \"wb\") as f:  # 'wb' mode is for writing in binary format\n",
    "        pkl.dump(question_db.Questions, f)\n",
    "\n",
    "print(\"\\n\\n\".join([str(quest.format_json()) for quest in question_db.Questions]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERRORS\n",
    "\n",
    "\n",
    "the thing we are checking for errors is number of api calls per errors. api calls during translation / errors during translation\n",
    "gpt-4 and gpt-3.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCEPTS RATIO OF NUMBER OF RELEVANT CONCEPTS OVER NUMBER OF CONCEPS\n",
    "GPT-3.5 vs GPT-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
